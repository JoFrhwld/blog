{
  "hash": "624ab8fd01e32fd36d41894da949fbf0",
  "result": {
    "markdown": "---\ntitle: \"A lesson in over-preparing\"\nauthor: \"Josef Fruehwald\"\ndate: 2022-12-10\norder: 10\nformat: html\neditor: visual\nimage: \"index_files/figure-html/fig-total-1.png\"\ntwitter-card:\n  image: \"index_files/figure-html/fig-path-1.png\"\nopen-graph:\n  image: \"index_files/figure-html/fig-path-1.png\"\nexecute:\n  echo: true\n  message: false\n# citation: \n#   url: \"https://jofrhwld.github.io/blog/posts/2022/12/2022-12-10.html\"\n---\n\n\nOne of the courses I taught in the Fall 2022 semester was Natural Language Processing (NLP). It was a fun course to teach, and I learned a lot since it's not a topic in my core areas of research. At the same time, the amount of work I put into it has made me really start to rethink how I prepare for teaching.\n\nMy tendency, for a while, has been to prepare extensive course notes that I publish on my website ([Exhibit A](https://jofrhwld.github.io/teaching/courses/2017_lsa/), [Exhibit B](https://jofrhwld.github.io/teaching/courses/2022_lin517/)). I've never really reflected on how much work that actually takes. Moreover, I don't tend to consider it when I reflect on how much \"writing\" I get done (and inevitably get a bit discouraged about my productivity).\n\nBut, it occurred to me that I could quantify how much writing I really did this semester. I wrote all my course notes in Quarto, which [generates a search index](https://quarto.org/docs/websites/website-search.html) in a .json file. To get a total count of words that I wrote in course notes, I just need to parse that json file and tokenize it! I found [a handy blog post about analyzing git repos](https://drsimonj.svbtle.com/embarking-on-a-tidy-git-analysis) that helped a lot in the process.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidyjson)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(padr)\nlibrary(glue)\n```\n:::\n\n\nThis block of code is just copied from the blog post I just mentioned.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remote repository URL\nrepo_url <- \"https://github.com/JoFrhwld/2022_lin517.git\"\n\n# Directory into which git repo will be cloned\nclone_dir <- file.path(tempdir(), \"git_repo\")\n\n# Create command\nclone_cmd <- glue(\"git clone {repo_url} {clone_dir}\")\n\n# Invoke command\nsystem(clone_cmd)\n```\n:::\n\n\nThere's a handful of R libraries for reading json files into R, but after searching around I went with `tidyjson` because I like using tidyverse things.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsearch_file <- file.path(clone_dir, \"_site\", \"search.json\")\nsite_search <- read_json(search_file)\n```\n:::\n\n\nI was really glad to find that the `search.json` file is relatively flat, so pulling out the metadata and text was not as complicated as it could have been.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsite_search |>\n  gather_array() |>\n  hoist(..JSON,\n    \"title\",\n    \"section\",\n    \"text\"\n  )  |>\n  as_tibble() |>\n  select(array.index, title, section, text) |>\n  unnest_tokens(input = text, output = \"words\") -> tokens_table\n```\n:::\n\n\nI noticed in the json file that there were multiple entries for a single page of course notes, one for each subsection, but there were also some entries with the subsection value set to blank. I just wanted to double check that the blank section entries weren't the *entire* page of lecture notes, with additional entries duplicating the text by subsection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens_table |>\n  mutate(has_section = section != \"\") |>\n  group_by(title, has_section) |>\n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 45 × 3\n# Groups:   title, has_section [45]\n   title                              has_section     n\n   <chr>                              <lgl>       <int>\n 1 Addendum                           FALSE          43\n 2 Addendum                           TRUE          100\n 3 Additional Neural Network Concepts FALSE         473\n 4 Additional Neural Network Concepts TRUE         1109\n 5 Comprehensions and Useful Things   FALSE        1116\n 6 Data Processing                    FALSE         654\n 7 Data Processing                    TRUE         1804\n 8 Data Sparsity                      FALSE         720\n 9 Data Sparsity                      TRUE         1724\n10 Evaluating models                  FALSE          42\n# … with 35 more rows\n```\n:::\n:::\n\n\nLooks like no. The blank titled sections are *probably* cases where I had a paragraph or two that came before the first section header.\n\n### So *how many words?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens_table |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 43637\n```\n:::\n:::\n\n\nBased on the default tokenization from `tidytext`, it looks like I wrote just north of 40k words. I don't have the best sense of how this compares to other kinds of genre writing, but apparently the goal of NaNoWriMo (National Novel Writing Month) is to [write a novel that is 50k words](https://nanowrimo.org/about-nano).\n\nSo, I didn't quite write a novel. But the amount of work that went into these 40k words was still considerable in terms of background research and trying to come to my own understanding of relatively complex mathematical formulae so that I could distill them into a comprehensible lesson. Also not accounted for was all the code I wrote and had to debug *within* the course notes!\n\n## How many words did I write over time?\n\nSince I was publishing the course notes to github, that means I also have a preserved history of how my word count grew over time. All I have to do is apply the same procedures to the history of `search.json`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"code to get the git commit history\"}\nlog_format_options <- c(datetime = \"cd\", commit = \"h\", parents = \"p\", author = \"an\", subject = \"s\")\noption_delim <- \"\\t\"\nlog_format   <- glue(\"%{log_format_options}\") |> glue_collapse(option_delim)\nlog_options  <- glue('--pretty=format:\"{log_format}\" --date=format:\"%Y-%m-%d %H:%M:%S\"')\nlog_cmd      <- glue('git -C {clone_dir} log {log_options}')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(log_cmd, intern = TRUE) |>\n  str_split_fixed(option_delim, length(log_format_options)) |>\n  as_tibble(.name_repair = \"minimal\") |>\n  setNames(names(log_format_options))->commit_history\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"`get_length()` function definition\"}\nget_length <- function(commit, clone_dir){\n  search_file <- file.path(clone_dir, \"_site\", \"search.json\")\n  checkout_cmd <- glue(\"git -C {clone_dir} checkout {commit} {search_file}\")\n  system(checkout_cmd)\n  site_search <- read_json(search_file)\n  site_search |>\n    gather_array() |>\n    hoist(..JSON,\n      \"title\",\n      \"section\",\n      \"text\"\n    )  |>\n    as_tibble() |>\n    select(array.index, title, section, text) |>\n    unnest_tokens(input = text, output = \"words\") |>\n    nrow() -> word_count\n  return(word_count)\n}\n```\n:::\n\n\nThe data frame `commit_history` contains metadata about each commit. What I'll do next is apply the function I wrote in the collapsed code above to the list of commit hashes, and get the word count at each commit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommit_history |>\n  mutate(word_count = map(commit, ~get_length(.x, clone_dir)) |>\n           simplify()) -> word_history\n```\n:::\n\n\nI often had many commits in a single day, so to simplify things a bit, I'll just get the max number of words I had by the end of a given day. Fortunately, I did not stay up past midnight pushing commits this semester, so the data shouldn't be messed up by work sessions overlapping across days.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_history |>\n  mutate(datetime = ymd_hms(datetime)) |>\n  thicken(\"day\") |>\n  group_by(datetime_day) |>\n  summarise(words = max(word_count)) |>\n  arrange(datetime_day) -> daily_words\n```\n:::\n\n\n### Cumulative Wordcount\n\nHere's the cumulative count of words over the course of the semester. I forget why, exactly, it starts out at 7,000 words. I think I might've been managing the course notes differently between the end of August and start of September, and then just copied them over. But it is a pretty continuous rise, with a few notable plateaus. Some of those plateaus occurred when I had actually prepared more than could be covered in a one or two class meetings, so we stayed with a set of lecture notes for a longer period of time.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"plotting code\"}\ndaily_words |>\n  pad(\"day\") |>\n  fill(words) |>\n  ggplot(aes(datetime_day, words))+\n    geom_ribbon(aes(ymin = 0, ymax = words), alpha = 0.2)+\n    geom_line(linewidth = 1,\n              lineend = \"round\")+\n    expand_limits(y = 0)+\n    scale_y_continuous(labels = label_comma(),\n                       expand = expansion(mult = c(0, 0.05)))+\n    scale_x_date(expand = expansion(mult = 0.01))+\n    labs(x = \"date\",\n         y = \"number of words\",\n         title = \"Cumulative number of words in course notes\")\n```\n\n::: {.cell-output-display}\n![The number of words I wrote over the course of Fall 2022](index_files/figure-html/fig-total-1.png){#fig-total fig-align='center' fig-alt='A line graph showing a continuous increase from about 7,000 words to over 40,000 words between September and December' width=576}\n:::\n:::\n\n\n### When was I writing?\n\nI can also double check *when* I was writing my course notes. I was teaching on a Monday-Wednesday-Friday schedule, and Fridays were usually given over to practical exercises (the text of which is largely absent from these counts...).\n\nIt was no surprise to me that I wrote most of the course notes on Sundays. The total number of Sunday words is about 17k, which comes out to about mean of about 1,300 words for Sundays this semester.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"plotting code\"}\ndaily_words |>\n  pad(\"day\") |>\n  fill(words) |>\n  mutate(daily_words = words - lag(words),\n         wday = wday(datetime_day, label = T)) |>\n  drop_na() |>\n  group_by(wday) |>\n  summarise(word = sum(daily_words)) |>\n  ggplot(aes(wday, word))+\n    geom_col(fill = \"grey90\")+\n    scale_y_continuous(labels = label_comma())+\n    labs(x = \"day of the week\",\n         y = \"total words\",\n         title = \"words written by day of week\")\n```\n\n::: {.cell-output-display}\n![The total number of words I wrote per day of the week.](index_files/figure-html/fig-byday-1.png){#fig-byday fig-align='center' width=576}\n:::\n:::\n\n\n## Thoughts\n\nWhile I feel pretty good about the material I produced, I really don't think this way of doing things is tenable. The writing that I *did* do for these course notes was writing that I *didn't* do for any other project. Moreover, you can almost see how much energy Sundays took out of me! And even though I wasn't writing as many *course notes* Tuesday through Friday, those *were* full work days that I was doing all of my other work during.\n\nSo the upshot is: I'm already brainstorming on how to *not* prepare like this for a class again!\n\n## Sources\n\n-   <https://drsimonj.svbtle.com/embarking-on-a-tidy-git-analysis>\n\n-   <https://tidyr.tidyverse.org/reference/hoist.html>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}