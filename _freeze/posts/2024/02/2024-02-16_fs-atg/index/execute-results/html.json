{
  "hash": "a8647ffb303a901b68b76996eb5cbd03",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using FastTrackPy and aligned-textgrid\"\ndate: 2024-02-16\norder: 2\nfilters:\n  - codeblocklabel\nbibliography: references.bib\nknitr:\n  opts_chunk: \n    warning: false\n    message: false\n---\n\n\nLast semester, I spent time co-developing some python packages:\n\n-   [aligned-textgrid](https://forced-alignment-and-vowel-extraction.github.io/alignedTextGrid/) with [Christian Brickhouse](https://christianbrickhouse.com/)\n\n-   [fasttrackpy](https://fasttrackiverse.github.io/fasttrackpy) with [Santiago Barreda](https://santiagobarreda.com/)\n\nSo, I thought I'd share a little walkthrough of a cool way to use them. They can both be installed with pip.\n\n``` bash\npip install fastttrackpy\npip install aligned-textgrid\n```\n\n## FastTrackPy\n\nfasttrackpy [@fruehwald2023] is a python implementation of [Santiago Barreda's Praat plugin](https://github.com/santiagobarreda/FastTrack) [@barreda2021]. Right now, its design is really geared towards command line usage, and has three different subcommands\n\n-   `fasttrack audio`\n\n    -   This will run fasttrack on a single audio file or a directory of audio files\n\n-   `fasttrack audio-textgrid`\n\n    -   This will run fasttrack on an (audio, textgrid) tuple\n\n-   `fasttrack corpus`\n\n    -   This will run fasttrack on a corpus of paired audio + textgrid files\n\nYou can check out the docs for all of [the processing options](https://fasttrackiverse.github.io/fasttrackpy/usage/all_arguments.html). I'll be using a config file that looks like this:\n\n``` yaml\n# config.yml\ncorpus: data/corpus/\noutput: data/results/formants.csv\nentry-classes: \"Word|Phone\"\ntarget-tier: Phone\ntarget-labels: \"[AEIOU]\"\nmin-duration: 0.05\nmin-max-formant: 4000\nmax-max-formant: 7000\nnstep: 20\nwhich-output: winner\ndata-output: formants\n```\n\nSome of these settings are just the defaults, but I'm just illustrating the kind of things you could do. To run it:\n\n``` bash\nfasttrack corpus --config config.yml\n```\n\nOn my laptop, it got formant estimates for 339 vowels in about 18 seconds.\n\n## Looking at the data\n\nLet's get R up and running\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setup libraries\"}\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(marginaleffects)\nlibrary(gt)\nlibrary(reticulate)\nlibrary(khroma)\nlibrary(geomtextpath)\nlibrary(downlit)\nlibrary(xml2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvowel_data <- read_csv(\"data/results/formants.csv\")\n```\n:::\n\n\nJust to skim over some data columns of interest\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowel_data |> \n colnames()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"F1\"            \"F2\"            \"F3\"            \"F1_s\"         \n [5] \"F2_s\"          \"F3_s\"          \"error\"         \"time\"         \n [9] \"max_formant\"   \"n_formant\"     \"smooth_method\" \"file_name\"    \n[13] \"id\"            \"group\"         \"label\"         \"F4\"           \n[17] \"F4_s\"         \n```\n\n\n:::\n:::\n\n\n::: callout-note\n## Useful Columns\n\nF1, F2, F3, F4\n\n:   The formant tracks as estimated by the LPC analysis\n\nF1_s, F2_s, F3_s, F4_s\n\n:   Smoothed formant tracks, using discrete cosine transform\n\nfile_name\n\n:   The basename for each file in the corpus\n\ngroup\n\n:   If there were multiple talkers annotated in a file, which talker\n\nid\n\n:   A unique ID for each phone\n:::\n\nI'm going to zoom in on my favorite vowel, \"AY\", and fit a quick model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Getting the ay data\nvowel_data |> \n  filter(\n    group %in% c(\"KY25A\", \"group_0\"),\n    str_detect(label, \"AY\")\n  ) |> \n  select(\n    file_name,\n    id,\n    group,\n    label,\n    F1_s, F2_s,\n    time\n  ) |> \n  mutate(\n    rel_time = time - min(time),\n    prop_time = rel_time / max(rel_time),\n    .by = c(file_name, id)\n  )->\n  ay_data\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Model fitting (not the main point)\"}\nay_data |> \n  group_by(\n    file_name\n  ) |> \n  nest() |> \n  mutate(\n    model = map(\n      data, \n      ~gam(\n        list(F1_s ~ s(prop_time),\n             F2_s ~ s(prop_time)),\n        data = .x,\n        family = mvn(d = 2)\n      )\n    ),\n    pred = map(\n      model,\n      ~predictions(\n        .x, \n        newdata = datagrid(\n          prop_time = seq(0,1,length = 100)\n        )\n      )\n    )\n  ) |> \n  select(file_name, pred) |> \n  unnest(pred) |> \n  select(file_name, rowid, group, estimate, prop_time) |> \n  mutate(\n    group = str_glue(\"F{group}\")\n  ) |> \n  pivot_wider(\n    names_from = group,\n    values_from = estimate\n  )->\n  ay_predictions\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plotting code\"}\nlibrary(scales)\nlog_rev_trans = trans_new(\n  name = \"log_rev\",\n  transform = \\(x) -log(x),\n  inverse = \\(x) exp(-x)\n)\n\nay_predictions |> \n  ggplot(\n    aes(\n      F2, \n      F1\n    )\n  )+\n    geom_path(\n      arrow = arrow(type = \"closed\"),\n      linewidth = 1\n    ) +\n    scale_x_continuous(trans = log_rev_trans)+\n    scale_y_continuous(trans = log_rev_trans)+\n    coord_fixed()+\n    facet_wrap(~file_name)\n```\n\n::: {.cell-output-display}\n![/ay/ trajectories](index_files/figure-html/fig-first-ay-1.png){#fig-first-ay width=80%}\n:::\n:::\n\n\nCool! Except... One of the most important factors for /ay/ is missing: whether or not the following segment is voiced or voiceless! Since fasttrackpy is designed to be very general purpose, (and not too feature laden) this kind of info isn't added to the output. But. we can easily get it with aligned-textgrid.\n\n## Working with aligned-textgrid\n\nRight now, aligned-textgrid [@fruehwald2023a] mostly designed to be worked with either in scripts, or interactively, so we're going to switch over to python code. I'll work over just one TextGrid for clarity.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom aligned_textgrid import AlignedTextGrid, Word, Phone\nfrom pathlib import Path\nimport pandas as pd\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntg1_path = Path(\n  \"data\", \n  \"corpus\",\n  \"josef-fruehwald_speaker.TextGrid\"\n  )\n  \ntg1 = AlignedTextGrid(\n  textgrid_path = tg1_path,\n  entry_classes = [Word, Phone]\n)\n\ntg1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAlignedTextGrid with 1 groups named ['group_0'] each with [2] tiers. [['Word', 'Phone']]\n```\n\n\n:::\n:::\n\n\nI want to grab out enriched data for each phone for the `group_0` speaker, which we can do with the dynamically created accessors for each speaker group and tier class like so.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nphone_tier = tg1.group_0.Phone\nphone_tier\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSequence tier of Phone; .superset_class: Word; .subset_class: Bottom_wp\n```\n\n\n:::\n:::\n\n\nWe can grab individual phones via indexing.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nphone_tier[30]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClass Phone, label: IY0, .superset_class: Word, .super_instance: the, .subset_class: Bottom_wp\n```\n\n\n:::\n:::\n\n\nBut I want to focus in on just the phones with an `AY` label, which I'll do with a list comprehension.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nays = [p for p in phone_tier if \"AY\" in p.label]\n```\n:::\n\n\nTo grab the following segment for each /ay/, we can use the `.fol` accessor.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# a single example\nays[0].fol.label\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'T'\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# for all /ays/\n[p.fol.label for p in ays]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['T', 'K', 'K', 'T', 'T', '#', 'Z', 'N', 'N', '#', 'D', '#', '#', '#', 'D', 'Z', 'Z', 'M', 'D', 'T', 'P']\n```\n\n\n:::\n:::\n\n\nYou can see that some /ay/ tokens have a `#` following segment, meaning a word boundary. If we wanted to get the following segment tier-wise, we can do so to.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n[p.get_tierwise(1).label for p in ays]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['T', 'K', 'K', 'T', 'T', 'AH0', 'Z', 'N', 'N', '', 'D', 'R', 'DH', 'DH', 'D', 'Z', 'Z', 'M', 'D', 'T', 'P']\n```\n\n\n:::\n:::\n\n\nLet's pop this all into a pandas dataframe\n\n\n::: {.cell}\n\n```{.python .cell-code}\nays_context = pd.DataFrame({\n  \"id\":       [p.id for p in ays],\n  \"fol\":      [p.fol.label for p in ays],\n  \"fol_abs\":  [p.get_tierwise(1).label for p in ays],\n  \"word\":     [p.within.label for p in ays],\n  \"fol_word\": [p.within.fol.label for p in ays ]\n})\n\nays_context\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           id fol fol_abs      word    fol_word\n0     0-0-3-4   T       T  sunlight     strikes\n1     0-0-4-3   K       K   strikes   raindrops\n2    0-0-12-1   K       K      like           a\n3    0-0-25-1   T       T     white       light\n4    0-0-26-1   T       T     light            \n5    0-0-49-1   #     AH0      high       above\n6    0-0-60-2   Z       Z   horizon            \n7    0-0-85-1   N       N     finds          it\n8   0-0-153-1   N       N      sign        from\n9   0-0-188-2   #               sky            \n10  0-0-193-2   D       D     tried          to\n11  0-0-209-1   #       R        by  reflection\n12  0-0-216-1   #      DH        by         the\n13  0-0-235-1   #      DH        by         the\n14  0-0-246-0   D       D     ideas       about\n15  0-0-263-1   Z       Z      size          of\n16  0-0-279-1   Z       Z      size          of\n17  0-0-287-2   M       M   primary     rainbow\n18  0-0-333-1   D       D      wide      yellow\n19  0-0-343-1   T       T    lights        when\n20  0-0-354-1   P       P      type          of\n```\n\n\n:::\n:::\n\n\nWith the way aligned-textgrid links intervals and relates their hierarchical structure, I'm able to quickly able to navigate up, down, and over between intervals using straightforwardly named accessors.\n\nWe can get pretty silly, like: what is the second to last phoneme in the word following the word this vowel is in?\n\n\n::: {.cell}\n\n```{.python .cell-code}\n[\n  ays[0].label,\n  ays[0].within.label,\n  ays[0].within.fol.label,\n  ays[0].within.fol.last.label,\n  ays[0].within.fol.last.prev.label\n]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['AY2', 'sunlight', 'strikes', 'S', 'K']\n```\n\n\n:::\n:::\n\n\n## Joining together\n\nBack to the /ays/ data, we can quickly join this enriched data onto the formant data, because the `id` column is the same between the two.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_data |> \n  left_join(\n    py$ays_context |> \n      mutate(file_name = \"josef-fruehwald_speaker\")\n  ) |> \n  filter(\n    !is.na(fol)\n  ) |> \n  mutate(\n    voicing = case_when(\n      fol %in% c(\"P\", \"T\", \"K\") ~ \"vless\",\n      fol == \"#\" ~ \"final\",\n      .default = \"vced\"\n    )\n  )->\n  ays_enriched\n\nays_enriched |> \n  head() |> \n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"file_name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"group\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"label\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1_s\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2_s\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"rel_time\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"prop_time\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fol\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"fol_abs\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"fol_word\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"voicing\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"josef-fruehwald_speaker\",\"2\":\"0-0-3-4\",\"3\":\"group_0\",\"4\":\"AY2\",\"5\":\"508.4787\",\"6\":\"1304.620\",\"7\":\"0.0255\",\"8\":\"0.000\",\"9\":\"0.00000000\",\"10\":\"T\",\"11\":\"T\",\"12\":\"sunlight\",\"13\":\"strikes\",\"14\":\"vless\"},{\"1\":\"josef-fruehwald_speaker\",\"2\":\"0-0-3-4\",\"3\":\"group_0\",\"4\":\"AY2\",\"5\":\"508.8226\",\"6\":\"1308.756\",\"7\":\"0.0275\",\"8\":\"0.002\",\"9\":\"0.03703704\",\"10\":\"T\",\"11\":\"T\",\"12\":\"sunlight\",\"13\":\"strikes\",\"14\":\"vless\"},{\"1\":\"josef-fruehwald_speaker\",\"2\":\"0-0-3-4\",\"3\":\"group_0\",\"4\":\"AY2\",\"5\":\"509.6878\",\"6\":\"1320.962\",\"7\":\"0.0295\",\"8\":\"0.004\",\"9\":\"0.07407407\",\"10\":\"T\",\"11\":\"T\",\"12\":\"sunlight\",\"13\":\"strikes\",\"14\":\"vless\"},{\"1\":\"josef-fruehwald_speaker\",\"2\":\"0-0-3-4\",\"3\":\"group_0\",\"4\":\"AY2\",\"5\":\"510.6127\",\"6\":\"1340.641\",\"7\":\"0.0315\",\"8\":\"0.006\",\"9\":\"0.11111111\",\"10\":\"T\",\"11\":\"T\",\"12\":\"sunlight\",\"13\":\"strikes\",\"14\":\"vless\"},{\"1\":\"josef-fruehwald_speaker\",\"2\":\"0-0-3-4\",\"3\":\"group_0\",\"4\":\"AY2\",\"5\":\"510.9429\",\"6\":\"1366.842\",\"7\":\"0.0335\",\"8\":\"0.008\",\"9\":\"0.14814815\",\"10\":\"T\",\"11\":\"T\",\"12\":\"sunlight\",\"13\":\"strikes\",\"14\":\"vless\"},{\"1\":\"josef-fruehwald_speaker\",\"2\":\"0-0-3-4\",\"3\":\"group_0\",\"4\":\"AY2\",\"5\":\"509.9791\",\"6\":\"1398.323\",\"7\":\"0.0355\",\"8\":\"0.010\",\"9\":\"0.18518519\",\"10\":\"T\",\"11\":\"T\",\"12\":\"sunlight\",\"13\":\"strikes\",\"14\":\"vless\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nAnd now I can refit the model and plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Modelling code\"}\n# gam is annoying and needs\n# voicing to explicitly be a factor\nays_enriched |> \n  mutate(voicing = factor(voicing)) ->\n  ays_enriched\n\nays_enriched_model <- gam(\n  list(\n    F1_s ~ voicing + s(prop_time, by = voicing),\n    F2_s ~ voicing + s(prop_time, by = voicing)\n  ),\n  data = ays_enriched,\n  family = mvn(d = 2) \n)\n\n\nays_enriched_model |> \n  predictions(\n    newdata = datagrid(\n      prop_time = seq(0, 1, length = 100),\n      voicing = unique\n    )\n  ) |> \n  as_tibble() |> \n  select(\n    rowid, group,\n    estimate, prop_time, voicing\n  ) |> \n  mutate(\n    group = str_glue(\"F{group}\")\n  ) |> \n  pivot_wider(\n    names_from = group,\n    values_from = estimate\n  ) ->\n  ays_enriched_pred\n```\n:::\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"plotting code\"}\nays_enriched_pred |> \n  ggplot(\n    aes(\n      F2,\n      F1,\n      color = voicing\n    )\n  )+\n    geom_textpath(\n      aes(label = voicing),\n      linewidth = 1,\n      arrow = arrow(type = \"closed\")\n    )+\n    scale_x_continuous(\n      trans = log_rev_trans\n    )+\n    scale_y_continuous(\n      trans = log_rev_trans\n    )+\n    scale_color_bright(\n      guide = \"none\"\n    )+\n    coord_fixed()\n```\n\n::: {.cell-output-display}\n![enriched /ays/ data](index_files/figure-html/fig-voicing-plot-1.png){#fig-voicing-plot width=60%}\n:::\n:::\n\n\n## Let me know how it goes! \n\nIf you start using either fasttrackpy or aligned-textgrid for any purpose, I'd love to know how it's going! For any feature requests, or bug reports, checkout their respective github repositories.\n\n-   [aligned-textgrid Github](https://github.com/Forced-Alignment-and-Vowel-Extraction/alignedTextGrid)\n\n-   [fasttrackpy Github](https://github.com/FastTrackiverse/fasttrackpy)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}