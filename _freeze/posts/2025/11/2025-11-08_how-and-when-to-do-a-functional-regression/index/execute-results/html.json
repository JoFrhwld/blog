{
  "hash": "f3ca363ba5fefbe3d89e7d60c905e6dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: How and when to regress over DCTs\nbibliography: references.bib\nimage: index_files/figure-html/unnamed-chunk-13-1.png\n---\n\nI recently [gave a talk](https://jofrhwld.github.io/nwav53/) about analyzing vowel formant trajectories using [the Discrete Cosine Transform](https://jofrhwld.github.io/blog/posts/2025/06/2025-06-17_dct-in-tidynorm/), and people have been asking me about when they could/should use a DCT based model versus a gamm.\nSo, I'll work through that question here!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"setup\"}\nsource(here::here(\"_defaults.R\"))\n\nlibrary(tidyverse)\nlibrary(tidynorm)\nlibrary(ggdist)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(distributional)\nlibrary(ggdensity)\nlibrary(marginaleffects)\nlibrary(tinytable)\nlibrary(scico)\nlibrary(scales)\n\nthis_seed <- 2025-11-13\n```\n:::\n\n\n## Data Setup\n\nI'll use the `speaker_tracks` data set from `{tidynorm}` as an example.\nLet's grab the data from speaker `s03`for /ay/.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspeaker_tracks |> \n  filter(\n    speaker == \"s03\",\n    vowel == \"AY\"\n  ) |> \n  mutate(\n    voicing = case_match(\n      plt_vclass,\n      \"ay0\" ~ \"voiceless\",\n      \"ay\" ~ \"voiced\"\n    ) |> \n      fct_relevel(\"voiced\")\n  ) ->\n  s03_ay\n```\n:::\n\n\nI'll look at the effect of duration and voicing on the F1 trajectory, so we need to prep that data.\nI'll log-transform and center vowel duration at the median.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns03_ay |> \n  mutate(\n    .by = id,\n    dur = diff(range(t)), \n    # proportional time\n    prop_t = (t - min(t))/(dur)\n  ) |> \n  mutate(\n    across(F1:F3, log),\n    log_dur = log2(dur),\n    log_dur_c = log_dur - median(log_dur)\n  ) ->\n  s03_ay_tomod\n```\n:::\n\n\n## Complications for a gamm\n\n### 1. Autocorrelation\n\nAs Márton Sóskuthy has pointed out in his tutorial [@soskuthyGeneralisedAdditiveMixed2017] and Journal of Phonetics paper [@soskuthyEvaluatingGeneralisedAdditive2021], a major complication for modelling vowel formant tracks in a gamm is the *autocorrelation* of the data.\nIf we just had a model formula like this:\n\n``` r\nbf(F1 ~ s(prop_t))\n```\n\nThe model will treat the data like a bunch of points associated with the `prop_t` variable, like so:\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\ns03_ay_tomod |> \n  ggplot(\n    aes(prop_t, F1)\n  ) +\n  geom_point() ->\n  point_p\n\npoint_p\npoint_p + theme_darkmode()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\n*But*, this is very much not what the data is actually like.\nWhat we have is a bunch of *trajectories* that play out over time.\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\ns03_ay_tomod |> \n  ggplot(\n    aes(prop_t, F1)\n  ) +\n  geom_line(\n    aes(group = id),\n    linewidth = 0.5\n  ) ->\n  traj_p\n\ntraj_p\ntraj_p + theme_darkmode()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThe value of F1 at each next measurement point isn't just correlated with the previous value: it's strongly determined by the previous value.\nThere is a very limited degree to which any two measurement points can be different due in part to the limited speech of speed articulation, and very large jumps and outliers are typically assumed to be measurement error.\n\n### 2: A ton of interactions\n\nFor every predictor that might have an effect of the shape of the trajectory, I'll need to include it in a non-linear interaction with `prop_t`.\nRight now I'm looking at duration and voicing, but I'll also need to include a random-factor smooth by word.\nAnd if I had more than one speaker, I'd need to include a random-factor smooth by speaker.\n\n## Fitting the gamm\n\nI'll be using `{brms}` for these models.\nHere's the gamm formula.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngam_formula <- bf(\n  F1 ~ voicing +\n    # the big interaction\n    t2(prop_t, log_dur_c, by = voicing) +\n    # random intercept\n    (1 | word) +\n    # random-factor smooth\n    s(prop_t, word, bs = \"fs\") +\n    # autoregressive term\n    # gr is the grouping term\n    ar(time = prop_t, gr = id)\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm(\n  gam_formula,\n  data = s03_ay_tomod,\n  cores = 4,\n  # no threading for a model with ar()\n  # threads = 2,\n  file = \"ay_gam_fit\",\n  #file_refit = \"always\",\n  backend = \"cmdstanr\",\n  control = list(adapt_delta = 0.99),\n  seed = this_seed\n) ->\n  ay_gam_fit\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## The full model summary, if you're into that sort of thing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_gam_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: F1 ~ voicing + t2(prop_t, log_dur_c, by = voicing) + (1 | word) + s(prop_t, word, bs = \"fs\") + ar(time = prop_t, gr = id) \n   Data: s03_ay_tomod (Number of observations: 920) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                                         Estimate Est.Error l-95% CI u-95% CI\nsds(t2prop_tlog_dur_cvoicingvoiced_1)        0.45      0.41     0.02     1.50\nsds(t2prop_tlog_dur_cvoicingvoiced_2)        2.82      1.13     1.13     5.45\nsds(t2prop_tlog_dur_cvoicingvoiced_3)        0.51      0.44     0.03     1.57\nsds(t2prop_tlog_dur_cvoicingvoiceless_1)     0.83      0.66     0.04     2.53\nsds(t2prop_tlog_dur_cvoicingvoiceless_2)     2.09      1.42     0.19     5.51\nsds(t2prop_tlog_dur_cvoicingvoiceless_3)     1.32      0.94     0.08     3.63\nsds(sprop_tword_1)                           0.76      0.10     0.59     0.95\nsds(sprop_tword_2)                           0.65      0.37     0.05     1.44\nsds(sprop_tword_3)                           1.46      0.39     0.78     2.33\n                                         Rhat Bulk_ESS Tail_ESS\nsds(t2prop_tlog_dur_cvoicingvoiced_1)    1.00      606     1640\nsds(t2prop_tlog_dur_cvoicingvoiced_2)    1.00     1373     1154\nsds(t2prop_tlog_dur_cvoicingvoiced_3)    1.00      862     1569\nsds(t2prop_tlog_dur_cvoicingvoiceless_1) 1.00     1194     1666\nsds(t2prop_tlog_dur_cvoicingvoiceless_2) 1.00     1641     1758\nsds(t2prop_tlog_dur_cvoicingvoiceless_3) 1.00      803     1468\nsds(sprop_tword_1)                       1.00     1057     1601\nsds(sprop_tword_2)                       1.02      610     1196\nsds(sprop_tword_3)                       1.01      981     1000\n\nCorrelation Structures:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nar[1]     0.92      0.02     0.88     0.96 1.00     2515     3064\n\nMultilevel Hyperparameters:\n~word (Number of levels: 19) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.04      0.03     0.00     0.13 1.00     1599     1864\n\nRegression Coefficients:\n                                     Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                                6.53      0.06     6.42     6.65 1.00\nvoicingvoiceless                        -0.32      0.12    -0.56    -0.10 1.00\nt2prop_tlog_dur_c:voicingvoiced_1        0.00      0.03    -0.05     0.05 1.00\nt2prop_tlog_dur_c:voicingvoiced_2       -0.14      0.03    -0.20    -0.07 1.00\nt2prop_tlog_dur_c:voicingvoiced_3        0.01      0.02    -0.02     0.05 1.00\nt2prop_tlog_dur_c:voicingvoiceless_1     0.09      0.08    -0.05     0.24 1.00\nt2prop_tlog_dur_c:voicingvoiceless_2    -0.00      0.06    -0.11     0.11 1.00\nt2prop_tlog_dur_c:voicingvoiceless_3    -0.09      0.05    -0.20     0.01 1.00\n                                     Bulk_ESS Tail_ESS\nIntercept                                1171     1758\nvoicingvoiceless                         1939     2885\nt2prop_tlog_dur_c:voicingvoiced_1        3016     2783\nt2prop_tlog_dur_c:voicingvoiced_2        2058     2756\nt2prop_tlog_dur_c:voicingvoiced_3        3384     2989\nt2prop_tlog_dur_c:voicingvoiceless_1     3245     3198\nt2prop_tlog_dur_c:voicingvoiceless_2     2515     2360\nt2prop_tlog_dur_c:voicingvoiceless_3     3027     3066\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.06      0.00     0.06     0.07 1.00     2629     2935\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n:::\n\nA quick look at the autocorrelation of the residuals looks ok\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nautocor_d = itsadug::acf_plot(\n  s03_ay_tomod$F1, \n  split_by = list(id = s03_ay_tomod$id),\n  return_all = T,\n  plot = F\n)\n\nautocor_r = itsadug::acf_plot(\n  resid(ay_gam_fit)[,1], \n  split_by = list(id = s03_ay_tomod$id),\n  return_all = T,\n  plot = F\n)\n\nlist(\n  data = autocor_d$dataframe,\n  residuals = autocor_r$dataframe\n) |> \n  list_rbind(\n    names_to = \"series\"\n  ) |> \n  group_by(lag, series) |> \n  mean_qi(acf, .width = pnorm(1) - pnorm(-1)) |> \n  ggplot(\n    aes(lag, acf, color = series)\n  ) +\n    geom_hline(yintercept = 0) +\n    geom_pointinterval(\n      aes(ymin = .lower, max = .upper),\n      position = position_dodge(width = 0.2)\n    ) +\n    labs(\n      caption = \"intervals = pnorm(1)-pnorm(-1)\"\n    ) ->\n  acf_p\n\nacf_p\nacf_p + theme_darkmode()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\nAnd the `ar` term is about where I'd expect it given previous experience:\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_gam_fit |> \n  gather_draws(ar[lag]) |> \n  ggplot(\n    aes(.value)\n  ) +\n  stat_slab(color = \"black\",linewidth = 0.5) +\n  scale_y_continuous(expand = expansion(0)) +\n  scale_x_continuous(\n    \"ar\",\n    limits = c(0.5, 1)\n  )->\n  ar_plot\n\nar_plot + theme_no_y()\nar_plot + theme_darkmode() + theme_no_y()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\n### Getting posterior draws\n\nI'll use the functions from `{marginaleffects}` to get expected posterior draws.\n\n::: callout-important\n## Important arguments\n\nI already knew I need to pass `re_formula = NA` to `predictions()`, but I had a hell of a time getting predictions out of a model with an `ar()` term until I came across [this thread](https://discourse.mc-stan.org/t/post-processing-model-with-autoregressive-correlation-structure-in-brms/20803), which informed me about `incl_autocor = FALSE`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_gam_fit |> \n  predictions(\n    newdata = datagrid(\n      log_dur_c = c(-0.5, 0.5),\n      voicing = c(\"voiced\", \"voiceless\"),\n      prop_t = seq(0, 1, length = 20)\n    ),\n    re_formula = NA,\n    incl_autocor = FALSE\n  ) |> \n  posterior_draws() ->\n  ay_gam_pred\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\" crop='true' renderings='[\"light\",\"dark\"]'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_gam_pred |> \n  ggplot(\n    aes(prop_t, draw)\n  )+\n  stat_lineribbon(\n    .width = 0.89,\n    aes(color = voicing, fill = voicing),\n    alpha = 0.4\n  )+\n  facet_wrap(~log_dur_c, labeller =  label_both) ->\n  smooth_plot\n\nsmooth_plot + theme(aspect.ratio = 0.681)\nsmooth_plot + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-2.png){fig-align='center' width=768}\n:::\n:::\n\n\n### Difference curves\n\nThere might be a fancy `{marginaleffects}` way to get difference curves, but I've found it easier to calculate them myself from the posterior draws.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_gam_pred |> \n  summarise(\n    .by = c(drawid, prop_t, log_dur_c),\n    diff = diff(draw)\n  ) ->\n  ay_voice_diff\n\n\nay_gam_pred |> \n  summarise(\n    .by = c(drawid, prop_t, voicing),\n    diff = diff(draw)\n  ) ->\n  ay_dur_diff\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_voice_diff |> \n  ggplot(\n    aes(prop_t, diff)\n  ) +\n  geom_hline(yintercept = 0) +\n  stat_lineribbon(\n    .width = 0.89,\n    color = ptol_red,\n    fill = ptol_red,\n    alpha = 0.4\n  ) +\n  labs(\n    y = \"voiceless - voiced\"\n  )+\n  facet_wrap(~log_dur_c, labeller = label_both)->\n  voice_diff\n\nvoice_diff + theme(aspect.ratio = 0.681)\nvoice_diff + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_dur_diff |> \n  ggplot(\n    aes(prop_t, diff)\n  ) +\n  geom_hline(yintercept = 0) +\n  stat_lineribbon(\n    .width = 0.89,\n    color = ptol_red,\n    fill = ptol_red,\n    alpha = 0.4\n  ) +\n  labs(\n    y = \"+1 log2(dur)\"\n  )+\n  facet_wrap(~voicing, labeller = label_both)->\n  dur_diff\n\ndur_diff + theme(aspect.ratio = 0.681)\ndur_diff + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-2.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Doing it with DCT parameters\n\nAn alternative approach to this data would be to first get the Discrete Cosine Transform parameters of each formant track curve, then model those parameters.\nThis is a form of \"functional data analysis.\" There's a whole suite of R packages for doing [functional data analysis](https://cran.r-project.org/web/views/FunctionalData.html) things (and I find them kind of overwhelming), and there's been some work on using different techniques in phonetic research (e.g. @gubianUsingFunctionalData2015).\nI'm not sure why fda methods haven't gotten more traction, but I suspect some of it could be due to the overwhelming feeling I have looking at the packages and documentation.\n\n### Preparing the data\n\nTo use DCT parameters in a regression, first I'll perform the DCT on the data with `tidynorm::reframe_with_dct()`, and the pivot the data wider.\nInstead of having an `F1` column, we'll have `F1_0`, `F1_1`, `F1_2`, `F1_3` and `F1_4`; one column for each DCT parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns03_ay_tomod |> \n  reframe_with_dct(\n    F1:F3,\n    .token_id_col = id,\n    .time_col = prop_t,\n    .order = 5\n  ) |> \n  pivot_wider(\n    names_from = .param,\n    values_from = F1:F3\n  ) ->\n  s03_ay_dct\n```\n:::\n\n\n### Specifying and fitting the model\n\nI've found [the brms vignette on fitting multivariate models](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html) helpful in figuring out this next step.\nWhen we have a model formula like this:\n\n``` r\nbf(F1 ~ duration)\n```\n\nWe're implicitly trying to model F1 as a kind of normal distribution, and the model will estimate a mean and variance for it.\nSomething like this:\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]'}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\nBut it's also possible to have multiple outcome variables on the left hand side of the formula:\n\n``` r\nbf(mvbind(F1, F2) ~ duration) + set_rescor(TRUE)\n```\n\n*This* formula is treating F1 and F2 as forming some kind of oval like shape (a multivariate normal) for which it will estimate a center point, how oblong it is, and its rotation.\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=60%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-2.png){fig-align='center' width=60%}\n:::\n:::\n\n\nThis notion can be expanded out beyond 2 dimensions, so we can specify a model of all 5 DCT parameters like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndct_formula <- bf(\n  mvbind(\n    F1_0, F1_1,\n    F1_2, F1_3,\n    F1_4\n  ) ~ voicing +\n    s(log_dur_c, by = voicing) + \n    (1|word)\n) +\n  set_rescor(TRUE)\n```\n:::\n\n\nSome things to notice\n\n-   Time is no longer a property of this model.\n    The shape of the trajectories over time is captured within the DCT parameters.\n\n-   I still have a non-linear smooth over duration, interacting with voicing.\n    I did this mainly so that this model would be most comparable to the gamm I fit above, but I could have also had simple linear terms interaction, like `log_dur_c * voicing`.\n\n-   I just have a simple random intercept by word.\n    In the gamm I needed to also have a random factor smooth by word, but here the random intercept itself will model word-level differences to the shape.\n\nNow all's that left is to fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm(\n  dct_formula,\n  data = s03_ay_dct,\n  cores = 4,\n  threads = 2,\n  file = \"ay_dct_fit\",\n  #file_refit = \"always\",\n  backend = \"cmdstanr\",\n  control = list(adapt_delta = 0.99),\n  seed = this_seed\n) ->\n  ay_dct_fit\n```\n:::\n\n\n### Getting predictions\n\nWe can get the predicted DCT parameters a lot like we did for the gamms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_dct_fit |> \n  predictions(\n    newdata = datagrid(\n      log_dur_c = c(-0.5, 0.5),\n      voicing = c(\"voiced\", \"voiceless\")\n    ),\n    re_formula = NA\n  ) |> \n  posterior_draws() ->\n  ay_dct_pred_init\n```\n:::\n\n\nIt's worth looking at what these predictions look like quickly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_dct_pred_init |> \n  select(\n    drawid, group, draw, log_dur_c, voicing\n  ) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  drawid group        draw log_dur_c   voicing\n1      1   F10  4.50421262      -0.5    voiced\n2      1   F10  4.47675843      -0.5 voiceless\n3      1   F10  4.51659168       0.5    voiced\n4      1   F10  4.53002865       0.5 voiceless\n5      1   F11 -0.01520459      -0.5    voiced\n6      1   F11 -0.01462255      -0.5 voiceless\n```\n\n\n:::\n:::\n\n\nTwo things to notice:\n\n-   The original column names `F1_0` etc have been squished together into `F10`.\n\n-   These aren't the predicted F1 trajectories, these are the predicted DCT coefficients.\n\nWe can deal with the smushed together column names with some tidyverse verbs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_dct_pred_init |> \n  separate_wider_position(\n    group,\n    widths = c(formant = 2, param = 1)\n  ) ->\n  ay_dct_pred\n\nay_dct_pred|> \n  select(\n    drawid, formant, param, draw, log_dur_c, voicing\n  ) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  drawid formant param    draw log_dur_c voicing  \n  <fct>  <chr>   <chr>   <dbl>     <dbl> <fct>    \n1 1      F1      0      4.50        -0.5 voiced   \n2 1      F1      0      4.48        -0.5 voiceless\n3 1      F1      0      4.52         0.5 voiced   \n4 1      F1      0      4.53         0.5 voiceless\n5 1      F1      1     -0.0152      -0.5 voiced   \n6 1      F1      1     -0.0146      -0.5 voiceless\n```\n\n\n:::\n:::\n\n\nOnly the first three DCT coefficients are easilly interpretable in and of themselves.\nTo get the formant curves, we need to apply the inverse DCT to these parameters, which we can do with `tidynorm::reframe_with_idct()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_dct_pred |> \n  reframe_with_idct(\n    draw,\n    .param_col = param,\n    .token_id_col = drawid,\n    .by = c(log_dur_c, voicing),\n    .n = 20\n  ) ->\n  ay_dct_smooth\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_dct_smooth |> \n  ggplot(\n    aes(.time, draw)\n  ) +\n  stat_lineribbon(\n    aes(color = voicing, fill = voicing),\n    .width = 0.89,\n    alpha = 0.4\n  ) +\n  facet_wrap(~log_dur_c, labeller = label_both)->\n  dct_plot\n\ndct_plot +  theme(aspect.ratio = 0.681)\ndct_plot + theme_darkmode() +  theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-2.png){fig-align='center' width=768}\n:::\n:::\n\n\n### Difference curves\n\nTo get difference curves, we can actually get the difference in DCT parameters between the levels of interest, and apply the IDCT to those differences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nay_dct_pred |> \n  summarise(\n    .by = c(drawid, param, log_dur_c),\n    diff = diff(draw)\n  ) |> \n  reframe_with_idct(\n    diff,\n    .token_id_col = drawid,\n    .param_col = param,\n    .by = log_dur_c\n  ) ->\n  ay_dct_voicing\n\n\nay_dct_pred |> \n  summarise(\n    .by = c(drawid, param, voicing),\n    diff = diff(draw)\n  ) |> \n  reframe_with_idct(\n    diff,\n    .token_id_col = drawid,\n    .param_col = param,\n    .by = voicing\n  ) ->\n  ay_dct_duration\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_dct_voicing |> \n  ggplot(\n    aes(.time, diff)\n  ) + \n  geom_hline(\n    yintercept = 0\n  ) +\n  stat_lineribbon(\n    color = ptol_red,\n    fill = ptol_red,\n    alpha = 0.4,\n    .width = 0.89\n  ) +\n  labs(\n    y = \"voiceless-voiced\"\n  ) +\n  facet_wrap(\n    ~log_dur_c, labeller = label_both\n  ) ->\n  dct_voice_p\n\ndct_voice_p + theme(aspect.ratio = 0.681)\ndct_voice_p + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-2.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nay_dct_duration |> \n  ggplot(\n    aes(.time, diff)\n  ) + \n  geom_hline(\n    yintercept = 0\n  ) +\n  stat_lineribbon(\n    color = ptol_red,\n    fill = ptol_red,\n    alpha = 0.4,\n    .width = 0.89\n  ) +\n  labs(\n    y = \"+1 log2(dur)\"\n  ) +\n  facet_wrap(\n    ~voicing, labeller = label_both\n  ) ->\n  dct_dur_p\n\ndct_dur_p + theme(aspect.ratio = 0.681)\ndct_dur_p + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-2.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Comparing the smooths\n\nNow, I don't want to compare these smooths in *too* much detail, because the models are so different.\nBut for a quick qualitative look, here they are:\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlist(\n  gam = ay_gam_pred,\n  dct = ay_dct_smooth |> \n    mutate(prop_t = (.time-1)/19)\n) |> \n  list_rbind(\n    names_to = \"model\"\n  ) |> \n  ggplot(\n    aes(prop_t, draw)\n  ) + \n  stat_lineribbon(\n    aes(color = model, fill = model),\n    .width = 0.89,\n    alpha = 0.4\n  ) + \n  facet_grid(\n     voicing ~ log_dur_c,\n     labeller = label_both\n  )->\n  curve_comp\n\ncurve_comp + theme(aspect.ratio = 0.681)\ncurve_comp + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-2.png){fig-align='center' width=768}\n:::\n:::\n\n\nThe curves, especially for the pre-voiceless cases, look pretty different.\nWhich one is \"better\" would require more time than I have here.\nReally the only way to judge would be to compare these smooths to the token level formant tracks.\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\ns03_ay_tomod |> \n  ggplot(\n    aes(prop_t, F1)\n  ) + \n  geom_line(\n    aes(group = id, color = id),\n    alpha = 0.8,\n    linewidth = 0.5\n  ) +\n  scale_color_scico(palette = \"hawaii\") +\n  guides(color = \"none\") +\n  facet_wrap(~voicing) ->\n  ay_tracks\n\nay_tracks + theme(aspect.ratio = 0.681)\nay_tracks + theme_darkmode() + theme(aspect.ratio = 0.681)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-2.png){fig-align='center' width=768}\n:::\n:::\n\n\nIMO: I think the sort of \"flat across the top\" shape of the DCT smooths match up better with how the pre-voiceless tokens look.\n\nAnother comparison that's useful to look at is the time it took to fit the two models\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nrstan::get_elapsed_time(ay_gam_fit$fit) |> \n  as_tibble(\n    rownames = \"chain\"\n  ) ->\n  gam_time\n\nrstan::get_elapsed_time(ay_dct_fit$fit) |> \n  as_tibble(\n    rownames = \"chain\"\n  ) ->\n  dct_time\n\nlist(\n  gam = gam_time,\n  dct = dct_time\n) |> \n  list_rbind(\n    names_to = \"model\"\n  ) |> \n  ggplot(\n    aes(warmup, sample)\n  ) + \n  geom_point(\n    aes(color = model),\n    size = 3\n  )+\n  coord_fixed() ->\n  time_comp\n\ntime_comp\ntime_comp + theme_darkmode()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\nOverall, the multivariate model over DCTs too less time.\n\n## When to *not* use DCTs\n\nLet's say, you wanted to find out of there was, say, a trend over the entire recording for the value of F1.\nI'll grab the midpoint of these tracks to plot it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspeaker_tracks |> \n  filter(\n    speaker == \"s03\"\n  ) |> \n  slice(\n    .by = id,\n    round(n()/2)\n  ) |> \n  mutate(across(F1:F3, log))->\n  vowel_mids\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nvowel_mids |> \n  ggplot(\n    aes(t, F1)\n  ) +\n  geom_point() ->\n  mid_trend\n\nmid_trend\nmid_trend + theme_darkmode()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-2.png){fig-align='center' width=80%}\n:::\n:::\n\n\nIn this case, each data point *isn't* part of some cohesive shape, and the observations aren't evenly spaced apart.\nHere, a good-old gamm makes sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf1_formula <- bf(\n  F1 ~ s(t) + (1|vowel) + (1|word)\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm(\n  f1_formula,\n  data = vowel_mids,\n  cores = 4,\n  threads = 2,\n  file = \"f1_model\",\n  #file_refit = \"always\",\n  backend = \"cmdstanr\",\n  control = list(adapt_delta = 0.99),\n  seed = this_seed\n) ->\n  f1_model\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf1_model |> \n  predictions(\n    newdata = datagrid(\n      t = \\(x) quantile(x, ppoints(500))\n    ),\n    re_formula = NA\n  ) |> \n  posterior_draws() ->\n  f1_pred\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\" renderings='[\"light\",\"dark\"]' crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nf1_pred |> \n  ggplot(\n    aes(t, draw)\n  ) +\n  stat_ribbon(\n    alpha = 0.1,\n    color = ptol_red,\n    fill = ptol_red,\n    .width = seq(0.1, 0.9, length = 20),\n    show.legend = F\n  ) ->\n  f1_smooth\n\nf1_smooth\nf1_smooth + theme_darkmode()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-2.png){fig-align='center' width=80%}\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}