[
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "",
    "text": "I’ve been working a lot with the Discrete Cosine Transform in python, specifically as it’s implemented in scipy. But, I really prefer doing my data munging and stats in R.1 What to do!\nI knew the answer rested in using the reticulate package, which lets you communicate back and forth between python and R, but I hadn’t appreciated how cool reticulate was, which is why I’m making this blog post.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#setup",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#setup",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Setup",
    "text": "Setup\n\n\nr\n\nIrrelevant R setuplibrary(tidyverse)\nlibrary(geomtextpath)\nlibrary(gt)\nsource(here::here(\"_defaults.R\"))\n\n\nIn order to communicate back and forth, I’ll need to load the reticulate package.\n\n\nr\n\nlibrary(reticulate)\n\nI’ll also need to make sure that I’ve got scipy installed for python, which you can do with reticulate::py_install().\n\n\nr\n\nreticulate::py_install(\"scipy\")\n\n\n\n\n\n\n\nPython environments\n\n\n\n\n\nPython environments are kind of notorious for being confusing to keep straight, which is why a whole host of tools for managing how python is installed have cropped up. Inside all of my R projects, I already use renv, which has an option to also manage your python environment for a project like so:\n\nr\n\nrenv::use_python()\nIf we pull up the python configuration for reticulate, we can see it’s installed in the local renv project.\n\n\nr\n\nreticulate::py_config()$python\n\n[1] \"/Users/joseffruehwald/Documents/blog/renv/python/virtualenvs/renv-python-3.11/bin/python\"\n\n\nBut, if you have a favorite other way of managing your python environments, there are ways to point reticulate at those too.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#background-what-is-the-dct",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#background-what-is-the-dct",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Background: What is the DCT?",
    "text": "Background: What is the DCT?\nThe Discrete Cosine Transform is very similar to the Fourier Transform (if that helps). It takes in a signal of wiggly data, and re-describes it in terms of weights on cosine functions of increasing frequency. Figure 1 plots the first DCT functions as they’re defined by a particular set of options in scipy.fft.dct.\n\n\n\n\n\n\n\nFigure 1: The first 5 cosine functions of the DCT.\n\n\n\n\nIf you use the same number of cosine functions as you had data points in the original signal, you can fully reconstruct the original signal. Or, if you use just a few (like 5 in this figure), it has the effect of smoothing the signal when you invert the DCT.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#option-1-passing-data-back-and-forth",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#option-1-passing-data-back-and-forth",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Option 1: Passing Data back and forth",
    "text": "Option 1: Passing Data back and forth\nSo, the fasttrackpy package gives you the option of saving DCT parameters to a csv file. I’ll load an example into R, and grab the rows for one example vowel so we can see what it looks like.\n\n\nr\n\n# Reading in the data\ndct_params &lt;- read_csv(                      \n  \"data/josef-fruehwald_speaker_param.csv\",  \n  col_types = cols()                         \n)                                            \n\n# getting the first token's id\nfirst_id &lt;- dct_params$id[1]                 \n\n# subsetting to get just\n# the first token's parameters\nfirst_df &lt;- dct_params |&gt;                    \n  filter(                                    \n    id == first_id                           \n  )                                          \n\n\n\nr\n\nTable Codefirst_df |&gt; \n  select(\n    label,\n    word,\n    param,\n    F1:F3\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(\n    columns = F1:F3\n  )\n\n\n\n\n\nlabel\nword\nparam\nF1\nF2\nF3\n\n\n\nay0\nsunlight\n0\n322.45\n1,178.19\n1,753.24\n\n\nay0\nsunlight\n1\n30.34\n−159.93\n55.47\n\n\nay0\nsunlight\n2\n−0.73\n−11.40\n23.71\n\n\nay0\nsunlight\n3\n2.25\n−15.00\n3.98\n\n\nay0\nsunlight\n4\n−5.07\n4.76\n4.59\n\n\n\n\n\n\nThese DCT parameters don’t look like much on their own. That’s even clearer if we plot them.\n\n\nr\n\nPlotting codefirst_df |&gt; \n  pivot_longer(\n    F1:F3\n  ) |&gt; \n  ggplot(\n    aes(param, value)\n  )+\n    geom_hline(\n      yintercept = 0\n    )+\n    geom_point(\n      aes(color = factor(param)),\n      size = 3\n    )+\n    guides(\n      color = \"none\"\n    )+\n    labs(\n      y = NULL\n    )+\n    facet_wrap(~name)+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\n\n\nFigure 2: DCT parameters for one vowel token\n\n\n\n\nTo get these values back into something interpretable, we need to apply the inverse discrete cosine transform. To do that, we can\n\npass these parameter values over to python,\napply scipy.fft.idct to them to get back formant-like values\npass the results back to R.\n\nPassing data to python\nTo do this, first I’m going to assign each set of parameters to a variable in R.\n\n\nr\n\nF1_param &lt;- first_df$F1\nF2_param &lt;- first_df$F2\nF3_param &lt;- first_df$F3\n\nHaving loaded reticulate before, any variable we’ve created in R are available in Python within an r object.\n\n\npython\n\nr.F1_param\n\n[322.4520974990528, 30.339268532723658, -0.7277856792300109, 2.25340821466954, -5.069135079372835]\n\n\nNow, we just need to import the idct function and apply it to each of these sets of parameters.\nApplying idct\n\n\npython\n\nfrom scipy.fft import idct\n\n\n\npython\n\nF1_expanded = idct(\n  r.F1_param,\n  n = 100,\n  orthogonalize = True,\n  norm = \"forward\"\n)\n\nF2_expanded = idct(\n  r.F2_param,\n  n = 100,\n  orthogonalize = True,\n  norm = \"forward\"\n)\n\nF3_expanded = idct(\n  r.F3_param,\n  n = 100,\n  orthogonalize = True,\n  norm = \"forward\"\n)\n\nPassing data back to R\nNow, we can get these expanded values back in R from an object called py.\n\n\nr\n\nhead(\n  py$F1_expanded\n)\n\n[1] 509.6159 509.6814 509.8102 509.9982 510.2390 510.5247\n\n\nI’ll pop these all into a tibble.\n\n\nr\n\nfirst_expanded &lt;- tibble(\n  F1 = py$F1_expanded,\n  F2 = py$F2_expanded,\n  F3 = py$F3_expanded\n) |&gt; \n  mutate(\n    prop_time = (row_number() - 1)/(n()-1)\n  )\n\n\n\nr\n\nPlotting codefirst_expanded |&gt; \n  pivot_longer(\n    F1:F3,\n    names_to = \"formant\",\n    values_to = \"frequency\"\n  ) |&gt; \n  ggplot(\n    aes(prop_time, frequency, color = formant)\n  )+\n    geom_textpath(\n      aes(label = formant),\n      linewidth = 1\n    )+\n    guides(\n      color = \"none\"\n    )+\n    labs(\n      x = \"proportional time\"\n    )+\n    expand_limits(y = 0)\n\n\n\n\n\n\nFigure 3: Inverse DCT formant results\n\n\n\n\nShortcomings\n\nThat was a lot of code to get back the formant-like values for just one token!\nIt’s not taking advantage of the really cool averaging properties of the DCT.\nIt didn’t fit into my nice tidyverse workflows at all!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#option-2-using-python-functions-inside-r",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#option-2-using-python-functions-inside-r",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Option 2: Using Python functions inside R",
    "text": "Option 2: Using Python functions inside R\nRather than passing data back and forth directly, instead, I’ll import the scipy function directly into R.\n\n\nr\n\nscipy &lt;- reticulate::import(\"scipy\")\nidct &lt;- scipy$fft$idct\n\nNow, we can use idct() (almost) like an R function. Here’s how it looks on one of the variables we created before.\n\n\nr\n\nnew_F1_expanded &lt;- idct(\n  F1_param,\n  n = 100L,\n  orthogonalize = TRUE,\n  norm = \"forward\"\n)\n  \nhead(new_F1_expanded)\n\n[1] 509.6159 509.6814 509.8102 509.9982 510.2390 510.5247\n\n\nI’ll combine this with the handy-dandy tidyverse functions across and reframe to get average formant trajectories.\nStep 1: Getting the average of the DCT parameters by token.\nWithsummarise() and across(), we’ll get the mean of the parameter values for F1, F2 and F3, grouped by label and parameter.\n\n\nr\n\ndct_params |&gt; \n  summarise(\n    across(\n      F1:F3, mean\n    ),\n    .by = c(label, param)\n  )-&gt;\n  dct_averages\n\nhead(dct_averages)\n\n# A tibble: 6 × 5\n  label param      F1      F2      F3\n  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 ay0       0 338.    1130.   1659.  \n2 ay0       1  33.8   -136.     26.9 \n3 ay0       2  -5.53    -5.95    5.37\n4 ay0       3  -0.580  -11.6    -1.87\n5 ay0       4  -1.20    -2.15   -3.60\n6 ey        0 274.    1380.   1736.  \n\n\nStep 2: Apply idct to the averages\nNow, I’ll use reframe() and across() to get the formant-like values from these averages\n\n\nr\n\ndct_averages |&gt; \n  reframe(\n    across(\n      F1:F3,\n      ~idct(\n        .x, \n        n = 100L,\n        orthogonalize = T, \n        norm = \"forward\"\n      )\n    ),\n    .by = label\n  ) |&gt; \n  mutate(\n    prop_time = (row_number()-1)/(n()-1),\n    .by = label\n  )-&gt;\n  average_smooths\n\nhead(average_smooths)\n\n# A tibble: 6 × 5\n  label        F1        F2        F3 prop_time\n  &lt;chr&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt;     &lt;dbl&gt;\n1 ay0        532.     1287.     2400.    0     \n2 ay0        532.     1287.     2400.    0.0101\n3 ay0        532.     1288.     2400.    0.0202\n4 ay0        532.     1290.     2400.    0.0303\n5 ay0        532.     1292.     2401.    0.0404\n6 ay0        532.     1295.     2401.    0.0505\n\n\nStep 3: Make some good plots\nHere’s a plot of the expanded formant trajectories for some of the more dynamic vowels.\n\n\nr\n\nPlotting codeaverage_smooths |&gt; \n  filter(\n    label %in% c(\n      \"iy\",\n      \"ey\",\n      \"ay\",\n      \"ay0\",\n      \"aw\",\n      \"Tuw\",\n      \"owL\"\n    )\n  ) |&gt; \n  ggplot(\n    aes(F2, F1, color = label)\n  )+\n    geom_textpath(\n      aes(\n        group = label,\n        label = label\n      ),\n      arrow = arrow(\n        type = \"closed\",\n        length = unit(0.2, \"cm\")\n      ),\n      linewidth = 1\n    )+\n    scale_y_reverse()+\n    scale_x_reverse()+\n    guides(\n      color = \"none\"\n    )+\n    theme(aspect.ratio = 1)\n\n\n\n\n\n\nFigure 4: Average formant trajectories",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#a-note",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#a-note",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "A note",
    "text": "A note\nUsually when you see a plot like Figure 4, it’s the result of some fairly complicated model fitting. But take a look through the source code here! Not a gam in sight! Just averaging, and the application of the idct!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#footnotes",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#footnotes",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Footnotes",
    "text": "Footnotes\n\npolars is growing on me, but I don’t think in it yet.↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html",
    "title": "R Package Exploration (Jan 2023)",
    "section": "",
    "text": "As I scroll through my feeds, I often come across a really cool looking package, or a new feature of a package, that I think looks really cool, and then I forget to go back to really kick the tires to see how it works. So I’ve decided to try to set up a workflow where I send the docs or pkgdown pages for the package to a Trello board, and then come back maybe once a month and experiment with them in a blog post.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-ggdensity-and-geomtextpath",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-ggdensity-and-geomtextpath",
    "title": "R Package Exploration (Jan 2023)",
    "section": "\n{ggforce}, {ggdensity} and {geomtextpath}\n",
    "text": "{ggforce}, {ggdensity} and {geomtextpath}\n\nThe packages I want to mess around with today are all extensions to ggplot2, so I’ll load up the palmerpenguins dataset for experimentation.\n\n## setup\nlibrary(tidyverse)\nlibrary(khroma)\nlibrary(palmerpenguins)\n\n## exploration packages\nlibrary(ggforce)\nlibrary(ggdensity)\nlibrary(geomtextpath)",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-and-convex-hulls",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-and-convex-hulls",
    "title": "R Package Exploration (Jan 2023)",
    "section": "\n{ggforce} and convex hulls",
    "text": "{ggforce} and convex hulls\nThe ggforce package as the option to add a convex hull over your data (ggforce::geom_mark_hull()), kind of indicating where the data clusters are. Here’s my base plot.\n\nplot1 &lt;- \n  penguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(bill_length_mm, bill_depth_mm, color = species))+\n    geom_point()+\n    scale_color_brewer(palette = \"Dark2\")+\n    scale_fill_brewer(palette = \"Dark2\")\nplot1\n\n\n\n\n\n\nFigure 1: The base penguins scatterplot\n\n\n\n\nI’ll throw on the default convex hull.\n\nplot1 +\n  geom_mark_hull()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\nFigure 2: Basic convex hull\n\n\n\n\nDefault is ok, but for this data set, the hulls are a bit jagged. That can be adjusted with the concavity argument. I’ll also throw in a fill color.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(\n      fill = species\n    )\n  )\n\n\n\n\n\n\nFigure 3: Smoothed out and filled convex hulls\n\n\n\n\nThat’s better. It also comes with a mappable label and description aesthetics. Here, it seems a bit more touchy.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(fill = species,\n        label = species,\n    ),\n    label.family = \"Fira Sans\"\n  )\n\n\n\n\n\n\nFigure 4: Attempted labelling of convex hulls\n\n\n\n\nThe labels actually appear in the RStudio IDE for me, but not in the rendered page here because it wants more headroom around the plot. I’ll add that in by setting the expand arguments to ggplot::scale_y_continuous() and ggplot::scale_x_continuous(), and I’ll drop the legend while I’m at it.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(fill = species,\n        label = species,\n    ),\n    label.family = \"Fira Sans\"\n  )+\n  scale_y_continuous(\n    expand = expansion(\n      mult = c(0.25, 0.25)\n    )\n  )+\n  scale_x_continuous(\n    expand = expansion(\n      mult = c(0.25, 0.25)\n    )\n  ) +\n  guides(\n    color = \"none\",\n    fill = \"none\"\n  )\n\n\n\n\n\n\nFigure 5: Labelled convex hulls\n\n\n\n\nThoughts\nI like the convex hulls as a presentational aide. It probably shouldn’t be taken as a statistical statement about, for example the degree of overlap between these three species, but is useful for outlining data points of interest.\nI kind of wish this was separated out into a few different, more conventional, ggplot2 layers. It’s called a geom_ but the convex hulls are definitely stat_s. The convex hull statistic layer isn’t exposed to users, so you can’t mix-and-match convex hull estimation and the geom used to draw it. On the other hand, I can see that it’s much more souped up than a typical geom. For example, you can filter the data within the aes() mapping.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(\n      filter = sex == \"female\"\n    )\n  )\n\n\n\n\n\n\nFigure 6: Filtered convex hulls",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggdensity",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggdensity",
    "title": "R Package Exploration (Jan 2023)",
    "section": "{ggdensity}",
    "text": "{ggdensity}\nAs pointed out on the ggdensity readme, there’s already a stat+geom in ggplot2 to visualize 2d density plots.\n\nplot2 &lt;- \n  penguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(bill_length_mm, bill_depth_mm))\n\nplot2 +\n  stat_density_2d_filled()\n\n\n\n\n\n\nFigure 7: Density contour plot\n\n\n\n\nThose levels are a little hard to follow, though, which is what ggdensity::stat_hdr() is for. It will plot polygons/contours for given probability levels, of the data distribution\n\nplot2 +\n  stat_hdr()\n\n\n\n\n\n\nFigure 8: Highest density region contour plot\n\n\n\n\nThe probabilities are mapped to transparency by default, so you can map the fill color to a different dimension.\n\nplot2 +\n  stat_hdr(aes(fill = species))+\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\nFigure 9: Highest density region contour plot, filled by species\n\n\n\n\nThe package also has a ggdensity::stat_hdr_rug() to add density distribution rugs to plots.\n\nplot2 +\n  geom_point()+\n  stat_hdr_rug(fill = \"grey90\")\n\n\n\n\n\n\nFigure 10: HDR rug",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#geomtextpath",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#geomtextpath",
    "title": "R Package Exploration (Jan 2023)",
    "section": "{geomtextpath}",
    "text": "{geomtextpath}\nI’ve actually been messing around with this for a bit, but geomtextpath allows you to place text along lines. There’s standalone geom_textpath() and geom_labelpath() functions, but just to stick with the penguins data, I’m going to match the textpath geom with a different stat.\n\nplot3 &lt;-\n  penguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(bill_length_mm, bill_depth_mm, color = species))+\n    scale_color_brewer(palette = \"Dark2\")\n\nplot3 +\n  stat_smooth(\n    geom = \"textpath\", \n    # you have to map a label aesthetic\n    aes(label = species),\n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\nFigure 11: Trendlines with text written along them\n\n\n\n\nYou can move the location of the text on the path back and forth by either setting or mapping hjust to a number between 0 and 1, and you can lift the text off the line with vjust.\n\nplot3 +\n  stat_smooth(\n    geom = \"textpath\", \n    # you have to map a label aesthetic\n    aes(label = species),\n    hjust = 0.1,\n    vjust = -1\n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\nFigure 12: Trendlines with text written along them\n\n\n\n\nMixing and matching statistics and these direct labels could get pretty powerful. For example, here’s the name of each species written around data ellipses.\n\nplot3 +\n  stat_ellipse(\n    geom = \"textpath\", \n    # you have to map a label aesthetic\n    aes(label = species),\n    hjust = 0.1  \n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\nFigure 13: Data ellipses text written along them",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#combo-ggdensity-and-geomtextpath",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#combo-ggdensity-and-geomtextpath",
    "title": "R Package Exploration (Jan 2023)",
    "section": "Combo {ggdensity} and {geomtextpath}\n",
    "text": "Combo {ggdensity} and {geomtextpath}\n\nSince the ggdensity statistics are ordinary stat_, we can also combine them with textpaths to label the probability levels directly.\n\nplot2 +\n  stat_hdr_lines(\n    aes(label = after_stat(probs)),\n    color = \"grey90\",\n    geom = \"textpath\"\n  ) +\n  guides(alpha = \"none\")\n\n\n\n\n\n\nFigure 14: Higest density region plot with direct labels",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "",
    "text": "I’m still thinking about priors, distributions, and logistic regressions. The fact that a fairly broad normal distribution in logit space turns into a bimodal distribution in probability space has got me thinking about the standard deviation of random effects in logistic regression. Specifically, what happens in cases where the population of individuals may be bimodal\nsetuplibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(marginaleffects)\n\nlibrary(gt)\n\nsource(here::here(\"_defaults.R\"))\n\nseed &lt;- 2023-6-29\n\nset.seed(seed)",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#simulating-some-data",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#simulating-some-data",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Simulating some data",
    "text": "Simulating some data\nI’ll kick things off with simulating some data. Our predictor variable will be just randomly sampled from ~normal(0,1), so it’ll handily already be z-scored. I’ll also go for a slope in logit space of 1, so logit(y) = x.\nI’ll treat each point I’ve sampled for X as belonging to an individual, or subject, grouping variable, and each individual’s personal probability will be sampled from a beta distribution. I’ll simulate two possibilities here, one where individuals are kind of closely clustered near each other, and another where they’re pretty strongly bifurcated close to 0 and 1.1\n\nsimulation of individualstibble(\n  x = rnorm(100),\n  logit = x,\n  prob = plogis(logit)\n) |&gt; \n  mutate(\n    individual = row_number(),\n    subj_prob_low = rbeta(\n      n = n(), \n      prob * 0.25, \n      (1-prob) * 0.25\n    ),\n    subj_prob_high = rbeta(\n      n = n(), \n      prob * 7, \n      (1-prob) * 7\n      )\n  )-&gt;\n  sim_params\n\n\n\nplotting codesim_params |&gt; \n  pivot_longer(\n    starts_with(\"subj_prob\")\n  ) |&gt; \n  mutate(\n    name = case_when(\n      name == \"subj_prob_high\" ~ \"unimodal\",\n      name == \"subj_prob_low\" ~ \"bifurcated\"\n    )\n  ) |&gt; \n  ggplot(aes(x, value))+\n    geom_point()+\n    labs(title = \"subject-level probabilities\",\n         y = \"prob\")+\n    scale_x_continuous(\n      breaks = seq(-2,2, by = 2)\n    ) +  \n    facet_wrap(~name)\n\n\n\n\n\n\nFigure 1: Probabilities for individuals\n\n\n\n\nFor each of these probabilities, I’ll simulate 50 binomial observations.\n\nsimulating utterancessim_params |&gt; \n  rowwise() |&gt; \n  mutate(\n    obs = list(tibble(\n      y_prob_low = rbinom(n = 50, size = 1, prob = subj_prob_low),\n      y_prob_high = rbinom(n = 50, size = 1, prob = subj_prob_high)\n    ))\n  ) |&gt; \n  unnest(obs) -&gt;\n  sim_obs\n\n\n\nplotting codesim_obs |&gt; \n  pivot_longer(\n    starts_with(\"y_\"),\n    names_to = \"simulation\",\n    values_to = \"observation\"\n  ) |&gt; \n  mutate(\n    simulation = case_when(\n      simulation == \"y_prob_high\" ~ \"unimodal\",\n      simulation == \"y_prob_low\" ~ \"bifurcated\"\n    )\n  ) |&gt; \n  ggplot(aes(x, factor(observation))) +\n    stat_sum(alpha = 0.3)+\n    labs(title = \"simulated observations\")+\n    scale_x_continuous(\n      breaks = seq(-2,2, by = 2)\n    ) +\n    facet_wrap(~simulation)\n\n\n\n\n\n\nFigure 2: simulated observations",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#looking-at-the-default-priors",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#looking-at-the-default-priors",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Looking at the default priors",
    "text": "Looking at the default priors\nIf we take a look at the default priors a logistic model would get in brms, we can see that both the Intercept and the slope get pretty broad priors (the blank prior for the slope means it’s a flat prior).\n\nget_prior(\n  bf(y_prob_low ~ x + (1|individual)),\n  data = sim_obs,\n  family = bernoulli(link = \"logit\")\n) |&gt; \n  as_tibble() |&gt; \n  select(prior, class, coef, group) |&gt; \n  gt()\n\n\n\n\n\nprior\nclass\ncoef\ngroup\n\n\n\n\nb\n\n\n\n\n\nb\nx\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\nsd\n\nindividual\n\n\n\nsd\nIntercept\nindividual\n\n\n\n\n\n\nIf we real quick look at how the prior on the intercept plays out in the probability space, we get one of these bimodal distributions.\n\nplotting codetibble(\n  x = rstudent_t(1e6, df = 3, sigma = 2.5)\n) |&gt; \n  ggplot(aes(plogis(x))) +\n    stat_slab(fill = ptol_red)+\n    theme_no_y()+\n    scale_y_continuous(\n      expand = expansion(mult = 0)\n    )+\n    labs(title = \"invlogit(student_t(3, 0, 2.5))\",\n         x = NULL)\n\n\n\n\n\n\nFigure 3: Student-t prior in the probability space.\n\n\n\n\nSo, for the intercept and slope priors, I’ll adjust them to be ~normal(0, 1.5) and ~normal(0,1), respectively.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#actually-fitting-the-models.",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#actually-fitting-the-models.",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Actually fitting the models.",
    "text": "Actually fitting the models.\nBimodal population\nFirst, here’s the model for the population where individuals’ probabilities were squished out towards 0 and 1.\n\nlow_mod &lt;- brm(\n  y_prob_low ~ x + (1|individual),\n  data = sim_obs,\n  family = bernoulli(link = \"logit\"),\n  prior = c(\n    prior(normal(0,1.5), class = \"Intercept\"),\n    prior(normal(0,1), class = \"b\", coef = \"x\")\n  ),\n  cores = 4, \n  seed = seed,  \n  backend = \"cmdstanr\",\n  file = \"low_mod.RDS\"\n) \n\n\nsummary tablelow_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mean_hdci() |&gt; \n  select(.variable, .value, .lower, .upper) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n.variable\n.value\n.lower\n.upper\n\n\n\nb_Intercept\n0.60\n−1.04\n2.13\n\n\nb_x\n3.01\n1.62\n4.36\n\n\nsd_individual__Intercept\n7.85\n5.85\n10.21\n\n\n\n\n\n\nThings have kind of clearly gone off the rails here. The intercepts and slopes are all over the place, but that’s maybe not surprising given the trade offs the model is making between the population level slopes and the individual level probabilities. It’s worth noting that this model had no diagnostic warnings and was well converged.\n\nplotting codelow_mod |&gt; \n  predictions(\n    newdata = datagrid(x = seq(-3, 3, length = 100)),\n    re_formula = NA\n  ) |&gt;\n  posterior_draws() |&gt; \n  ggplot(aes(x, draw)) +\n    stat_lineribbon(linewidth = 0.5)+\n    scale_fill_brewer() +\n    labs(\n      title = \"Bifurcated population\",\n      y = \"prob\"\n    )\n\n\n\n\n\n\nFigure 4: The posterior fitted values.\n\n\n\n\nIn fact, that posterior distribution for the between-speaker sd is very extreme at about 8. If we plot the kind of distribution of individuals it suggests when the population level probability = 0.5, we get those steep walls near 0 and 1 again.\n\nplotting codelow_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    regex = T\n  ) |&gt; \n  slice_sample(\n    n = 10\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(\n    individuals = list(tibble(\n      individual = rnorm(1e5, mean = 0, sd=.value)\n    ))\n  ) |&gt; \n  unnest(individuals) |&gt; \n  ggplot(aes(plogis(individual)))+\n    stat_slab(\n      aes(group = factor(.value)),\n      linewidth = 0.5,\n      fill = NA,\n      color = ptol_red\n    ) +\n    scale_y_continuous(expand = expansion(mult = 0))+\n    labs(\n      title = \"Random intercepts distribution around 0.5\"\n    )+\n    theme_no_y()\n\n\n\n\n\n\nFigure 5: Implied distribution of individuals in the bifircated population.\n\n\n\n\nThe Unimodal Population\nLet’s do it all again, but now for the population where individuals’ probabilities were clustered around the population probability.\n\nhigh_mod &lt;- brm(\n  y_prob_high ~ x + (1|individual),\n  data = sim_obs,\n  family = bernoulli(link = \"logit\"),\n  prior = c(\n    prior(normal(0,1.5), class = \"Intercept\"),\n    prior(normal(0,1), class = \"b\", coef = \"x\")\n  ),\n  cores = 4,\n  adapt_delta = 0.9,\n  seed = seed,\n  backend = \"cmdstanr\",\n  file = \"high_mod.RDS\"\n) \n\n\nsummary tablehigh_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mean_hdci() |&gt; \n  select(\n    .variable, .value, .lower, .upper\n  ) |&gt; \n  gt() |&gt; \n   fmt_number(decimals = 2)\n\n\n\n\n\n.variable\n.value\n.lower\n.upper\n\n\n\nb_Intercept\n−0.06\n−0.23\n0.10\n\n\nb_x\n1.05\n0.87\n1.22\n\n\nsd_individual__Intercept\n0.77\n0.63\n0.91\n\n\n\n\n\n\nThe intercepts and slope posteriors are much more tight, and the inter-speaker sd posterior is &lt;1.\n\nplottng codehigh_mod |&gt; \n  predictions(\n    newdata = datagrid(x = seq(-3, 3, length = 100)),\n    re_formula = NA\n  ) |&gt; \n  posterior_draws() |&gt; \n  ggplot(aes(x, draw)) +\n    stat_lineribbon(linewidth = 0.5)+\n    scale_fill_brewer()+\n    labs(\n      title = \"Unimodal population\",\n      y = \"prob\"\n    )\n\n\n\n\n\n\nFigure 6: Posterior fitted values for the unimodal population.\n\n\n\n\nLet’s look at the implied individual level distribution around 0.5.\n\nplotting codehigh_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    regex = T\n  ) |&gt; \n  slice_sample(\n    n = 20\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(\n    individuals = list(tibble(\n      individual = rnorm(1e5, mean = 0, sd=.value)\n    ))\n  ) |&gt; \n  unnest(individuals) |&gt; \n  ggplot(aes(plogis(individual)))+\n    stat_slab(\n      aes(group = factor(.value)),\n      linewidth = 0.5,\n      fill = NA,\n      color = ptol_red\n    ) +\n    scale_y_continuous(expand = expansion(mult = 0))+\n    labs(\n      title = \"Random intercepts distribution around 0.5\"\n    )+\n    theme_no_y()\n\n\n\n\n\n\nFigure 7: Implied distribution of individuals in the unimodal population.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#the-upshot",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#the-upshot",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "The Upshot",
    "text": "The Upshot\nEven though I’ve fit and looked at hierarchical logistic regressions before, I hadn’t stopped to think about how to interpret the standard deviation of the random intercepts before. If you’d asked me before what a large sd implied about the distribution of individuals in the probability space, I think I would have said they’d be more uniformly distributed, but actually it means they’re more bifurcated!\nAlso, if you’ve got a fairly bifurcated population, the population level estimates are going to get pretty wonky.\nAll food for thought moving forward.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#footnotes",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#footnotes",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially also tried simulating a case where individuals were strictly categorical based on the probability associated with their x, and things did not go well for the models.↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "",
    "text": "Last semester, I spent time co-developing some python packages:\nSo, I thought I’d share a little walkthrough of a cool way to use them. They can both be installed with pip.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#fasttrackpy",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#fasttrackpy",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "FastTrackPy",
    "text": "FastTrackPy\nfasttrackpy (Fruehwald and Barreda 2023) is a python implementation of Santiago Barreda’s Praat plugin (Barreda 2021). Right now, its design is really geared towards command line usage, and has three different subcommands\n\n\nfasttrack audio\n\nThis will run fasttrack on a single audio file or a directory of audio files\n\n\n\nfasttrack audio-textgrid\n\nThis will run fasttrack on an (audio, textgrid) tuple\n\n\n\nfasttrack corpus\n\nThis will run fasttrack on a corpus of paired audio + textgrid files\n\n\n\nYou can check out the docs for all of the processing options. I’ll be using a config file that looks like this:\n\nyaml\n\n# config.yml\ncorpus: data/corpus/\noutput: data/results/formants.csv\nentry-classes: \"Word|Phone\"\ntarget-tier: Phone\ntarget-labels: \"[AEIOU]\"\nmin-duration: 0.05\nmin-max-formant: 4000\nmax-max-formant: 7000\nnstep: 20\nwhich-output: winner\ndata-output: formants\nSome of these settings are just the defaults, but I’m just illustrating the kind of things you could do. To run it:\n\nbash\n\nfasttrack corpus --config config.yml\nOn my laptop, it got formant estimates for 339 vowels in about 18 seconds.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#looking-at-the-data",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#looking-at-the-data",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Looking at the data",
    "text": "Looking at the data\nLet’s get R up and running\n\n\nr\n\nSetup librariessource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(marginaleffects)\nlibrary(gt)\nlibrary(reticulate)\nlibrary(khroma)\nlibrary(geomtextpath)\nlibrary(downlit)\nlibrary(xml2)\n\n\n\n\nr\n\nvowel_data &lt;- read_csv(\"data/results/formants.csv\")\n\nJust to skim over some data columns of interest\n\n\nr\n\nvowel_data |&gt; \n colnames()\n\n [1] \"F1\"            \"F2\"            \"F3\"            \"F1_s\"         \n [5] \"F2_s\"          \"F3_s\"          \"error\"         \"time\"         \n [9] \"max_formant\"   \"n_formant\"     \"smooth_method\" \"file_name\"    \n[13] \"id\"            \"group\"         \"label\"         \"F4\"           \n[17] \"F4_s\"         \n\n\n\n\n\n\n\n\nUseful Columns\n\n\n\n\nF1, F2, F3, F4\n\nThe formant tracks as estimated by the LPC analysis\n\nF1_s, F2_s, F3_s, F4_s\n\nSmoothed formant tracks, using discrete cosine transform\n\nfile_name\n\nThe basename for each file in the corpus\n\ngroup\n\nIf there were multiple talkers annotated in a file, which talker\n\nid\n\nA unique ID for each phone\n\n\n\n\nI’m going to zoom in on my favorite vowel, “AY”, and fit a quick model.\n\n\nr\n\n# Getting the ay data\nvowel_data |&gt; \n  filter(\n    group %in% c(\"KY25A\", \"group_0\"),\n    str_detect(label, \"AY\")\n  ) |&gt; \n  select(\n    file_name,\n    id,\n    group,\n    label,\n    F1_s, F2_s,\n    time\n  ) |&gt; \n  mutate(\n    rel_time = time - min(time),\n    prop_time = rel_time / max(rel_time),\n    .by = c(file_name, id)\n  )-&gt;\n  ay_data\n\n\n\nr\n\nModel fitting (not the main point)ay_data |&gt; \n  group_by(\n    file_name\n  ) |&gt; \n  nest() |&gt; \n  mutate(\n    model = map(\n      data, \n      ~gam(\n        list(F1_s ~ s(prop_time),\n             F2_s ~ s(prop_time)),\n        data = .x,\n        family = mvn(d = 2)\n      )\n    ),\n    pred = map(\n      model,\n      ~predictions(\n        .x, \n        newdata = datagrid(\n          prop_time = seq(0,1,length = 100)\n        )\n      )\n    )\n  ) |&gt; \n  select(file_name, pred) |&gt; \n  unnest(pred) |&gt; \n  select(file_name, rowid, group, estimate, prop_time) |&gt; \n  mutate(\n    group = str_glue(\"F{group}\")\n  ) |&gt; \n  pivot_wider(\n    names_from = group,\n    values_from = estimate\n  )-&gt;\n  ay_predictions\n\n\n\n\nr\n\nPlotting codelibrary(scales)\nlog_rev_trans = trans_new(\n  name = \"log_rev\",\n  transform = \\(x) -log(x),\n  inverse = \\(x) exp(-x)\n)\n\nay_predictions |&gt; \n  ggplot(\n    aes(\n      F2, \n      F1\n    )\n  )+\n    geom_path(\n      arrow = arrow(type = \"closed\"),\n      linewidth = 1\n    ) +\n    scale_x_continuous(trans = log_rev_trans)+\n    scale_y_continuous(trans = log_rev_trans)+\n    coord_fixed()+\n    facet_wrap(~file_name)\n\n\n\n\n\n\nFigure 1: /ay/ trajectories\n\n\n\n\nCool! Except… One of the most important factors for /ay/ is missing: whether or not the following segment is voiced or voiceless! Since fasttrackpy is designed to be very general purpose, (and not too feature laden) this kind of info isn’t added to the output. But. we can easily get it with aligned-textgrid.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#working-with-aligned-textgrid",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#working-with-aligned-textgrid",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Working with aligned-textgrid",
    "text": "Working with aligned-textgrid\nRight now, aligned-textgrid (Fruehwald and Brickhouse 2023) mostly designed to be worked with either in scripts, or interactively, so we’re going to switch over to python code. I’ll work over just one TextGrid for clarity.\n\n\npython\n\nfrom aligned_textgrid import AlignedTextGrid, Word, Phone\nfrom pathlib import Path\nimport pandas as pd\n\n\n\npython\n\ntg1_path = Path(\n  \"data\", \n  \"corpus\",\n  \"josef-fruehwald_speaker.TextGrid\"\n  )\n  \ntg1 = AlignedTextGrid(\n  textgrid_path = tg1_path,\n  entry_classes = [Word, Phone]\n)\n\ntg1\n\nAlignedTextGrid with 1 groups named ['group_0'] each with [2] tiers. [['Word', 'Phone']]\n\n\nI want to grab out enriched data for each phone for the group_0 speaker, which we can do with the dynamically created accessors for each speaker group and tier class like so.\n\n\npython\n\nphone_tier = tg1.group_0.Phone\nphone_tier\n\nSequence tier of Phone; .superset_class: Word; .subset_class: Bottom_wp\n\n\nWe can grab individual phones via indexing.\n\n\npython\n\nphone_tier[30]\n\nClass Phone, label: IY0, .superset_class: Word, .super_instance: the, .subset_class: Bottom_wp\n\n\nBut I want to focus in on just the phones with an AY label, which I’ll do with a list comprehension.\n\n\npython\n\nays = [p for p in phone_tier if \"AY\" in p.label]\n\nTo grab the following segment for each /ay/, we can use the .fol accessor.\n\n\npython\n\n# a single example\nays[0].fol.label\n\n'T'\n\n\n\n\npython\n\n# for all /ays/\n[p.fol.label for p in ays]\n\n['T', 'K', 'K', 'T', 'T', '#', 'Z', 'N', 'N', '#', 'D', '#', '#', '#', 'D', 'Z', 'Z', 'M', 'D', 'T', 'P']\n\n\nYou can see that some /ay/ tokens have a # following segment, meaning a word boundary. If we wanted to get the following segment tier-wise, we can do so to.\n\n\npython\n\n[p.get_tierwise(1).label for p in ays]\n\n['T', 'K', 'K', 'T', 'T', 'AH0', 'Z', 'N', 'N', '', 'D', 'R', 'DH', 'DH', 'D', 'Z', 'Z', 'M', 'D', 'T', 'P']\n\n\nLet’s pop this all into a pandas dataframe\n\n\npython\n\nays_context = pd.DataFrame({\n  \"id\":       [p.id for p in ays],\n  \"fol\":      [p.fol.label for p in ays],\n  \"fol_abs\":  [p.get_tierwise(1).label for p in ays],\n  \"word\":     [p.within.label for p in ays],\n  \"fol_word\": [p.within.fol.label for p in ays ]\n})\n\nays_context\n\n           id fol fol_abs      word    fol_word\n0     0-0-3-4   T       T  sunlight     strikes\n1     0-0-4-3   K       K   strikes   raindrops\n2    0-0-12-1   K       K      like           a\n3    0-0-25-1   T       T     white       light\n4    0-0-26-1   T       T     light            \n5    0-0-49-1   #     AH0      high       above\n6    0-0-60-2   Z       Z   horizon            \n7    0-0-85-1   N       N     finds          it\n8   0-0-153-1   N       N      sign        from\n9   0-0-188-2   #               sky            \n10  0-0-193-2   D       D     tried          to\n11  0-0-209-1   #       R        by  reflection\n12  0-0-216-1   #      DH        by         the\n13  0-0-235-1   #      DH        by         the\n14  0-0-246-0   D       D     ideas       about\n15  0-0-263-1   Z       Z      size          of\n16  0-0-279-1   Z       Z      size          of\n17  0-0-287-2   M       M   primary     rainbow\n18  0-0-333-1   D       D      wide      yellow\n19  0-0-343-1   T       T    lights        when\n20  0-0-354-1   P       P      type          of\n\n\nWith the way aligned-textgrid links intervals and relates their hierarchical structure, I’m able to quickly able to navigate up, down, and over between intervals using straightforwardly named accessors.\nWe can get pretty silly, like: what is the second to last phoneme in the word following the word this vowel is in?\n\n\npython\n\n[\n  ays[0].label,\n  ays[0].within.label,\n  ays[0].within.fol.label,\n  ays[0].within.fol.last.label,\n  ays[0].within.fol.last.prev.label\n]\n\n['AY2', 'sunlight', 'strikes', 'S', 'K']",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#joining-together",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#joining-together",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Joining together",
    "text": "Joining together\nBack to the /ays/ data, we can quickly join this enriched data onto the formant data, because the id column is the same between the two.\n\n\nr\n\nay_data |&gt; \n  left_join(\n    py$ays_context |&gt; \n      mutate(file_name = \"josef-fruehwald_speaker\")\n  ) |&gt; \n  filter(\n    !is.na(fol)\n  ) |&gt; \n  mutate(\n    voicing = case_when(\n      fol %in% c(\"P\", \"T\", \"K\") ~ \"vless\",\n      fol == \"#\" ~ \"final\",\n      .default = \"vced\"\n    )\n  )-&gt;\n  ays_enriched\n\nays_enriched |&gt; \n  head() |&gt; \n  rmarkdown::paged_table()\n\n\n  \n\n\n\nAnd now I can refit the model and plot.\n\n\nr\n\nModelling code# gam is annoying and needs\n# voicing to explicitly be a factor\nays_enriched |&gt; \n  mutate(voicing = factor(voicing)) -&gt;\n  ays_enriched\n\nays_enriched_model &lt;- gam(\n  list(\n    F1_s ~ voicing + s(prop_time, by = voicing),\n    F2_s ~ voicing + s(prop_time, by = voicing)\n  ),\n  data = ays_enriched,\n  family = mvn(d = 2) \n)\n\n\nays_enriched_model |&gt; \n  predictions(\n    newdata = datagrid(\n      prop_time = seq(0, 1, length = 100),\n      voicing = unique\n    )\n  ) |&gt; \n  as_tibble() |&gt; \n  select(\n    rowid, group,\n    estimate, prop_time, voicing\n  ) |&gt; \n  mutate(\n    group = str_glue(\"F{group}\")\n  ) |&gt; \n  pivot_wider(\n    names_from = group,\n    values_from = estimate\n  ) -&gt;\n  ays_enriched_pred\n\n\n\n\nr\n\nplotting codeays_enriched_pred |&gt; \n  ggplot(\n    aes(\n      F2,\n      F1,\n      color = voicing\n    )\n  )+\n    geom_textpath(\n      aes(label = voicing),\n      linewidth = 1,\n      arrow = arrow(type = \"closed\")\n    )+\n    scale_x_continuous(\n      trans = log_rev_trans\n    )+\n    scale_y_continuous(\n      trans = log_rev_trans\n    )+\n    scale_color_bright(\n      guide = \"none\"\n    )+\n    coord_fixed()\n\n\n\n\n\n\nFigure 2: enriched /ays/ data",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#let-me-know-how-it-goes",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#let-me-know-how-it-goes",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Let me know how it goes!",
    "text": "Let me know how it goes!\nIf you start using either fasttrackpy or aligned-textgrid for any purpose, I’d love to know how it’s going! For any feature requests, or bug reports, checkout their respective github repositories.\n\naligned-textgrid Github\nfasttrackpy Github",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html",
    "href": "posts/2022/12/2022-12-17/index.html",
    "title": "What is R?",
    "section": "",
    "text": "In the Spring 2023 semester, I’m going to be teaching two R intensive courses: a statistics for linguists course, and an R for the Arts and Sciences course. For both, I’m going to have to do a “What is R” discussion during week 1, and given the breadth of tools I hope students come away with, I’ve been rethinking my usual answers.\nLoading Librarieslibrary(tidyverse)\nlibrary(crandep)\nlibrary(igraph)\nlibrary(ggnetwork)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(plotly)\nlibrary(khroma)\nlibrary(scales)\n\nsource(here::here(\"_defaults.R\"))",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#a-programming-language",
    "href": "posts/2022/12/2022-12-17/index.html#a-programming-language",
    "title": "What is R?",
    "section": "A programming language?",
    "text": "A programming language?\nThe Wikipedia slug for R says\n\nR is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing\n\nAnd yeah, it is definitely a programming language. Here it is doing some programming language things:\n\n2+2\n\n[1] 4\n\n\n\n2+2 &lt; 5\n\n[1] TRUE\n\n\nAnd it can do statistical computing, like a linear model\n\ncars_model &lt;- lm(dist ~ speed, data = cars)\nsummary(cars_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nAnd it can do graphics.\n\n# `PLOT_FONT &lt;- \"Fira Sans\"` in my .Rprofile\npar(family = PLOT_FONT)\nplot(cars)\n\n\n\n\n\n\nFigure 1: A plot\n\n\n\n\nObviously, none of these things are unique to R. In the other programming language I know best, Python, you can fit a linear model and make a scatter plot. What differentiates programming languages, in my experience, is what kinds of operations, data structures, and workflows it prioritizes.\nFor R, I think it’s uncontroversial to say it prioritizes rectangular data with mixed data-type rows & single data-type columns and also provides a lot of options for indexing column-wise. And a lot of the extensions to R have leaned into this prioritization hard.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#an-ecosystem",
    "href": "posts/2022/12/2022-12-17/index.html#an-ecosystem",
    "title": "What is R?",
    "section": "An ecosystem?",
    "text": "An ecosystem?\nBut “R” isn’t just a programming language, it’s also an ecosystem of community created packages. “Learning R” involves learning about these packages, and how they’re interrelated. I grabbed the list of all packages on CRAN and the packages they import with the crandep package.\n\ncran_df &lt;- crandep::get_dep_all_packages()\n\n\nImport Summariescran_df |&gt; \n  filter(type == \"imports\", !reverse) |&gt; \n  count(to) |&gt; \n  arrange(desc(n)) |&gt; \n  mutate(rank = 1:n()) -&gt; imported\n\nimported_10 &lt;- imported |&gt; slice(1:10)\n\n\nIf you count up how often each package gets imported and rank them, you get the familiar power-law plot. I’ve plotted this one out a bit non standard-ly so that frequency is on the x axis for both the main plot and the inset, and so that I could include the package names in the inset with horizontal text.\n\nPlotting codeimported |&gt; \n  ggplot(aes(n, rank))+\n    geom_point()+\n    scale_x_log10(labels = label_comma())+\n    scale_y_log10(labels = label_comma())+\n    labs(title = \"Frequency by rank of imported R packages\") -&gt; mainplot\n\nimported_10 |&gt; \n  mutate(to = as.factor(to),\n          to = fct_reorder(to, rank)) |&gt; \n  ggplot(aes(n, to))+\n    geom_col(fill = \"white\")+\n    geom_text(aes(label = to,\n                  x = 0), \n              color = \"grey10\",\n              hjust = 0,\n              nudge_x = 100, \n              family = PLOT_FONT,\n              size = 4.5)+\n    scale_x_continuous(expand = expansion(mult = 0),\n                       labels = label_comma())+\n    theme(axis.text.y = element_blank(),\n          text = element_text(size = 10),\n          panel.grid.minor  = element_blank(),\n          panel.grid.major.y = element_blank())+\n    labs(y = NULL,\n         title = \"top10\") -&gt; inset\n \nmainplot + inset_element(inset, 0.05, 0.05, 0.5, 0.6)\n\n\n\n\n\n\nFigure 2: A log(rank) by log(frequency) plot of R imports\n\n\n\n\nHere’s a network visualization of these imports and dependencies. I color coded the nodes according to common R package naming trends\n\ngg* - Packages extending ggplot2\ntidy* - Packages declaring their adherence to tidy-data principles (and the tidyverse more generally)\n*r - Packages declaring that they are… R packages\n\n\nNetwork graph setupimported |&gt;\n  filter(n &gt;= 5) |&gt; \n  pull(to) -&gt; to_network\n\ncran_df |&gt;\n  filter(!reverse, type == \"imports\",\n         to %in% to_network) |&gt;\n  df_to_graph(nodelist = cran_df |&gt; rename(name = from)) -&gt; cran_network\n\nset.seed(300)\ncran_flat &lt;- ggnetwork(cran_network, layout = with_drl())\n\nxclip &lt;- quantile(cran_flat$x, c(0.0025, 0.9975))\nyclip &lt;- quantile(cran_flat$y, c(0.0025, 0.9975))\n\n\n\nNetwork graphcran_flat |&gt; \n  mutate(name_pattern = case_when(str_detect(name, \"[rR]$\") ~ \"thingr\",\n                                  str_detect(name, \"tidy\") ~ \"tidy\",\n                                  str_detect(name, \"^[Gg]g\") ~ \"gg\",\n                                  T ~ \"else\"),\n         name_pattern = factor(name_pattern, levels = c(\"tidy\", \"gg\", \"thingr\", \"else\"))) |&gt; \n  arrange(desc(name_pattern)) |&gt; \n  filter(x &gt;= xclip[1], x &lt;= xclip[2],\n         y &gt;= yclip[1], y &lt;= yclip[2]) |&gt; \nggplot(aes(x = x, y = y, xend = xend, yend = yend, color = name_pattern))+\n  #geom_nodes()+  \n  geom_nodes(aes(alpha = name_pattern))+\n  scale_color_bright(limits = c(\"tidy\", \"gg\", \"thingr\", \"else\"),\n                     labels = c(\"tidy*\", \"gg*\", \"*r\", \"else\"))+\n  dark_theme_void()+\n  scale_alpha_manual(values = c(0.5,0.3, 0.08, 0.02), \n                     limits = c(\"tidy\", \"gg\", \"thingr\", \"else\"),\n                     guide = \"none\")+\n  labs(color = NULL,\n       title = \"CRAN imports network visualization\")+\n  theme(legend.position = c(0.2,0.8), \n        legend.text = element_text(family = PLOT_FONT),\n        text = element_text(family = PLOT_FONT),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nFigure 3: CRAN network graph",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#a-communications-platform",
    "href": "posts/2022/12/2022-12-17/index.html#a-communications-platform",
    "title": "What is R?",
    "section": "A communications platform?",
    "text": "A communications platform?\nBut beyond just the R packages that implement specific analysis or process data in a specific way, there are also all of the tools built around R (and mostly around the RStudio IDE) that also make R what I might call a “communications platform.” From Sweave to knitr to rmarkdown and now Quarto, the kind of literate programming you can do in R has moved from ugly1 Beamer slides to, well, full on blogs.\nBut, it’s not just for the novelty or nerd appeal that I think it’s important to learn about R authoring tools available. They’ve also changed my own discovery and learning process about new packages. You can always find the documentation for a package on CRAN, but you should really try to find its pkgdown site.2",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#what-does-it-mean-to-know-r",
    "href": "posts/2022/12/2022-12-17/index.html#what-does-it-mean-to-know-r",
    "title": "What is R?",
    "section": "What does it mean to “know R”?",
    "text": "What does it mean to “know R”?\nWhen I think about what it means to “know R”, and my goal for the kind of knowledge my students should start getting a handle on, it involves all of these components: the programming syntax, the social graph of the ecosystem, and the authoring tools to use and seek out.\nA lot of other programming languages have similar kinds of features, especially Python with pypi or conda keeping track of the ecosystem and Sphinx providing the authoring tools. There too I’d say that getting to “know Python” involves a lot more than learning its syntax.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#footnotes",
    "href": "posts/2022/12/2022-12-17/index.html#footnotes",
    "title": "What is R?",
    "section": "Footnotes",
    "text": "Footnotes\n\nsorry, but they are↩︎\nMost often, click on the URL listed on CRAN, which takes you to the package’s github, which then probably has a link to the pkgdown site in its “about” box.↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-19_logit-priors-again/index.html",
    "href": "posts/2024/11/2024-11-19_logit-priors-again/index.html",
    "title": "Random effect priors, redo",
    "section": "",
    "text": "For me, teaching stats this semester has turned into a journey of discovering what the distributional and ggdist packages can do for me. The way I make illustrative figures will never be the same. So I thought I’d revisit my post about hierarchical variance priors, this time implementing the figures using these two packages.\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scico)\nlibrary(ggdist)\nlibrary(distributional)\nCustom y theme and scaletheme_no_y &lt;- function(){\n\n  out_theme &lt;- theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank()\n  )\n\n  out_theme  \n}\n\nscale_y_tight &lt;- function(...) {\n  scale_y_continuous(expand = expansion(0), ...)\n}",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "11",
      "Random effect priors, redo"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-19_logit-priors-again/index.html#looking-for-a-more-neutral-distribution",
    "href": "posts/2024/11/2024-11-19_logit-priors-again/index.html#looking-for-a-more-neutral-distribution",
    "title": "Random effect priors, redo",
    "section": "Looking for a more neutral distribution",
    "text": "Looking for a more neutral distribution\nLet’s see what different \\(\\sigma\\)s look like in the probability space.\n\npossible_dists &lt;- tibble(\n  sigma = seq(\n    1, 2.1, by = 0.1\n  ),\n  dist = dist_normal(0, sigma),\n  p_dist = dist_transformed(\n    dist, plogis, qlogis\n  )\n)\n\nggplot(\n  possible_dists,\n  aes(\n    xdist = p_dist,\n    fill = sigma\n  )\n)+\n  stat_slab(\n    color = \"black\",\n    linewidth = 0.5\n  )+\n  scale_fill_scico(\n    palette = \"devon\",\n    guide = \"none\"\n  )+\n  scale_y_tight()+\n  facet_wrap(~sigma)+\n  theme_no_y()+\n  theme_no_x()\n\n\n\n\n\n\n\nIt looks like somewhere between 1.3 and 1.4 is the sweet spot for a maximally flat random effects distribution in the probability space.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "11",
      "Random effect priors, redo"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-19_logit-priors-again/index.html#really-honing-in-on-it",
    "href": "posts/2024/11/2024-11-19_logit-priors-again/index.html#really-honing-in-on-it",
    "title": "Random effect priors, redo",
    "section": "Really honing in on it",
    "text": "Really honing in on it\nI can try getting even more precise by looking at a vectorized version of these distributions, and finding the largest sigma what still has its density peak at 0.5.\n\nlibrary(purrr)\n\n# a vector of sigmas\nsigmas = seq(1.3, 1.5, length = 100)\n\n# a vectorized normal\nvec_dist &lt;- dist_normal(\n  mu = 0,\n  sigma = sigmas\n) \n\n# a vectorized ilogit(normal)\nvec_p_dist &lt;- dist_transformed(\n  vec_dist,\n  plogis,\n  qlogis\n)\n\n# the density function\n# from 0 to 0.5\np_densities &lt;- density(\n  vec_p_dist, \n  seq(0, 0.5, length = 100)\n)\n\n# The index of the max\n# density\nwhere_is_max &lt;- p_densities |&gt; \n  map_vec(\n    which.max\n  ) \n\n# if where_is_max == 100\n# peak density was at 0.5\nflat_idx &lt;- (where_is_max == 100) |&gt; \n  which() |&gt; \n  max()\n\nflattest_sigma &lt;- sigmas[flat_idx]\n\nflattest_sigma\n\n[1] 1.413131\n\n\nLet’s take a look at it:\n\nflat_pdist &lt;- dist_normal(0, flattest_sigma) |&gt; \n  dist_transformed(plogis,qlogis)\n  \nggplot()+\n  stat_slab(\n    aes(\n      xdist = flat_pdist\n    ),\n    fill = \"#EE6677\"\n  )+\n  scale_y_tight()+\n  theme_no_y()",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "11",
      "Random effect priors, redo"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "",
    "text": "I’ve been doing a lot of python development recently, and really leaning into object oriented programming for my projects. For example, in the aligned-textgrid package, I started off by defining a class to represent intervals, which at their core have start and end times, and a label. Each interval also points to the interval objects preceding and following, as well as any intervals it contains or is contained in.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html#doing-it-in-r",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html#doing-it-in-r",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "Doing it in R?",
    "text": "Doing it in R?\nI don’t exactly want to replicate the entire package inside R, and for a while I wasn’t even sure how I could, because of R’s copy-on-modify behavior.\nLet’s say we had the following sequence of intervals:\n\n\n\n\n\nWe could represent the basic information (start, end, and label) in lists. I’ll also set up a reference to interval_b as following interval_a.\n\n\nr\n\ninterval_a &lt;- list(\n  start = 1,\n  end   = 2,\n  label = \"a\"\n)\n\ninterval_b &lt;- list(\n  start = 3,\n  end   = 4,\n  label = \"b\"\n)\n\ninterval_a$fol &lt;- interval_b\n\nWe can double check that the fol reference is working with identical() .\n\n\nr\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nidentical()\n\n\n\n\n\nThis took me a while to find. Python has a commonly used is operator for checking whether two variables refer to the same object, rather than just being equal.\n\n\npython\n\na = 1.0\nb = 2/2\n\na is b\n\nFalse\n\n\n\n\npython\n\na == b\n\nTrue\n\n\nBut I’ve never used R’s identical() before, and it doesn’t usually show up in intros to the language like python’s is.\n\n\n\n\nCopy-on-modify\nThings become a problem if we want to make changes to interval_b, though. One of the core tasks I wanted aligned-textgrid to make easy is the modification of interval labels. But in R, if we change the value of interval_b$label, that change won’t be reflected in the values in interval_a$fol, and the reference between the two objects will be broken.\n\n\nr\n\ninterval_b$label &lt;- \"new B\"\n\n# this is now false\nidentical(interval_a$fol, interval_b)\n\n[1] FALSE\n\n\n\n\nr\n\n# this is the original label\ninterval_a$fol$label\n\n[1] \"b\"\n\n\nTo understand what’s going on here, I’d recommend checking out Hadley Wickham’s Advanced R chapter on Names and Values, especially the section on Copy-on-modify.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html#oop-in-r",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html#oop-in-r",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "OOP in R",
    "text": "OOP in R\nOne of these days, there’s going to be a new R package called yaoop for Yet Another Object Oriented Paradigm. The class systems that ship with R are called S3 and S4, and there are two new-ish class packages R6 and S7. These two new packages both have pretty interesting properties, but as pointed out in Advanced R\n\nR6 objects are mutable, which means that they are modified in place, and hence have reference semantics.\n\nS7 is the newer package, but I’ve double checked, and it also follows copy-on-modify, which means R6 is the way to go for this kind of use case.\n\nTrying out \n\n\nr\n\nlibrary(R6)\n\nFollowing the intro in the R6 documentation, the most basic SequenceInterval class would be something like this.\n\n\nr\n\n\n\n1\n\nThe name of the class you want to be returned when you use class() .\n\n2\n\nAny generally available class properties and methods are declared in a list passed to the public argument.\n\n3\n\nI believe a any properties you want to use or set through the rest of the class definition need to be declared here first.\n\n4\n\ninitialize is a special method that’s called when you use SequenceInterval$new()\n\n\n\nSequenceInterval &lt;- R6Class(\n1  classname = \"SequenceInterval\",\n2  public = list(\n3    start = numeric(0),\n    end   = numeric(0),\n    label = character(0),\n    prev  = NULL,\n    fol   = NULL,\n                                    \n4    initialize = function(\n      start = numeric(0),\n      end   = numeric(0),\n      label = character(0)\n    ){\n      self$start = start\n      self$end   = end\n      self$label = label\n    }\n    \n  )                               \n)\n\nMy two sequence objects would then be:\n\n\nr\n\ninterval_a &lt;- SequenceInterval$new(\n  start = 1,\n  end   = 2,\n  label = \"a\"\n)\n\ninterval_b &lt;- SequenceInterval$new(\n  start = 2,\n  end   = 3,\n  label = \"b\"\n)\n\ninterval_a$fol &lt;- interval_b\n\nLet’s double check that sequence_b is appropriately following sequence_a.\n\n\nr\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\nNow, for the moment of truth, we’ll change the label on interval_b and see if it breaks things.\n\n\nr\n\ninterval_b$label &lt;- \"new B\"\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\n\n\nr\n\ninterval_a$fol$label\n\n[1] \"new B\"\n\n\nSuccess!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html#getting-more-complicated",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html#getting-more-complicated",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "Getting more complicated",
    "text": "Getting more complicated\nI’m going to try to make things a little more complicated with respect to thefol and prev properties. I want\n\nWhen fol is set, prev is automatically set.\nWhen prev is set, fol is automatically set.\n\nThese aren’t just nice quality of life features, but also capture the necessary logical properties of following and preceding.\nI think the way to go about this will be\n\nto lock off fol and prev from being directly settable. I think the best way to do this is to move them to active bindings, which seems a lot like using the python @property decorator on a method.\nAdd private .fol and .prev properties.\nDefine setter functions that will update the _fol and _prev properties, being careful to avoid infinite recursion!\n\nI’ve described each new piece in the code annotations.\n\n\nr\n\n\n\n1\n\nDefining private properties. Convention in python is for the name of these properties to start with and underscore, but that’s not allowed in R, so I’m going with dots.\n\n2\n\nThese are the getter functions to return the actual .fol and .prev objects.\n\n3\n\nSame as the original class definition above.\n\n4\n\nThis is the setter function for the following interval.\n\n5\n\nSUPER IMPORTANT. The set_fol() method is calling set_prev(), and the set_prev() method is calling set_fol(). To avoid infinite recursion, the function should stop here if it’s already the preceding interval to its following interval.\n\n6\n\nA kind of interesting thing is I have to use the public set_prev() method here, because the method won’t be able to dig into the private .prev property of interval.\n\n7\n\nSame logic as set_fol().\n\n\n\nSequenceInterval &lt;- R6Class(\n  classname = \"SequenceInterval\",\n  \n1  private = list(\n    .fol = NULL,\n    .prev = NULL\n  ),\n\n2  active = list(\n    fol = function(){\n      return(private$.fol)\n    },\n    prev = function(){\n      return(private$.prev)\n    }\n  ),\n\n3  public = list(\n    start = numeric(0),\n    end   = numeric(0),\n    label = character(0),\n\n    initialize = function(\n      start = numeric(0),\n      end   = numeric(0),\n      label = character(0)\n    ){\n      self$start = start\n      self$end   = end\n      self$label = label\n    },\n\n4    set_fol = function(interval){\n      private$.fol = interval\n\n5      if(identical(self$fol$prev, self)){\n        return(invisible(self))\n      }\n\n6      self$fol$set_prev(self)\n\n    },\n\n7    set_prev = function(interval){\n      private$.prev = interval\n\n      if(identical(self$prev$fol, self)){\n        return(invisible(self))\n      }\n\n      self$prev$set_fol(self)\n    }\n\n  )\n)\n\nWe can still create intervals the same way as above.\n\n\nr\n\ninterval_a &lt;- SequenceInterval$new(1, 2, \"a\")\ninterval_b &lt;- SequenceInterval$new(1, 2, \"b\")\n\nBut to set up interval_b as the interval following interval_a, you’ve got to use the set_fol() setter function.\n\n\nr\n\ninterval_a$set_fol(interval_b)\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\nBut now, interval_a has been automatically set as preceding interval_b!\n\n\nr\n\nidentical(interval_b$prev, interval_a)\n\n[1] TRUE\n\n\nThe major defect here is that if I, or a user, didn’t know about the setter functions, we’ll get a very inscrutable error message when trying to directly set fol or prev.\n\n\nr\n\ninterval_a$fol &lt;- interval_b\n\nError in (function () : unused argument (base::quote(&lt;environment&gt;))\n\n\nI might want to figure out if it’s possible to get a better error here, or even better, some way to short circuit this assignment attempt to set_fol(), but I think that’s a bit beyond my patience and time for right now.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html",
    "href": "posts/2022/12/2022-12-10/index.html",
    "title": "A lesson in over-preparing",
    "section": "",
    "text": "One of the courses I taught in the Fall 2022 semester was Natural Language Processing (NLP). It was a fun course to teach, and I learned a lot since it’s not a topic in my core areas of research. At the same time, the amount of work I put into it has made me really start to rethink how I prepare for teaching.\nMy tendency, for a while, has been to prepare extensive course notes that I publish on my website (Exhibit A, Exhibit B). I’ve never really reflected on how much work that actually takes. Moreover, I don’t tend to consider it when I reflect on how much “writing” I get done (and inevitably get a bit discouraged about my productivity).\nBut, it occurred to me that I could quantify how much writing I really did this semester. I wrote all my course notes in Quarto, which generates a search index in a .json file. To get a total count of words that I wrote in course notes, I just need to parse that json file and tokenize it! I found a handy blog post about analyzing git repos that helped a lot in the process.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#setup",
    "href": "posts/2022/12/2022-12-10/index.html#setup",
    "title": "A lesson in over-preparing",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidyjson)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(padr)\nlibrary(glue)\n\nsource(here::here(\"_defaults.R\"))\n\nThis block of code is just copied from the blog post I just mentioned.\n\n# Remote repository URL\nrepo_url &lt;- \"https://github.com/JoFrhwld/2022_lin517.git\"\n\n# Directory into which git repo will be cloned\nclone_dir &lt;- file.path(tempdir(), \"git_repo\")\n\n# Create command\nclone_cmd &lt;- glue(\"git clone {repo_url} {clone_dir}\")\n\n# Invoke command\nsystem(clone_cmd)\n\nThere’s a handful of R libraries for reading json files into R, but after searching around I went with tidyjson because I like using tidyverse things.\n\nsearch_file &lt;- file.path(clone_dir, \"_site\", \"search.json\")\nsite_search &lt;- read_json(search_file)\n\nI was really glad to find that the search.json file is relatively flat, so pulling out the metadata and text was not as complicated as it could have been.\n\nsite_search |&gt;\n  gather_array() |&gt;\n  hoist(..JSON,\n    \"title\",\n    \"section\",\n    \"text\"\n  )  |&gt;\n  as_tibble() |&gt;\n  select(array.index, title, section, text) |&gt;\n  unnest_tokens(input = text, output = \"words\") -&gt; tokens_table\n\nI noticed in the json file that there were multiple entries for a single page of course notes, one for each subsection, but there were also some entries with the subsection value set to blank. I just wanted to double check that the blank section entries weren’t the entire page of lecture notes, with additional entries duplicating the text by subsection.\n\ntokens_table |&gt;\n  mutate(has_section = section != \"\") |&gt;\n  group_by(title, has_section) |&gt;\n  count()\n\n# A tibble: 45 × 3\n# Groups:   title, has_section [45]\n   title                              has_section     n\n   &lt;chr&gt;                              &lt;lgl&gt;       &lt;int&gt;\n 1 Addendum                           FALSE          43\n 2 Addendum                           TRUE          100\n 3 Additional Neural Network Concepts FALSE         473\n 4 Additional Neural Network Concepts TRUE         1109\n 5 Comprehensions and Useful Things   FALSE        1116\n 6 Data Processing                    FALSE         654\n 7 Data Processing                    TRUE         1804\n 8 Data Sparsity                      FALSE         720\n 9 Data Sparsity                      TRUE         1724\n10 Evaluating models                  FALSE          42\n# ℹ 35 more rows\n\n\nLooks like no. The blank titled sections are probably cases where I had a paragraph or two that came before the first section header.\nSo how many words?\n\n\ntokens_table |&gt;\n  nrow()\n\n[1] 43637\n\n\nBased on the default tokenization from tidytext, it looks like I wrote just north of 40k words. I don’t have the best sense of how this compares to other kinds of genre writing, but apparently the goal of NaNoWriMo (National Novel Writing Month) is to write a novel that is 50k words.\nSo, I didn’t quite write a novel. But the amount of work that went into these 40k words was still considerable in terms of background research and trying to come to my own understanding of relatively complex mathematical formulae so that I could distill them into a comprehensible lesson. Also not accounted for was all the code I wrote and had to debug within the course notes!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#how-many-words-did-i-write-over-time",
    "href": "posts/2022/12/2022-12-10/index.html#how-many-words-did-i-write-over-time",
    "title": "A lesson in over-preparing",
    "section": "How many words did I write over time?",
    "text": "How many words did I write over time?\nSince I was publishing the course notes to github, that means I also have a preserved history of how my word count grew over time. All I have to do is apply the same procedures to the history of search.json.\n\ncode to get the git commit historylog_format_options &lt;- c(datetime = \"cd\", commit = \"h\", parents = \"p\", author = \"an\", subject = \"s\")\noption_delim &lt;- \"\\t\"\nlog_format   &lt;- glue(\"%{log_format_options}\") |&gt; glue_collapse(option_delim)\nlog_options  &lt;- glue('--pretty=format:\"{log_format}\" --date=format:\"%Y-%m-%d %H:%M:%S\"')\nlog_cmd      &lt;- glue('git -C {clone_dir} log {log_options}')\n\n\n\nsystem(log_cmd, intern = TRUE) |&gt;\n  str_split_fixed(option_delim, length(log_format_options)) |&gt;\n  as_tibble(.name_repair = \"minimal\") |&gt;\n  setNames(names(log_format_options))-&gt;commit_history\n\n\nget_length() function definitionget_length &lt;- function(commit, clone_dir){\n  search_file &lt;- file.path(clone_dir, \"_site\", \"search.json\")\n  checkout_cmd &lt;- glue(\"git -C {clone_dir} checkout {commit} {search_file}\")\n  system(checkout_cmd)\n  site_search &lt;- read_json(search_file)\n  site_search |&gt;\n    gather_array() |&gt;\n    hoist(..JSON,\n      \"title\",\n      \"section\",\n      \"text\"\n    )  |&gt;\n    as_tibble() |&gt;\n    select(array.index, title, section, text) |&gt;\n    unnest_tokens(input = text, output = \"words\") |&gt;\n    nrow() -&gt; word_count\n  return(word_count)\n}\n\n\nThe data frame commit_history contains metadata about each commit. What I’ll do next is apply the function I wrote in the collapsed code above to the list of commit hashes, and get the word count at each commit.\n\ncommit_history |&gt;\n  mutate(word_count = map(commit, ~get_length(.x, clone_dir)) |&gt;\n           simplify()) -&gt; word_history\n\nI often had many commits in a single day, so to simplify things a bit, I’ll just get the max number of words I had by the end of a given day. Fortunately, I did not stay up past midnight pushing commits this semester, so the data shouldn’t be messed up by work sessions overlapping across days.\n\nword_history |&gt;\n  mutate(datetime = ymd_hms(datetime)) |&gt;\n  thicken(\"day\") |&gt;\n  group_by(datetime_day) |&gt;\n  summarise(words = max(word_count)) |&gt;\n  arrange(datetime_day) -&gt; daily_words\n\nCumulative Wordcount\nHere’s the cumulative count of words over the course of the semester. I forget why, exactly, it starts out at 7,000 words. I think I might’ve been managing the course notes differently between the end of August and start of September, and then just copied them over. But it is a pretty continuous rise, with a few notable plateaus. Some of those plateaus occurred when I had actually prepared more than could be covered in a one or two class meetings, so we stayed with a set of lecture notes for a longer period of time.\n\nplotting codedaily_words |&gt;\n  pad(\"day\") |&gt;\n  fill(words) |&gt;\n  ggplot(aes(datetime_day, words))+\n    geom_ribbon(aes(ymin = 0, ymax = words), alpha = 0.2)+\n    geom_line(linewidth = 1,\n              lineend = \"round\")+\n    expand_limits(y = 0)+\n    scale_y_continuous(labels = label_comma(),\n                       expand = expansion(mult = c(0, 0.05)))+\n    scale_x_date(expand = expansion(mult = 0.01))+\n    labs(x = \"date\",\n         y = \"number of words\",\n         title = \"Cumulative number of words in course notes\")\n\n\n\n\n\n\nFigure 1: The number of words I wrote over the course of Fall 2022\n\n\n\n\nWhen was I writing?\nI can also double check when I was writing my course notes. I was teaching on a Monday-Wednesday-Friday schedule, and Fridays were usually given over to practical exercises (the text of which is largely absent from these counts…).\nIt was no surprise to me that I wrote most of the course notes on Sundays. The total number of Sunday words is about 17k, which comes out to about mean of about 1,300 words for Sundays this semester.\n\nplotting codedaily_words |&gt;\n  pad(\"day\") |&gt;\n  fill(words) |&gt;\n  mutate(daily_words = words - lag(words),\n         wday = wday(datetime_day, label = T)) |&gt;\n  drop_na() |&gt;\n  group_by(wday) |&gt;\n  summarise(word = sum(daily_words)) |&gt;\n  ggplot(aes(wday, word))+\n    geom_col(fill = \"grey90\")+\n    scale_y_continuous(labels = label_comma())+\n    labs(x = \"day of the week\",\n         y = \"total words\",\n         title = \"words written by day of week\")\n\n\n\n\n\n\nFigure 2: The total number of words I wrote per day of the week.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#thoughts",
    "href": "posts/2022/12/2022-12-10/index.html#thoughts",
    "title": "A lesson in over-preparing",
    "section": "Thoughts",
    "text": "Thoughts\nWhile I feel pretty good about the material I produced, I really don’t think this way of doing things is tenable. The writing that I did do for these course notes was writing that I didn’t do for any other project. Moreover, you can almost see how much energy Sundays took out of me! And even though I wasn’t writing as many course notes Tuesday through Friday, those were full work days that I was doing all of my other work during.\nSo the upshot is: I’m already brainstorming on how to not prepare like this for a class again!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#sources",
    "href": "posts/2022/12/2022-12-10/index.html#sources",
    "title": "A lesson in over-preparing",
    "section": "Sources",
    "text": "Sources\n\nhttps://drsimonj.svbtle.com/embarking-on-a-tidy-git-analysis\nhttps://tidyr.tidyverse.org/reference/hoist.html",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html",
    "title": "Simulating DND Rolls",
    "section": "",
    "text": "I’ve recently started playing Dungeons and Dragons, and have been really enjoying the campaign my sibling runs. I’m still getting a handle on the mechanics, especially in combat, where the sequence of events that are allowed, and keeping track of your what you roll when is still a little confusing to me. Even though it’s not playing out in real time, it feels urgent, and I don’t always keep track of things like “Am I rolling with advantage?”, “Do I have bardic inspiration” etc.\nBut in the time in between sessions, in addition to thinking through the mechanics to remember, I’ve also been thinking about the probabilities of it all. And what do you know! There’s an R package for that: {droll} !",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#simulating-rolls",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#simulating-rolls",
    "title": "Simulating DND Rolls",
    "section": "Simulating rolls",
    "text": "Simulating rolls\n\nlibrary(tidyverse)\nlibrary(geomtextpath)\nlibrary(khroma)\nlibrary(droll)\n\nsource(here::here(\"_defaults.R\"))\n\nThe {droll} package works seems to be explicitly built to compatible with the DnD directions work. For example, you might roll a 20 sided die, or a “d20”, and add an ability “modifier” to the result. In droll commands, we’ll create a d20, set a dexterity modifier, then roll a random value then add that modifier:\n\nset.seed(12)\n\n\n# make a d20\nd20 &lt;- d(20)\n# low dex\ndex &lt;- 1\nd20 + dex\n\n[1] 3\n\n\nAnother thing you might do is roll multiple dice, then add the result together. For example “roll 3d8” means you roll three 8-sided dice, then add the result together for something to happen.\n\nd8 &lt;- d(8)\n3 * d8\n\n[1] 17",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#distributions",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#distributions",
    "title": "Simulating DND Rolls",
    "section": "Distributions",
    "text": "Distributions\nIt also comes with a few probability distributions built to get the density, cumulative probability, and quantiles of die, which might already be familiar to some R users. Here’s the density distributions of rolling 1, 2, 3, and 4 d8s.\n\n## A function to make a tibble\n## of n rolls of a die.\nmake_roll_tibble &lt;- function(n, die){\n  nfaces &lt;- max(die@faces)\n  tibble(\n    rolls = n:(n*nfaces),\n    faces = nfaces,\n    density = droll(rolls, n*die)\n  )\n}\n\n\n## Constructing the roll densities\ntibble(n = 1:4) |&gt; \n  mutate(\n    roll_df = map(\n      n,\n      # new R anonymous function\n      \\(n) make_roll_tibble(n, d8)\n    )\n  ) |&gt; \n  unnest(roll_df) -&gt; \n  roll_densities\n\n\n## plotting the roll densities\nroll_densities |&gt; \n  mutate(\n    nd = str_c(n, \"d\", faces)\n  ) |&gt; \n  ggplot(aes(rolls, density))+\n    geom_area(fill = \"grey90\")+\n    expand_limits(x = 1)+\n    facet_wrap(\n      ~nd, \n      scales = \"free_x\"\n      )+\n  labs(\n    title = \"Density distributions of 1 through 4 d8 rolls\"\n  )\n\n\n\n\n\n\nFigure 1: Density distributions of nd8 rolls",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#advantage-vs-disadvantage",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#advantage-vs-disadvantage",
    "title": "Simulating DND Rolls",
    "section": "Advantage vs Disadvantage",
    "text": "Advantage vs Disadvantage\nOne mechanic in DnD is rolling with “Advantage” vs rolling with “Disadvantage”. If you have advantage (say, because an enemy is restrained), you roll two d20s and take the highest value. If you roll with disadvantage (say, because you are restrained), you roll two d20s and take the lowest value.\nThere’s not a straightforward way to get the advantage vs disadvantage rolls, but I figured out a way to do with with some tidyverse tricks.\n\n## Set up the number of rolls\nnsims = 10000\nnrolls = nsims * 2\n\n## Initial tibble with \n## random rolls\ntibble(\n  roll_id = 0:(nrolls-1),\n  roll_value = rroll(nrolls, d20)\n) |&gt; \n  ## convert to roll groups\n  mutate(\n    roll_group = floor(roll_id/2)\n  ) |&gt; \n  ## group\n  group_by(roll_group) |&gt; \n  ## number the rolls\n  mutate(\n    roll_num = row_number()\n  ) |&gt; \n  ## Get advantage, \n  ## disadvantage\n  ## and first roll\n  summarise(\n    advantage = max(roll_value),\n    disadvantage = min(roll_value),\n    normal = roll_value[1]\n  ) -&gt; \n  simulated_rolls\n\nhead(simulated_rolls)\n\n# A tibble: 6 × 4\n  roll_group advantage disadvantage normal\n       &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1          0        12            3     12\n2          1        16            2     16\n3          2        17            5     17\n4          3        16           12     16\n5          4         8            2      2\n6          5        11            1      1\n\n\nNext step is to count up how many of each value we got, which requires pivoting.\n\nsimulated_rolls |&gt; \n  pivot_longer(\n    cols = advantage:normal,\n    names_to = \"roll_type\",\n    values_to = \"roll_value\"\n  ) -&gt;\n  rolls_long\n\nhead(rolls_long)\n\n# A tibble: 6 × 3\n  roll_group roll_type    roll_value\n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1          0 advantage            12\n2          0 disadvantage          3\n3          0 normal               12\n4          1 advantage            16\n5          1 disadvantage          2\n6          1 normal               16\n\n\nAfter pivoting long, I’ll calculate the cumulative probability that a player will pass the skill check.\n\nrolls_long |&gt; \n  count(roll_type, roll_value) |&gt; \n  arrange(desc(roll_value)) |&gt; \n  mutate(\n    .by = roll_type,\n    prob = cumsum(n)/sum(n)\n  )  -&gt; \n  check_prob\nhead(check_prob)\n\n# A tibble: 6 × 4\n  roll_type    roll_value     n   prob\n  &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1 advantage            20   979 0.0979\n2 disadvantage         20    31 0.0031\n3 normal               20   515 0.0515\n4 advantage            19   884 0.186 \n5 disadvantage         19    86 0.0117\n6 normal               19   503 0.102 \n\n\nLast thing to do is make a plot!\n\ncheck_prob |&gt; \n  ggplot(aes(roll_value, prob, color = roll_type))+\n    geom_textpath(\n      aes(label = roll_type),\n      linewidth = 2\n    )+\n    scale_x_continuous(\n      breaks = c(5, 10, 15, 20),\n      minor_breaks = c(\n        1:4,\n        6:9,\n        11:14,\n        16:19\n      )\n    )+\n    scale_color_manual(\n      values = c(\"#b59e54\", \"#AB6dac\",\"#c73032\" )\n    )+\n    guides(\n      color = \"none\"\n    )+\n    labs(\n      title = \"Probability of passing a skill check, no modifier\",\n      x = \"Difficulty class\"\n    )\n\n\n\n\n\n\nFigure 2: Cumulative probability density functions",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#closing-thoughts",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#closing-thoughts",
    "title": "Simulating DND Rolls",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nOne thought I had, while writing this post, was how the different reasons for doing these dice rolls in the game affected the kind probability plot I made. Most often you’ll be rolling 3d8 in order to calculate how much damage you’re going to do, so for that plot what you want to know what the point probabilities of each outcome is, hence the density functions.\nFor rolling d20s with advantage or disadvantage, you’re wanting to see what the probability is that you’ll pass the skill check, that is, that you’ll roll at least some value, hence the inverse cumulative probability distributions!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html",
    "title": "Rising Wooders",
    "section": "",
    "text": "This is a blog post to accompany my American Dialect Society poster.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#wooder",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#wooder",
    "title": "Rising Wooders",
    "section": "“Wooder”",
    "text": "“Wooder”\nIf there’s one thing people know about the Philadelphia dialect, it’s that we say [wʊɾɚ], often spelled “wooder” for the word water. It was how the LA Times opened their story about Mare of Easttown.\n\nBarely 11 minutes into the first episode of “Mare of Easttown,” Kate Winslet goes where few actors have gone before.\nShe says the word “wooder.”\nAs in, what the good people of southeastern Pennsylvania call the stuff that comes out of the faucet.\n\nWhen accepting a Webby award in 2019, the NHL mascot Gritty held up a sign saying\n\nIt Pronounced Wooder, not Water.\n\n\n\nGritty (2019)\n\nThe fact that Philadelphians say “wooder” is also the number one thing anyone ever wants to talk to me about when they find out I study the Philadelphia dialect. Back in 2013, when the big paper about what we found in the Philadelphia Neighborhood Corpus came out (Labov, Rosenfelder, and Fruehwald 2013), we got interviewed by the local news, and they asked me “What about ‘wooder’”? I said “Philadelphians say wooder, and that’s that.”\nWhen I said it, I meant it almost apologetically. We hadn’t investigated anything about the word, mostly because we were focusing on larger structural shifts in the vowel system. As far as I knew, the “wooder” pronunciation was just a one off alteration to a single word, and I didn’t think there was anything too interesting to say about it. “That’s that.” The way it got edited into the final broadcast, it seemed like I was making more of a statement of finality, as if to say “Lots of other things are changing about the Philadelphia dialect, but not ‘wooder.’ And that’s that.”\nThis project is me circling back around to both of those possible messages behind “and that’s that” and asking “is it really?”",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-notice-wooder",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-notice-wooder",
    "title": "Rising Wooders",
    "section": "When did people notice “wooder”?",
    "text": "When did people notice “wooder”?\nI start off the poster looking at what people have had to say about “wooder”. In part, this is inspired by a disagreement I’ve had with how the vowel ought to be described. With my own introspection, I think the eye-dialect version &lt;wooder&gt; is just right. I think the vowel in the first syllable is /ʊ/, or Foot class. However, Labov has suggested here and there that it’s more of an extremely raised and stereotyped realization of /ɔ/ or Thought class.\nI was explaining this to a friend, and she said “Well, you would think it was /ʊ/ growing up with the ‘wooder’ spelling in media and print.” To which I replied, “I don’t think &lt;wooder&gt; was a thing people wrote out when I was growing up.” So, that launched my first systematic exploration of ‘wooder.’\nWooder in print\nMy best approach, so far, to see how long the pronunciation [wʊɾɚ] has been represented as &lt;wooder&gt; is to do a NewsBank search for it in Philadelphia area newspapers. Those results are represented in the next figure.\n\nData Loadinglibrary(tidyverse)\nwooder &lt;- read_csv(\"data/wooder.csv\")\n\n\n\n\nplotting codeall_year &lt;- tibble(\n  year = seq(\n    min(wooder$year),\n    max(wooder$year),\n    by = 1\n  )\n)\n\nwooder |&gt;\n  full_join(all_year) |&gt; \n  replace_na(list(hits = 0)) |&gt; \n  ggplot(aes(year, hits))+\n    stat_smooth(method = \"gam\", \n                method.args = list(family = \"poisson\")) +\n    geom_point()+\n    labs(\n      title = str_wrap(\n        \"'wooder' in print\",\n        width = 30\n        ),\n      subtitle = str_wrap(\n        \"The number of hits for the word 'wooder'\n        in Philadelphia area newspapers by year\",\n        width = 40\n        ),\n      caption = \"Source: Newsbank\"\n      )+\n    theme(aspect.ratio = 5/8,\n          text = element_text(size = 16),\n          plot.subtitle = element_text(size = 10, color = \"grey80\"),\n          plot.caption = element_text(size = 10, color = \"grey80\"))\n\n\n\n\n\n\nFigure 1: Search results for &lt;wooder&gt; in print.\n\n\n\n\nIt won’t be surprising to Philly area people, but every hit prior to 1994 is from a single columnist, Clark DeLeon1. DeLeon wrote a popular column, and often had a keen ear for Philly area dialect features. But other than one columnist’s keen ear, there’s essentially nothing until about the year 2000 when a gradual increase begins. There’s a bit of a phase change in 2015, and when I looked at most of those stories, they were about Philadelphia Brewing Company’s “Holy Wooder,” a Belgian Tripel they released in honor of the papal visit to the city.\nIt’s worth saying I didn’t personally realize there was anything distinctive about the way I said water until I was 18 and in college, and someone did a double take when I said [wʊɾɚ]. That was in 2003, just before the &lt;wooder&gt; boom. Needless to say, I don’t think an 18 year old Philadelphian could repeat that experience today!\nTalking about wooder\nBy this point, I’ve gone through and listened to every token of water in the Philadelphia Neighborhood Corpus, and there are a number of points of “metalinguistic commentary”, or people talking about the word’s pronunciation. The earliest example is from 1974, when a young man said\n\nThat’s what a lot of people say, [wɑtʰɚ], cause [wʊɾɚ] sounds like W-O-O-D-D-E-R or something.\n\nThis is a really interesting comment, cause he’s specifically singling out the [ɑ] vowel pronunciation as something a lot of (presumably other) people say. In fact, two of the other instances of commentary about water only ever use the [ɑ] vowel version as something marked about New York City speech. He also combines [ɑ] with an aspirated [tʰ], which would seem to align use of [ɑ] with hyper- or hypercorrect- speech. While flapping the /t/ in water is nearly a North American standard, it seems to get almost just as much notice and attention as the vowel quality.\nAnd speaking of vowel quality! This comment also identifies the vowel category with /ʊ/, because he specifically spells it out “W-O-O-D”. So, there’s one point in my column!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-start-saying-wooder",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-start-saying-wooder",
    "title": "Rising Wooders",
    "section": "When did people start saying “wooder”?",
    "text": "When did people start saying “wooder”?\nNow, it’s possible for a linguistic feature to be part a community’s repertoire for a long time without it becoming noticed or commentable. Take the distinctively named Philly area dessert, “water ice,” as an example. This phrase contains two distinctively Philly area pronunciation features:\n\nThe pronunciation of “water” as [wʊɾɚ].\nThe pre-voiceless centralization of the onset of /ay/ to [ʌi] in “ice”.\n\nI’ve actually done a lot of research on this second feature, which is now a well established feature of the dialect. But I’m actually not aware of any public commentary about it, and when the whole phrase is rendered in eye dialect or printed on a tee-shirt, it comes out as just &lt;wooder ice&gt;, not &lt;wooder uhys&gt;\nThat is to say, just because the earliest time I can find anyone discussing “wooder” is 1974 doesn’t mean that’s when people started pronouncing it that way. In fact, it definitely means people starting pronouncing it that way well before! Figuring out the history here became the second part of the project, which I’ll write about in a second post.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#footnotes",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#footnotes",
    "title": "Rising Wooders",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you have access to NewsBank or the Inquirer Archives, the first &lt;wooder&gt; hit is from April 26, 1983 in DeLeon’s column The Scene with the lede “LANGUAGE: YO, BILL, WATCHES TAWKIN’ ABOUT?”. The “Bill” in question was William Safire.↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html",
    "title": "Using Quarto Blogs for Teaching",
    "section": "",
    "text": "This semester I was teaching two R intensive courses: Quantitative Investigations in the Social Sciences and Quantitative Methods in Linguistics. For both courses, I had students maintain Quarto blogs that they pushed to Github Classroom and I thought I’d write up my thoughts on what worked, what didn’t work, and what I wish I’d planned out in advance.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#github-classroom-basics",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#github-classroom-basics",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Github Classroom basics",
    "text": "Github Classroom basics\nFirstly, I spent relatively little time on the actual Github Classroom site. There may be more functionality to it that’s useful, but largely I didn’t need it. Each “Classroom” creates a Github Organization from which you can access every student’s assignment repository.\nCreating assignments\nTo create an assignment, you first have to set up a template repository on Github (could be under your own account or under the classroom organization) that gets associated with the assignment. Github Classroom generates an invite link that you can post to Canvas/Blackboard/whatever that will auto-fork the template for the student in the organization, and name it {name-of-assignment}-{gh-username}.\n\n\n\n\n\n\nRepository Visibility\n\n\n\nThese repositories are, by default, private! In the example repo I show here, I went in and manually changed it to be public\n\n\nFor example, here’s the repository it created when I clicked the invite link to the mapping project:\nhttps://github.com/A-S500/maps-final-JoFrhwld\nThe repository isn’t under my personal account, it’s under the A-S500 organization.\nTip 1: Take your time on the templates\nFor a blog-type assignment that students will be updating across the entire semester, take your time on the template. Get your template blog to be exactly like you want it. Obviously, students can make changes to how the blog will render, but your initial settings will have a major inertia effect.\nHere’s one thing I spent time on in set up that I’ll do differently in the future: I created blank template posts for every post students were going to do that year. For one class, I was having them write weekly reflections, so I created a directory and index.qmd for every week. e.g.\nposts/01_week/index.qmd\nposts/02_week/index.qmd\n...\nposts/15_week/index.qmd\nFor the the other course, I had them write a post for each chapter of the textbook, and the setup was similar.\nI’d do that differently now for a few reasons. First, when looking at the source code and the rendered blog, it makes it harder to eyeball where the students have created new content. It might not seem like a lot, but it’s just a little extra bit of friction to click through to an index.qmd post to find that it’s still just the original template.\nSecond, it facilitated some poor metadata practices for the students. I really couldn’t get everyone to touch the yaml header to update the date of the post if there was already a date: there, so for some people all of their posts were dated January, when I created the template.\nThoughts: The feedback pull request isn’t worth it.\nGithub Classroom will give you the option of automatically creating a feedback branch and open a pull request to leave comments on the code. While this makes sense for some assignments, these quarto blogs with their rendered html are too unwieldy to bother.\nI just opened feedback issues on the blogs referencing the specific lines I was commenting on.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#quarto-things",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#quarto-things",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Quarto Things",
    "text": "Quarto Things\nSome quarto things that I’d recommend:\nTip 2: Set up freeze: auto\n\nTo save yourself and your students’ time be sure to configure freezing in the _quarto.yml file.\nexecute: \n  freeze: auto\nThis means quarto will save the results of any executed R code, and won’t rerun the code when rendering the project unless the content of the file changes. This will save you and your students a lot of time when re-rendering blogs, especially later in the semester.\n\n\n\n\n\n\ncommit `_freeze/`\n\n\n\nBe sure to add and commit the _freeze/ directory though!\n\n\nTip 3: Encourage (enforce!) actual rendering of the blog.\nOne of the positives of any code notebook system in general is that they look nice while you’re writing, and interleave prose, code, & results. A downside for a Quarto blog, though, is that students may not realize that the code results they see in the notebook don’t get saved and committed. You (the reader/grader) will have to re-run all of the code chunks, or re-render the html blog.\nBut, as long as students click the “Render Project” button, the generated html pages will be viewable in _site, with all of the prose, code and code results… if they click “Render Project”.\nI didn’t pick up on the fact many students were just running the code in the notebook and never clicking “Render” until too late in the semester, and by that point we were very busy trying to get through the course content for me to turn that ship. That meant I had to re-render their blogs (takes time) and I had to deal with/fix any inconsistencies between our R environments (more time!)\nTip 4: Set error: true in the quarto project\nI think you should set error: true in _quarto.yml. Usually, if there’s an error in your R code, the project will fail to render. With error: true, the project will render, with the R errors included in the output.\nMaybe you’re thinking “But don’t you want to make sure that students fix the errors?” Well, yes I do, but if there’s errors in their code and they push it anyway, when you try to render the project, you’ll have to hunt down and fix the R errors before you can look at the rendered project!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#rrstudio-things",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#rrstudio-things",
    "title": "Using Quarto Blogs for Teaching",
    "section": "R/RStudio Things",
    "text": "R/RStudio Things\nTip 5: Teach them about {here} early\nSo, no matter how many times I left comments about “please use relative paths”, I would still get posts with code that looked like\n\ndata &lt;- read_csv(\"~/usernamene/Documents/Courses/blog/data/data.csv\")\n\nWhen combined with the fact that many people weren’t clicking “Render” on their projects, this meant I had to go in and fix these global paths in order to read their posts.\nThe here package works within RStudio projects to help construct reproducible paths. here::here() returns the path to the main project directory, and will concatenate any further arguments to that path. So rewriting the example above would look like:\n\nlibrary(here)\ndata &lt;- read_csv(here(\"data\", \"data.csv\"))\n\nOn the one hand, using here in this way does bypass needing to come to grips with paths, which is a crucial component of scientific computing. But on the other, you need to make a decision about where your class time is spent.\nIf you go all in on *nix-style paths and their navigation, you will burn a lot of class time and your own creative energy on a topic that is increasingly conceptually difficult for students before you even get to course content. Moreover, students won’t neatly delimit course content like “paths are utility background information, stats are a collection of theories and methods, R code is an implementation of those theories and methods.” Rather, it’ll all get dumped into one big bucket of “stats is hard.”\nOr, you could teach them to use here.\nTip 6: Figure out {renv}.\nThis is more of a “to-do”. I’m relatively new to using renv and am still trying to work out the kinks of making it work in distributed assignments like this.\nI think step one would be to make sure they’ve installed renv globally, before opening any given project. Then, in your template, commit your renv.lock file, but not the .Rprofile. Then, once they’ve created the project in RStudio, run renv::init() once, restore from the lock file.\nI think…\nTip 7: Remind them to save!\nRemind them that this is not like Google Docs or Word! The document is not auto-saving as you go along! Save save save!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#gitgrading-things",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#gitgrading-things",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Git/Grading Things",
    "text": "Git/Grading Things\nOn the reading/grading side of things, I worked out a git workflow that worked pretty well.\nTip 8: Add the student blogs as git submodules\nI originally had a grand idea of adding every student’s blog to one larger quarto project I would render, and look at all of their posts in one place, but that didn’t work out great. Instead, I created a bare bones RStudio project, and added each student’s blog repository as a submodule in a `blogs/` directory:\ngit submodule add git@github.com:...\nAfter initial setup, this meant I could pull all student’s new posts with\ngit submodule update --remote\nTip 9: Make use of RStudio’s project navigation.\nThe way I’d read and grade the blog posts was by navigating to a student’s blog repository in RStudio’s file browser, then clicking on the blog.Rproj file. This will auto open the student’s blog as an RStudio project which you can render and browse in isolation from all other students’ projects (especially if you’ve initialized renv inside).\nTip 10: Always commit all changes, but don’t push, after grading\n\n\n\n\n\n\nCommit Changes!\n\n\n\nI’m saying the same thing twice because it’s important.\n\n\nAfter re-rendering and reading the student’s blog, be sure to commit all changes, but don’t push them. This is because of a detail of how both quarto and git submodules work.\nEvery time you re-render a project, the “last modified” metadata in the rendered html files gets updated. Meaning even if you just changed one page, all of the html pages get modified.\nAdditionally, if there are uncommitted changes within the git submodule, when you run git submodule update --remote, git won’t pull down the new commits until you’ve committed those changes within then submodule.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#final-thoughts",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#final-thoughts",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOverall, I was pretty happy with the outcomes! The blog format worked well for incremental logging of course progress, and it started socializing students into the practices of collaborative coding projects.\nI should note that both classes were relatively small, roughly seminar size, so I’m not sure how this would scale up to 25+ students.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html",
    "title": "Github Onboarding with RStudio",
    "section": "",
    "text": "I’m writing this primarily for students I’ll be teaching in Spring 2023 who I want to use Git/Github with Posit Workbench.\n\nThis tutorial is appropriate for:\n\nAnyone using RStudio/RStudio Server/Posit Workbench/Posit Cloud\n\nI will assume:\n\n\nGit is already installed and available.\nYou have not already configured Git locally.\nYou cannot access the terminal.\n\n\n\nIf item number 1 is not correct, or you want more detail on using Git and Github with RStudio, you should check out Jennifer Bryan’s much more extensive Happy Git and GitHub for the useR.\nItem number 2 might be a strange assumption, but the Posit Workbench configuration I have access to actually does not allow opening a terminal.\nInstruction boxes with  should be done on github, and instruction boxes with  should be done in RStudio.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#the-target-audience",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#the-target-audience",
    "title": "Github Onboarding with RStudio",
    "section": "",
    "text": "I’m writing this primarily for students I’ll be teaching in Spring 2023 who I want to use Git/Github with Posit Workbench.\n\nThis tutorial is appropriate for:\n\nAnyone using RStudio/RStudio Server/Posit Workbench/Posit Cloud\n\nI will assume:\n\n\nGit is already installed and available.\nYou have not already configured Git locally.\nYou cannot access the terminal.\n\n\n\nIf item number 1 is not correct, or you want more detail on using Git and Github with RStudio, you should check out Jennifer Bryan’s much more extensive Happy Git and GitHub for the useR.\nItem number 2 might be a strange assumption, but the Posit Workbench configuration I have access to actually does not allow opening a terminal.\nInstruction boxes with  should be done on github, and instruction boxes with  should be done in RStudio.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-1-create-a-github-account",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-1-create-a-github-account",
    "title": "Github Onboarding with RStudio",
    "section": "Step 1: Create a Github Account",
    "text": "Step 1: Create a Github Account\nGo over to Github and create a free account.\n\n\n\n\n\n\n\n\n\n\n\nGo To\n\nhttps://github.com/\n\n\n\n\nAs suggested in Happy Git Chapter 4, it would make sense to register an account username that aligns with your professional identity.\nAfter you’ve created your free account, if you are affiliated with a university, I would also suggest applying for the education benefits here: https://education.github.com/. There are a few nice, but not mandatory, perks.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-2-configure-git-in-rstudio",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-2-configure-git-in-rstudio",
    "title": "Github Onboarding with RStudio",
    "section": "Step 2: Configure Git in RStudio",
    "text": "Step 2: Configure Git in RStudio\nNow, you need to tell Git a little bit about yourself on the computer/server you’re using RStudio on.\n\n\n\n\n\n\n\n\n\n\n\nGo To\n\nWherever you are using RStudio ( could be Posit Workbench, Posit Cloud, RStudio Server, or RStudio Desktop)\n\nThen Go To\n\nThe Console (a.k.a the R Prompt)\n\n\n\n\n\n\n\n\n\nThe Console in RStudio is here.\n\nNext, we need to tell the local version of Git who you are, specifically your username (which should match your Github username) and your email address (which should match the email address you registered for Github with).\n\n\n\n\n\n\n, \n\n\n\nIn the code below, USERNAME should be replaced with your Github username and EMAIL should be replaced with the email you registered your github account with.\n\n\n\nRun this in the R Console:\n\nsystem('git config --global user.name \"USERNAME\"')\nsystem('git config --global user.email \"EMAIL\"')",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-3-configure-rstudio-to-communicate-with-github",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-3-configure-rstudio-to-communicate-with-github",
    "title": "Github Onboarding with RStudio",
    "section": "Step 3: Configure RStudio to Communicate with Github",
    "text": "Step 3: Configure RStudio to Communicate with Github\nIn order to be able to push commits from RStudio to Github, you’ll need to set up secure communication between wherever you are using RStudio and Github. I’ll walk you through how to do this with SSH credentials. (See also Happy Git with R for personal access tokens via HTTPS).\nRStudio Configuration\n\n\n\n\n\n\n\n\n\n\n\nGo To:\n\nThe Tools menu, then Global Options\n\n\n\n\n\n\n\nThen Go To:\n\nGit/SVN from the left hand side option selector. Its icon is a cardboard box\n\n\n\n\n\n\n\nThen Go To\n\nCreate SSH Key\n\n\n\n\n\n\n\nThen\n\nThe default options should be fine to use. The passphrase here is for the ssh key. It should not be your Github password, or the password for logging into Posit Workbench or Posit Cloud. Once you’re ready, click Create.\n\nThen\n\nAfter creating the SSH key, you should see the option “View Public Key”. Click on it, and copy the text that appears.\n\n\n\n\nThis concludes everything necessary on the RStudio side of things. You should probably keep the session open so that you can come back to re-copy your public key.\nGithub Configuration\nNow, you’ll need to go over to github to add the public key to your profile.\n\n\n\n\n\n\n\n\n\n\n\nGo To\n\nYour Github Profile Settings\n\n\n\n\n\n\n\nThen Go To\n\nSSH and GPG keys from the left side menu\n\n\n\n\n\n\n\nThen\n\nClick on the New SSH key button\n\n\n\n\n\n\n\nThen\n\nGive this key an informative name so you can remember which computer it’s coming from.\n\nThen\n\nPaste the text you copied from RStudio into the Key box and click Add SSH Key.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#configured",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#configured",
    "title": "Github Onboarding with RStudio",
    "section": "Configured",
    "text": "Configured\nNow, wherever you are using RStudio from should be able to push commits to your Github account.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "",
    "text": "I’m teaching a class on quantitative methods in linguistics, and after getting tired, over the years, of debugging everyone’s computer before we could get to the content of learning R (much less quantitative methods), I decided to seize control. I’m running the course through GitHub Classroom which comes with an educational allowance for using GitHub codespaces. There have been pros and cons to running things through there:\nAfter working through some issues today, I thought I’d start a running document of decisions I’ve already made, leaving space for updates down the line. This is for my own sake, to refer back to, but also for any other poor souls out there.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-devcontainers-can-take-a-long-time-to-load.",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-devcontainers-can-take-a-long-time-to-load.",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: Devcontainers can take a long time to load.",
    "text": "Problem: Devcontainers can take a long time to load.\nLaunching a codespace can take a long time, especially if you are doing a lot of configuration at the creation time. My solution was to create my own devcontainer image, which should speed some things up.\n\nI forked the rocker-org/devcontainer-images repo\nI made some tweaks to the to the build/args.json file and the build/matrix.json file, which mostly have the effect of changing the name of the resulting devcontainer image, and the paths to source files used to generate it.\n\nI also put in a text file called .rpackages in a sensible place in the devcontainer repo and added the following lines to the dockerfile to get them installed on the image.\n\ndockerfile\n\nCOPY assets/.rpackages /home/rstudio/.rpackages\nRUN install2.r --error --skipinstalled $(cat /home/rstudio/.rpackages)",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-dont-want-a-login-screen-to-rstudio",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-dont-want-a-login-screen-to-rstudio",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: I don’t want a login screen to Rstudio",
    "text": "Problem: I don’t want a login screen to Rstudio\nBy default, if you just launch rserver from the devcontainer image (which has RStudio Server installed), you’ll get a login screen when you visit the forwarded port. I could have left this, but needing to explain “It looks like a login screen, but the username and password are just rstudio/rstudio” is too sharp an edge.\nFortunately, there’s an rstudio server devcontainer feature I can add to the template assignment that enables single sign on mode, and even updates the default working directory to be the workspace, rather than ~.\n\njson\n\n{\n  \"image\": \"ghcr.io/lin611-2024/devcontainer/bayesdevcontainer:4\",\n  \"features\": {\n    \"ghcr.io/rocker-org/devcontainer-features/rstudio-server\": {\n      \"singleUser\": true\n    }\n  }\n}",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-on-launch-rstudio-doesnt-open-a-project",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-on-launch-rstudio-doesnt-open-a-project",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: On launch, RStudio doesn’t open a project",
    "text": "Problem: On launch, RStudio doesn’t open a project\nEven if the default working directory has an .Rproj file in it, RStudio doesn’t default open it as a project on launch, which also means the git pane isn’t available. This was another sharp edge that I knew would cause me problems.\nI found a hook I could add to the system .Rprofile to fix this\n\nr\n\nsetHook(\n  \"rstudio.sessionInit\",\n  function(newSession){\n    if (newSession && is.null(rstudioapi::getActiveProject())) {\n      rstudioapi::openProject(\".\")\n    }\n  },\n  action = \"append\"\n)\nI added this to a sensible place in the devcontainer repository, then I had to make sure to copy it to the image in the Dockerfile.\n\ndockerfile\n\nCOPY --chown=rstudio:rstudio assets/.Rprofile /home/rstudio/",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-there-were-no-git-credentials-in-rstudio-git-pane",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-there-were-no-git-credentials-in-rstudio-git-pane",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: There were no git credentials in RStudio git pane",
    "text": "Problem: There were no git credentials in RStudio git pane\nIf you use the VS Code interface in a codespace, you can push and pull to the original repo, no problem. But I couldn’t do it inside the RStudio Server session. I could have had students go back to VS Code for this, but, again, another sharp edge.\nThe necessary environment variables that are available in the VS Code interface are GITHUB_USER and GITHUB_TOKEN, but they weren’t available to R. This is something that needs to be dealt with after the codespace has built (not in the devcontainer image), so I added the following to assignment template’s devcontainer.json.\n\njson\n\n \"postAttachCommand\": {\n   \"github_user\": \"echo \\\"GITHUB_USER=$GITHUB_USER\\\"&gt;&gt;~/.Renviron && echo \\\"GITHUB_TOKEN=$GITHUB_TOKEN\\\"&gt;&gt;~/.Renviron\"\n}\nThis just copies the values in the environment variables to R’s environment variables.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-wanted-the-most-up-to-date-quarto",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-wanted-the-most-up-to-date-quarto",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: I wanted the most up-to-date Quarto",
    "text": "Problem: I wanted the most up-to-date Quarto\nWe’re going to be talking a bit about authoring, sometimes using typst, and the newest update that styles tables from gt was too good to not have. This required going back to the dockerfile.\n\ndockerfile\n\nARG QUARTOVERSION=1.5.56\n\n# stuff\n\nARG QUARTOVERSION\n\nRUN cd /usr/share/ \\\n  && wget https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTOVERSION}/quarto-${QUARTOVERSION}-linux-amd64.tar.gz \\\n  && tar -xvzf quarto-${QUARTOVERSION}-linux-amd64.tar.gz \\\n  && mv quarto-${QUARTOVERSION} .quarto \\\n  && rm /usr/local/bin/quarto \\\n  && ln -s .quarto quarto\n\nENV PATH \"$PATH:/usr/share/quarto/bin\"",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-the-.rproj-file-name-didnt-match-the-repository-name",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-the-.rproj-file-name-didnt-match-the-repository-name",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: The .Rproj file name didn’t match the repository name",
    "text": "Problem: The .Rproj file name didn’t match the repository name\nThis might not have mattered too much, but GitHub classroom creates a new repository for each student called {assignment-name}-{username}. I thought it would be not quite the same as using RStudio projects locally if the project file was called assignment-template.Rproj in all cases.\nThis was another case for adding a post-attach command in the assignment template devcontainer.json.\n\njson\n\n\"postAttachCommand\": {\n  ...\n  \"project-rename\": \"(mv *.Rproj $(basename $(pwd)).Rproj || echo 'moved' )\",\n  ...\n}",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-a-github-classroom-race-condition",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-a-github-classroom-race-condition",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: A GitHub Classroom race condition",
    "text": "Problem: A GitHub Classroom race condition\nSo, when github classroom creates the student’s assignment repo, it adds two buttons to the README.md. But, if the student is quick on the draw and launch their codespace before the bot commits the change to the readme, their codespace will be a commit behind the remote, but they won’t necessarily realize this.\nThen, if they keep working and commit, they’ve got a divergent branch from the remote, and they’ll need to reconcile it, which the RStudio git gui interface doesn’t really have capacity for.\nThis is another item for the devcontainer.json\n\njson\n\n\"postAttachCommand\": {\n  ...\n  \"git-config\": \"git config pull.rebase false && git pull\",\n  ...\n}",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-codespaces-going-idle-too-quickly",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-codespaces-going-idle-too-quickly",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: Codespaces going idle too quickly",
    "text": "Problem: Codespaces going idle too quickly\nI’m still trying to debug this issue (I’ve opened an issue with GH, waiting to hear back). Students’ codespaces seem to be going idle very quickly when we’re working on an in-class assignment. This results in their RStudio session starting to return mysterious 400 errors.\nSome things I’ve already done:\n\nNew codespaces in the organization are owned by the organization.\nI’ve set a custom timeout policy for codespaces within the organization.\n\nThis didn’t seem to fix things. A few more hypotheses I have are:\n\nBecause we’re working in RStudio, largely in Quarto notebooks, and not in the VS Code interface, the codespace isn’t registering any activity.\nThere might be some kind of browser specific memory saving thing going on, but that I’m not 100% on.\n\nTo try to deal with this, I’ve added an awake.sh shell script to the .devcontainer directory.\n\nsh\n\n#!/bin/sh\nwhile true\ndo\n  echo \"Stay Awake\" \n  sleep 20\ndone\nThen added the following to my devcontainer.json\n\njson\n\n \"postAttachCommand\": {\n   ...\n   \"awake\": \"timeout --foreground 90m sh .devcontainer/awake.sh\"\n...\n}\nThis will print Stay Awake every 20 seconds for 90 minutes… hopefully keeping our RStudio sessions active long enough for a class meeting.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#rstudio-prefs",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#rstudio-prefs",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "RStudio Prefs",
    "text": "RStudio Prefs\nSome rstudio preferences I wanted to set were:\n\nRainbow parentheses\nUse of the native pipe operator\nUse of the Menlo font in the editor.\n\n\n\n\n\n\n\nUpdate (2024-09-10)\n\n\n\nI also just found out you can globallychange the default line-wrapping behavior in the source document when working in the Visual Editor. I’ll go with sentence-level wrapping, which will be nice for git diffs.\n\n\nTo get these working, I added the following to the default rstudio-prefs.json which I included in the devcontainer repository\n\njson\n\n{\n  \"save_workspace\": \"never\",\n  \"always_save_history\": false,\n  \"reuse_sessions_for_project_links\": true,\n  \"posix_terminal_shell\": \"bash\",\n  \"initial_working_directory\": \"/workspaces\",\n  \"visual_markdown_editing_is_default\": true,\n  \"editor_theme\": \"Tomorrow\",\n  \"rainbow_parentheses\": true,\n  \"insert_native_pipe_operator\": true,\n  \"server_editor_font_enabled\": true,\n  \"server_editor_font\": \"Menlo\"\n  \"visual_markdown_editing_wrap\": \"sentence\"\n}\nThen added this to the dockerfile\n\ndockerfile\n\nCOPY --chown=rstudio:rstudio assets/rstudio-prefs.json /home/rstudio/.config/rstudio/",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#keybindings",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#keybindings",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Keybindings",
    "text": "Keybindings\nThere’s really just one custom keybinding that I really like, which is  for inserting the pipe.\n\nTo make this work, I created a keybindings directory in the devcontainer repo with empty addins.json and editor_bindings.json files, and a rstudio_bindings.json with the following:\n\njson\n\n{\n  \"insertPipeOperator\": \"Cmd+.\"\n}\nThen added the following to the dockerfile\n\ndockerfile\n\nCOPY --chown=rstudio:rstudio assets/keybindings/ /home/rstudio/.config/rstudio/keybindings/",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m an Assistant Professor of Linguistics at the university of Kentucky. You can find out more about me and my research on my homepage."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I’m an Assistant Professor of Linguistics at the university of Kentucky. You can find out more about me and my research on my homepage."
  },
  {
    "objectID": "about.html#about-the-title",
    "href": "about.html#about-the-title",
    "title": "About",
    "section": "About the title",
    "text": "About the title\nIn my variety of Philadelphia English, the diphthong /aw/ merges with /æː/ before /l/. So vowel and val both sound like [væːɫ], and sometimes vow gets into the mix as well.\nThroughout grad school and a few years after, I maintained a blog called Val Systems. It’s still there, but I got out of the habit of blogging there, and eventually came to feel a bit hampered by the drafting, posting & designing process there. So now, maybe I’ll maintain Væl Space."
  },
  {
    "objectID": "about.html#site-fonts",
    "href": "about.html#site-fonts",
    "title": "About",
    "section": "Site fonts",
    "text": "Site fonts\n\nHeaders\n\nComfortaa\n\nBody\n\nPublic Sans\n\nFigures\n\nFira Sans\n\nCode\n\nFira code"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Væl Space",
    "section": "",
    "text": "Hello!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Effects and Overdispersion\n\n\n\n\n\n\n\n\n\n\n\n2024-11-20\n\n\nJosef Fruehwald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nRandom effect priors, redo\n\n\n\n\n\n\n\n\n\n\n\n2024-11-19\n\n\nJosef Fruehwald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSetting default ggplot2 colors\n\n\n\n\n\n\nggplot2\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n2024-10-01\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding up a complex {gt} table.\n\n\n\n\n\n\n\n\n\n\n\n2024-09-15\n\n\nJosef Fruehwald\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nRolling for Damage with the Central Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n2024-09-08\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting RStudio for GitHub Classrom and Codespaces\n\n\nA living document\n\n\n\n\n\n\n\n\n2024-09-06\n\n\nJosef Fruehwald\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with Object Oriented Programming in R\n\n\n\n\n\n\n\n\n\n\n\n2024-09-02\n\n\nJosef Fruehwald\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with the Discrete Cosine Transform in R\n\n\n\n\n\n\n\n\n\n\n\n2024-07-19\n\n\nJosef Fruehwald\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing FastTrackPy and aligned-textgrid\n\n\n\n\n\n\n\n\n\n\n\n2024-02-16\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nSome lessons in using Github Classroom and Codespaces\n\n\n\n\n\n\n\n\n\n\n\n2024-02-14\n\n\nJosef Fruehwald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nThinking About Hierarchical Variance Parameters\n\n\n\n\n\n\n\n\n\n\n\n2023-06-29\n\n\nJosef Fruehwald\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nGetting a sense of priors for logistic regression\n\n\n\n\n\n\n\n\n\n\n\n2023-06-28\n\n\nJosef Fruehwald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nChanging Project Defaults\n\n\n\n\n\n\n\n\n\n\n\n2023-06-16\n\n\nJosef Fruehwald\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Quarto Blogs for Teaching\n\n\n\n\n\n\n\n\n\n\n\n2023-05-03\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating DND Rolls\n\n\n\n\n\n\n\n\n\n\n\n2023-02-12\n\n\nJosef Fruehwald\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nA handy dplyr function for linguistics\n\n\n\n\n\n\n\n\n\n\n\n2023-02-05\n\n\nJosef Fruehwald\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Plotly Plot\n\n\n\n\n\n\nplotly\n\n\n\n\n\n\n\n\n\n2023-01-29\n\n\nJosef Fruehwald\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nR Package Exploration (Jan 2023)\n\n\n\n\n\n\nR\n\n\nR package exploration\n\n\n{ggforce}\n\n\n{geomtextpath}\n\n\n{ggdensity}\n\n\n\n\n\n\n\n\n\n2023-01-27\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a spectrogram in R\n\n\n\n\n\n\nhow-to\n\n\nr\n\n\n\n\n\n\n\n\n\n2023-01-22\n\n\nJosef Fruehwald\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nRising Wooders: Part 3\n\n\nPart 3: The role of r-dissimilation\n\n\n\nresearch\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\n2023-01-04\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nRising Wooders: Part 2\n\n\nPart 2: Saying Wooder\n\n\n\nresearch\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\n2023-01-01\n\n\nJosef Fruehwald\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nRising Wooders\n\n\nPart 1: Talking about Wooder\n\n\n\nresearch\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\n2022-12-31\n\n\nJosef Fruehwald\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Onboarding with RStudio\n\n\n\n\n\n\n\n\n\n\n\n2022-12-21\n\n\nJosef Fruehwald\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is R?\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n2022-12-17\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nA lesson in over-preparing\n\n\n\n\n\n\n\n\n\n\n\n2022-12-10\n\n\nJosef Fruehwald\n\n\n8 min\n\n\n\n\n\n\nNo matching items\n\nReuseCC-BY-SA 4.0"
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html",
    "title": "Rising Wooders: Part 3",
    "section": "",
    "text": "This is part 3 of my blog posts to accompany my ADS2023 Poster.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#waughter-worder-wooder",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#waughter-worder-wooder",
    "title": "Rising Wooders: Part 3",
    "section": "Waughter? Worder? Wooder?",
    "text": "Waughter? Worder? Wooder?\nIn my first post, I talked about the timeline of when Philadelphians came to realize there was something distinctive about the way we say water, and the use of &lt;wooder&gt; in print. In my second post, I modelled the timeline of when Philadelphians started saying [wʊɾɚ], which seems to have been in variation with [wɔɾɚ] and [wɔɹɾɚ] for people born across the 20th century. Now there’s just the question of why any of this happened at all.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#story-1-wɔɾɚ-raising",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#story-1-wɔɾɚ-raising",
    "title": "Rising Wooders: Part 3",
    "section": "Story 1: [wɔɾɚ] raising",
    "text": "Story 1: [wɔɾɚ] raising\nOne possibility is that changes in the pronunciation of /ɔ/ just naturally shifted the vowel’s pronunciation in water in particular. If you look at the &lt;ɔ&gt; symbol on an IPA chart, you’d probably describe it as a mid to low back vowel. However, /ɔ/ is pretty significantly raised in Philly so that for many speakers its more like a high back vowel.\n\n\n\n\n\n\n\n\n\n(a) “canonical” ɔ placement\n\n\n\n\n\n\n\n\n\n(b) Philly ɔ placement\n\n\n\n\n\n\nFigure 1: The “canonical” placement of /ɔ/ vs a typical Philadelphia /ɔ/\n\n\nIn fact, this is essentially the story we told in 100 Years of Sound Change. (“/oh/” and “open-o” are how we referred to this vowel).\n\nThe parallel back vowel is /oh/ in long open-o words talk, lost, off, and so on. Here the social stereotype is firmly fixed on one word, water, pronounced with a high back nucleus.\nLabov, Rosenfelder, and Fruehwald (2013)\n\nIn a lot of ways this story makes sense, since not only is /ɔ/ very high in Philadelphia, but with it coming immediately after a /w/, the coarticulatory effect could pull it even higher to [ʊ].\nSome shortcomings\nA shortcoming for me about this story is that there are other words with a /wɔ/ sequence that haven’t shifted to [wʊ]. For example, wall [wɔɫ] is very distinct from wool [wʊɫ] for me. Similarly, none of walk, Waldo, walnut, Walter, waltz have [ʊ] in them.\nThis is a classic kind of problem/debate in the study of sound change. Is it exceptionless? Does it happen all at once, or does it move word by word?1 This could be an example of “lexical diffusion”, where for some reason or another, change from [ɔ] ➡️ [ʊ] happened in just one word, water, and not in any of the other words that are similar.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#story-2-passing-through-worder.",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#story-2-passing-through-worder.",
    "title": "Rising Wooders: Part 3",
    "section": "Story 2: Passing through Worder.",
    "text": "Story 2: Passing through Worder.\nAnother possibility here that recently occurred to me is that maybe the [wɔɹɾɚ] tokens I found in the PNC aren’t just some third variant in the mix, but are actually evidence of how water moved from [ɔ] to [ʊ]. But it has a few steps to it.\nr-dissimilation\nUnlike a lot of eastern seaboard cities in North America, like Boston, New York City and Charleston, Philadelphia has always been an /r/ pronouncing city. Where in Boston and NYC they might drop the /r/ in park [phaːk] and car [khaː], Philly has always pronounced that /r/: [phɒɹk] and [khɒɹ].2\nThe one exception to this rule is when there is more than one /r/ in the word, and if one of those /r/s is dropable, people are likely to drop it (Ellis, Groff, and Mead 2006). This tendency is maybe most notable in the name of a nearby suburb and college “Swarthmore”, which is usually pronounced [swɑθ.mɔɹ]. In fact, Swarthmore College recently capitalized on this variation in dueling billboards.3\n\n\n\n\n\n\n\n\n\n“Swathmore” with the first &lt;r&gt; dropped.\n\n\n\n\n\n\n\n\n“Swathmore”, with the first &lt;r&gt; present.\n\n\n\n\n\nFigure 2: Dueling enregisterments.\n\n\nBut, this /r/ dissimilation isn’t restricted to just the word “Swarthmore.” The first /r/ is usually dropped in words like “quarter” or “corner”.\nReinterpretation\nI think this /r/ dissimilation is what got “worder” in the door, which I’ll try to illustrate with an example. Let’s say you’re in Philadelphia and from Philadelphia, and some shows you this coin and says “Here’s a [kʰwɔ.ɾɚ].”\n\n\n\n\n\nFigure 3: A quarter\n\n\nThere are two possible ways you could decide to interpret the underlying form of this word. The first would be to take into account /r/ dissimulation, and stick the /r/ back in, for a /kʰwɔɹ.tɹ̩/ representation. The other would be to not take into account /r/ dissimilation, and decided on a /kʰwɔ.tɹ̩/representation.\n\n\n\n\n\nFigure 4: The possible re-interpretations of kʰwɔ.ɾɚ\n\n\nNow, let’s just take off the [kʰ] at the beginning, and we have a remarkably similar kind of interpretation pathways for water!\n\n\n\n\n\nFigure 5: Possible re-interpretations of wɔ.ɾɚ\n\n\nMerger\nSo, not only is the /wɔɹ.tɹ̩/ reinterpretation plausible, I heard a lot of [wɔɹɾɚ] when I was coding the data. The next thing to know about these vowels in Philly is that the vowels /ɔɹ/ (as in tore) and /uɹ/ tour have merged to a high back position. Perhaps the most noticeable aspect of this merger is that it’s dragged /aɹ/, as in tar, to a much higher and rounded position in Philly than many other varieties.\n\n\n\n\n\n\n\n\n\ntar, tore and tour prior to the merger\n\n\n\n\n\n\n\n\ntar, tore and tour after the merger\n\n\n\n\n\nFigure 6: Before and after the merger\n\n\nIf people were inserting an /ɹ/ into the middle of water, it would belong to this /ɔɹ/ vowel, and we would expect it to be merged into /uɹ/.\nSo, once you’ve put an /ɹ/ into the middle of water and merged it with /uɹ/, what do you get if you then do /r/ dissimilation on that?\nWooder!\n\n\n\n\n\nFigure 7: My proposed pathway of “wooder” development.\n\n\nShortcomings\nSo, a shortcoming of this story about how we got “wooder” is that it could seem a little convoluted. Like we decided to just stick an /ɹ/ in just to delete it later, and voilà! Wooder!\nBenefits\nOn the other hand, there are a bunch of [wɔɹɾɚ] tokens in the Philadelphia Neighborhood Corpus. It also would account for why non of the other /wɔ/ words had this shift to [ʊ]. It also captures a but of what’s going on with daughter as satirized by the SNL skit Murder Durder.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#which-story-is-the-right-one",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#which-story-is-the-right-one",
    "title": "Rising Wooders: Part 3",
    "section": "Which story is the right one?",
    "text": "Which story is the right one?\nI’d say I need to do a bit more work, looking at the acoustic data more closely, looking at how individuals vary, as well as looking at daughter and the other /ɔɹ/ words that both have and don’t have /r/ dissimilation to really get closer to settling the issue.\nBut I think the way you react to each account could shed a little light on the kind of linguistic analyses you prefer. I really like Story 2. All the pieces that make it work are happening and have been described already, so I didn’t need to describe any new phenomenon to get “water” to turn into “wooder”. It just got pulled up in the interplay of different sound changes. It looks like a beautiful pattern, to me.\nHowever, maybe Story 2 looks overwrought to you. “Words just do things sometimes!” you might say. “The /ɔ/ was rising, and there’s a /w/ right there!” And you know what, fair enough! That was my own analysis until about a year ago as well.\nThe good news is that Story 2 has piqued my interest, which means I’m going to be digging into this more in the near future. And maybe as I go I’ll find the “passage through worder” account unsupported, but for right now it seems really plausible to me!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#footnotes",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#footnotes",
    "title": "Rising Wooders: Part 3",
    "section": "Footnotes",
    "text": "Footnotes\n\nLabov (1981), Bybee (2002), Phillips (2006), Fruehwald (2007), and many many others↩︎\nI’ll come back to that different vowel in a sec.↩︎\n\n\n\n\nHave you seen the new signs at the (SEPTA?) train station? 👀 pic.twitter.com/3DYfiGcRPs\n\n— Swarthmore College ((swarthmore?)) August 26, 2022\n\n↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "",
    "text": "I’m currently teaching a computational linguistics course, and for just the second time, I’m using Github Classroom to manage assignments. I might make a few posts about what I’ve learned in the process, but for now, I think I’ll focus on what’s gone right, and what’s been more challenging in using Github Codespaces.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html#github-codespaces",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html#github-codespaces",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "Github Codespaces",
    "text": "Github Codespaces\nA Github Codespace is basically a virtual machine hosted by Github with VS Code as the default interface. First of all, since running a codespace involves data storage and running computation, they usually cost extra money, but you can get an educational discount to $0, as described here.\nWhy Codespaces?\nThe number 1 reason to use some kind of cloud computing environment for teaching a computational course is to avoid the development environment paradox I’ve experienced:\n\n\n\n\n\n\nThe development environment paradox\n\n\n\nIf you are teaching a computational course with \\(N\\) students using their own computers, the number of of development environments to debug is \\(&gt;N\\).\n\n\nWhen I previously taught an NLP course, we used replit, but I increasingly felt the goals of the platform were inappropriate for an educational context (e.g. they have “bounties” with dollar amounts attached to them), and they’re sunsetting their educational program anyway.\nWith Codespaces, I’m still getting the benefits of having full control over the students’ development environment, but the way they interact with it will be very similar to what it’s like to interact with programming in a local environment.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html#how-ive-customized-the-codespaces",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html#how-ive-customized-the-codespaces",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "How I’ve customized the Codespaces",
    "text": "How I’ve customized the Codespaces\nWhen you launch a codespace, the instructions for how it should be configured are located in .devcontainer/devcontainer.json. You can check out how I’ve customized my assignment template repository here. I’ve left most of the configuration options at their default values, but here’s what I’ve customized.\nThe postCreateCommand\n\nAfter initially setting up the codespace, you can have it run shell commands, like installing any python dependencies. I wound up wanting it to do so many things that I’ve moved them into a separate config.sh, which gets run like this\n\njson\n\n{\n \"postCreateCommand\": \"sh .devcontainer/config.sh\"\n}\nInstalling python requirements\nFirst, I need the codespace install any python requirements for the assignment:\n\nsh\n\necho \"Installing Developer Requirements\"\npip3 install --user -r requirements/requirements.txt\nConfigure merge commits\nWe ran into a problem where, if I made some changes to which tests should run for the autograder, it resulted in a divergent history between the remote git repo and the student’s local repo, which VS Code does not present in the most user friendly way.\nSo, in my post-creation config file, I’ve added merge commit configuration\n\nsh\n\necho \"Merge Commits Only\" \ngit config pull.rebase false\nA pre-commit hook\nWe also ran into a problem where I didn’t always remember to run pip freeze after writing an assignment. When students tried to run the code that I said would just work, it didn’t. While learning “module not found” means you should just run pip install is a valuable lesson, I decided to add a pre-commit git hook to run pip freeze and add it to the commit. This also has the benefit that if students install a package and use it, tests should still all pass.\nI saved this hook to .devcontainer/pre-commit:\n\nsh\n\n#!/bin/sh\npip freeze &gt; requirements/requirements.txt\ngit add requirements/requirements.txt\nAnd added the following to .devcontainer/config.sh\n\nsh\n\necho \"pip freeze pre commit\"\nln .devcontainer/pre-commit .git/hooks/\nchmod +x .git/hooks/pre-commit\nTesting\nGithub Classroms lets you write unit tests that will “autograde” submissions. I wanted students to be able to run these tests locally, so they could tell where things weren’t going right. So I added the following VC Code customizations to devcontainer.json.\n\njson\n\n{\n  \"customizations\": {\n        \"vscode\": {\n            \"settings\": {\"python.testing.pytestArgs\": [\n                \"tests\"\n            ],\n            \"python.testing.unittestEnabled\": false,\n            \"python.testing.pytestEnabled\": true}\n        }\n    }\n}\nThis sets up the testing pane in their VS Code automatically.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html#shouldnt-debugging-be-part-of-the-learning-process",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html#shouldnt-debugging-be-part-of-the-learning-process",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "Shouldn’t debugging be part of the learning process?",
    "text": "Shouldn’t debugging be part of the learning process?\nMost of these customizations are me trying to foresee and forestall buggy problems students are going to run into. It’s worth asking whether that’s counterproductive. Maybe students should run into these problems and figure out how to fix them, since that’s more realistic to what it’s like to do computational work.\nTo that, I’d say maybe, depending on the level of the course. But if both programming and the course content are all new to many students in the class, I think they just need to experience things working first. When first starting out, you’re faced with a real problem of how to attribute errors. If you click the git Sync button in VS Code and this error pops up:\n\n\n\n\nYou have no idea whether:\n\nYou clicked the wrong button.\nYou’re not supposed to click any button.\nYou actually need to click the button twice.\nThe code you wrote was wrong.\nThe internet is temporarily down.\nYour computer is broken.\nYou need to install an additional program.\nYou’re too stupid to take this course.\nIt actually worked and the error message was an error.\n…\n\nWithout a baseline framework for understanding how things are working, you can’t process and accommodate error messages like this, so, honestly, better for me, as the instructor, to try to make things just work, and present errors and bugs in a planned and structured way.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html",
    "title": "Changing Project Defaults",
    "section": "",
    "text": "I’ve moved a bunch of R defaults that I want for each post from .Rprofile into _defaults.R, and now run source(here::here(\"_defaults.R\")) in each post where I want them. That looks like more work, but it actually makes things run a bit faster with the way Quarto runs R and freezes outputs.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#upshot",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#upshot",
    "title": "Changing Project Defaults",
    "section": "",
    "text": "I’ve moved a bunch of R defaults that I want for each post from .Rprofile into _defaults.R, and now run source(here::here(\"_defaults.R\")) in each post where I want them. That looks like more work, but it actually makes things run a bit faster with the way Quarto runs R and freezes outputs.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#initial-defaults",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#initial-defaults",
    "title": "Changing Project Defaults",
    "section": "Initial Defaults",
    "text": "Initial Defaults\nAround when I was setting up this blog project, I decided that I wanted some consistent theming for the figures so that they would fit in nicely into the rest of the blog, but I didn’t want to have to include a megablock of code in every post that looked like this:\n\nlibrary(ggplot2)\nlibrary(khroma)\nlibrary(ggdark)\nlibrary(showtext)\nlibrary(colorspace)\n\n# get Fira Sans from google\nfont_add_google(name = \"Fira Sans\", family = \"Fira Sans\")\nshowtext_auto()\nbody_bg &lt;- \"#222222\"\nplot_bg &lt;- darken(\"#375a7f\", 0.50)\n\nmajor &lt;- lighten(\n  plot_bg,\n  amount = 0.25\n)\n\nminor &lt;- lighten(\n  plot_bg,\n  amount = 0.125\n)\n\nstrip_bg &lt;- lighten(plot_bg, 0.5)\n\ntheme_set(dark_theme_gray(base_size = 12) + \n            theme(text = element_text(family = \"Fira Sans\"),\n                  plot.background = element_rect(fill = plot_bg),\n                  panel.background = element_blank(),\n                  panel.grid.major = element_line(color = major, linewidth = 0.2),\n                  panel.grid.minor = element_line(color = minor, linewidth = 0.2),\n                  legend.key = element_blank(),\n                  strip.background = element_rect(fill = strip_bg),\n                  strip.text = element_text(color = body_bg),\n                  axis.ticks = element_blank(),\n                  legend.background = element_blank()))\n\noptions(\n  ggplot2.discrete.colour = khroma::scale_color_bright,\n  ggplot2.discrete.fill = khroma::scale_fill_bright,\n  ggplot2.continuous.colour = khroma::scale_color_batlow,\n  ggplot2.continuous.fill = khroma::scale_fill_batlow\n)\n\nAll that means I can just do some minimal ggplot2 code in each post and it’ll look something like this:\n\ndata(penguins, package = \"palmerpenguins\")\n\nggplot(\n  penguins, \n  aes(\n    x = bill_length_mm,\n    y = bill_depth_mm,\n    color = species\n  )\n)+\n  geom_point()\n\n\n\n\n\n\n\nSo, I stuck that big block of code into the .Rprofile for the blog project so that every time I opened the project, R would automatically source it. Nice, right?",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#the-heaviness-of-.rprofile",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#the-heaviness-of-.rprofile",
    "title": "Changing Project Defaults",
    "section": "The heaviness of .Rprofile\n",
    "text": "The heaviness of .Rprofile\n\nI started realizing this wasn’t optimal every time I re-rendered the blog for a new post. I have my quarto set to “freeze” each post after it’s rendered, meaning it won’t re-run all of the R code in a post unless I make a change to it, instead using the output of the previous time it ran. That’s a time saver, cause even with many very simple posts with code, it just takes a while to run everything.\nThe issue was, even with freeze: true, Quarto would still source .Rprofile on every post. Which means that big block of code, including the call to showtext::font_add_google() would run for every post when I re-rendered the blog. And that was starting to get tedious!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#moving-to-_defaults.r",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#moving-to-_defaults.r",
    "title": "Changing Project Defaults",
    "section": "Moving to _defaults.R\n",
    "text": "Moving to _defaults.R\n\nSo, I moved all of the customization code from .Rprofile into _defaults.R file. I forget where I saw a _defaults.R first, but I think it was in some repository maintained by Hadley Wickham. The downside is that it’s not as automatic as .Rprofile, in that I need to source it at the start of every post. That would be annoying if I was going to write the path out by hand, but it’s a little easier with here::here().\n\nsource(\n  here::here(\"_defaults.R\")\n)\n\nThe major upside, though, is that sourcing code gets frozen along with all of the other code chunks in a post! So when I re-render the whole blog, Quarto won’t re-run all of the code in _defaults.R unless the code has changed in a post. Overall, it feels worth it!.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html",
    "title": "Making a Plotly Plot",
    "section": "",
    "text": "I’m a bit nervous about investing time into an interactive plotting framework after getting burned by Google Motion Charts.1 But, plotly seems to work even offline, which I think means once I’ve installed it, it doesn’t depend on a service or code hosted by the plotly company. That makes me feel a little more confident. I’d like to build some animations in it, but that means learning how it works, so here I go!\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(palmerpenguins)",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#basic-scatter.",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#basic-scatter.",
    "title": "Making a Plotly Plot",
    "section": "Basic scatter.",
    "text": "Basic scatter.\nFollowing the book and the docs, it looks like if I were to take the “layers” analogy to building a plot, the most basic layer function is going to be plotly::add_trace(). Data gets mapped to plot aesthetics with function notation.\n\nplot_ly(\n  data = penguins,\n  x = ~bill_length_mm,\n  y = ~bill_depth_mm,\n  color = ~species\n) |&gt; \n  add_trace(\n    type = \"scatter\",\n    mode = \"markers\",\n    size = 4\n  )\n\n\n\n\n\nSome thoughts:\n\nI think the type argument defines the kind of “space” the plot is placed in? Putting in an unsupported type returns a pretty diverse set of options that’s leaving me a bit confused about the exact work this argument does.\nI think mode is how you go about defining the plotted geometry, with \"markers\" being points.\nIt’s nice how the points default to ColorBrewer Dark2 with a slight transparency for overplotting.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#theming",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#theming",
    "title": "Making a Plotly Plot",
    "section": "Theming",
    "text": "Theming\nIt looks like the approach to theming is to just set everything by hand in plotly::layout(). This took a little bit of messing around with to find what all of the various parameters are called in plotly. My ggplot2::theme() translations are:\n\n\nggplot2\nplotly\n\n\n\nplot.background\npaper_bgcolor\n\n\npanel.background\nplot_bgcolor\n\n\npanel.grid.major.[x,y]\n[xy]axis\n\n\n\nAnother thing to note is that to get a transparent layer, you need to give it a hex code with 00 transparency at the end, rather than an NA or NULL value.\nI have a few of these colors defined in my blog .Rprofile.\n\nplot_bg\n\n[1] \"#122F4A\"\n\nmajor\n\n[1] \"#4C5D75\"\n\nminor\n\n[1] \"#31455E\"\n\n\nAlso, to get the right font family, you have to reference fonts you’ve imported in the html page, rather than fonts imported into R with showtext.\nHere it is in plotly.\n\nplot_ly() |&gt; \n  add_trace(\n    data = penguins,\n    x = ~bill_length_mm,\n    y = ~bill_depth_mm,\n    color = ~species,\n    type = \"scatter\",\n    mode = \"markers\"\n  ) |&gt; \n  layout(\n    plot_bgcolor = \"#ffffff00\",\n    paper_bgcolor = plot_bg,\n    font = list(\n      family = \"Fira Sans\",\n      color = \"#fff\"\n    ), \n    xaxis = list(\n      gridcolor = minor\n    ),\n    yaxis = list(\n      gridcolor = minor\n    )    \n  )",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#ggplotly",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#ggplotly",
    "title": "Making a Plotly Plot",
    "section": "ggplotly",
    "text": "ggplotly\nplotly also has a the ability to convert ggplot2 plots into plotly plots, at least somewhat. Here’s how it does by default.\n\npenguin_plot &lt;- \n  ggplot(\n    data = penguins, \n    aes(x = bill_length_mm, \n        y = bill_depth_mm, \n        color = species\n      )\n  ) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n\nggplotly(penguin_plot)\n\n\n\n\n\nSo, it looks like the panel.background = element_blank() I set in my blog theme doesn’t translate over in the conversion. Which is honestly a good illustration of why its probably worth learning a little bit about how the actual plotly system works, even if you’re going to mostly be interacting with it through plotly::ggplotly() like I am\n\nggplotly(penguin_plot) |&gt; \n  layout(\n    plot_bgcolor = \"#ffffff00\"\n  )",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#footnotes",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#footnotes",
    "title": "Making a Plotly Plot",
    "section": "Footnotes",
    "text": "Footnotes\n\nReally, it’s the deprecation of Flash, but Google never updated how the motion charts work.↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html",
    "title": "Getting a sense of priors for logistic regression",
    "section": "",
    "text": "This summer, I’ve been spending some time getting more familiar with the nitty-gritty of Bayesian models, and I’m working on some modelling in Stan for logistic regression. Setting up the basic model formula in Stan for logistic regression is pretty straightforward:\nAnd then you “just” need to set a prior for for the intercept and slope terms. But I hadn’t thought about how my impulses for setting priors for ordinary linear models could be pathological for logistic regression!\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(gt)\nsource(here::here(\"_defaults.R\"))",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#simulating-some-intercepts",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#simulating-some-intercepts",
    "title": "Getting a sense of priors for logistic regression",
    "section": "Simulating some intercepts",
    "text": "Simulating some intercepts\nLet’s start by assuming that the intercept value should be about 0, which corresponds to a probability of 0.5.\n\nzero_intercept &lt;- 0\n\n## inverse logit\nplogis(zero_intercept)\n\n[1] 0.5\n\n\nSo, I’ll normal distribution with a mean of 0, but what should its standard deviation be? My impulse from gaussian models is that I don’t want to set the standard deviation too small, cause then I’d be using an overly informative prior. Let’s simulate some intercepts from normal distributions with sds of 1, 3, and 5.\n\ntibble(\n  sd = c(1, 3, 5)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    intercepts = list(tibble(\n      intercept = rnorm(\n        1e6, \n        mean = 0, \n        sd = sd)\n    ))\n  ) |&gt; \n  unnest(intercepts)-&gt;\n  intercept_samples\n\nAnd we can visualize these samples from the prior like so.\n\nintercept_samples |&gt; \n  ggplot(\n    aes(\n      intercept, \n      fill = factor(sd)\n    )\n  )+\n    stat_slab(normalize = \"panels\")+\n    facet_wrap(~sd)+\n    theme_no_y()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\n\nFigure 1: Samples from normal priors.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#how-it-looks-in-the-probability-space",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#how-it-looks-in-the-probability-space",
    "title": "Getting a sense of priors for logistic regression",
    "section": "How it looks in the probability space",
    "text": "How it looks in the probability space\nSo, maybe we should go with the ~normal(0,5) prior, so as to not to be overly informative. Real quick, though, let’s plot these distributions in the probablity space.\n\nintercept_samples |&gt; \n  ggplot(\n    aes(\n      plogis(intercept), \n      fill = factor(sd)\n    )\n  )+\n    stat_slab(normalize = \"panels\")+\n    facet_wrap(~sd)+\n    theme_no_y()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\n\nFigure 2: Inverse logit samples from normal priors.\n\n\n\n\nWait! My uninformative ~normal(0,5) prior is suddenly looking pretty informative. The probability mass for the priors with larger standard deviations is ammassed closer to 0 and 1, rather than mostly toward the middle at 0.5! If we just roughly look at roughly how much probability mass each prior puts below 0.1 versus the middle 0.1, it’s pretty clear.\n\nCodeintercept_samples |&gt; \n  mutate(\n    prob_intercept = plogis(intercept)\n  ) |&gt; \n  group_by(sd) |&gt; \n  summarise(\n    under_5 = mean(\n      prob_intercept &lt; 0.1\n    ),\n    middling = mean(\n      prob_intercept &gt; 0.45 &\n        prob_intercept &lt; 0.55\n    )\n  ) |&gt; \n  gt() |&gt; \n    fmt_number(\n      decimals = 3, \n      columns = 2:3\n    ) |&gt; \n    cols_label(\n      under_5=\"bottom 0.1\",\n      middling = \"middle 0.1\"\n      \n    )\n\n\n\n\n\nsd\nbottom 0.1\nmiddle 0.1\n\n\n\n1\n0.014\n0.159\n\n\n3\n0.231\n0.053\n\n\n5\n0.330\n0.032",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#the-upshot",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#the-upshot",
    "title": "Getting a sense of priors for logistic regression",
    "section": "The Upshot",
    "text": "The Upshot\nThe reason the symmetric and spread out distributions in the logit space turn into sharp bimodal distributions in the probability space is because the (inverse) logit transform is non linear. Basically every value below about -4.5 is going to get squished to 0, and basically every value above 4.5 is going to get squished to 1. If I want the probability distribution over the intercept to be roughly unimodal in the probability space, then the standard deviation in the logit space should be relatively small to avoid extreme values!\nIf we take 0.01 and 0.99 as about most extreme values, in the probability space, that the intercept could be, we can convert that to logit, and divide by 3 to get a good standard deviation for the prior, since almost all data falls within 3sds of the mean.\n\nqlogis(0.99)/3\n\n[1] 1.531707\n\n\nThe prior I had just above, with sd=1, has a slightly narrower range in the probability space.\n\nplogis(1*3)\n\n[1] 0.9525741\n\n\n\nCodetibble(\n  sd = c(1, 1.5)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    intercepts = list(tibble(\n      intercept = rnorm(\n        1e6, \n        mean = 0, \n        sd = sd)\n    ))\n  ) |&gt; \n  unnest(intercepts)-&gt;\n  new_intercept_samples\n\n\nThe ~normal(0, 1.5) prior looks almost like a uniform distribution in the probability space.\n\nCodenew_intercept_samples |&gt; \n  ggplot(\n    aes(\n      plogis(intercept), \n      fill = factor(sd)\n    )\n  )+\n    stat_slab(normalize = \"panels\")+\n    facet_wrap(~sd)+\n    theme_no_y()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\n\nFigure 3: Inverse logit samples from normal priors.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html",
    "title": "Rising Wooders: Part 2",
    "section": "",
    "text": "This is part 2 of my blog posts to accompany my ADS2023 Poster.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#saying-wooder",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#saying-wooder",
    "title": "Rising Wooders: Part 2",
    "section": "Saying Wooder",
    "text": "Saying Wooder\nAs I said in the previous post, people seem to be very aware now that pronouncing “water” as [wʊɾɚ] is a feature of Philadelphia English, but the surge in written recognition from 2000 onwards isn’t an accurate indicator of when people started pronouncing it that way.\nThe order in which I actually approached this is a little different from how it’s laid out in the poster, but for the sake of blogginess, I’ll describe it in more chronological order. My bias was that this was a very old feature of Philadelphia English, so I started with the oldest records of Philadelphia speech I know of in the Linguistic Linguistic Atlas of the Middle and South Atlantic States (LAMSAS) (Kretzschmar et al. 1993).",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#written-records",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#written-records",
    "title": "Rising Wooders: Part 2",
    "section": "Written Records",
    "text": "Written Records\nLAMSAS was a linguistic atlas project carried out between 1933 and 1974. The interviews had a heavy focus on regional and dialect words, and for the earliest interviews, the only records available are fieldworker transcriptions. In 1939, Guy Lowman interviewed 8 people in Philadelphia, and for one target item, transcribed how they pronounced “glass/tumbler of water.”\n\nLAMSAS Informant Demographics\n\nLAMSAS ID\nGender\nRace\nAge\nYear of Birth\n\n\n\nPA1G\nman\nwhite\n70\n1869\n\n\nPA1A\nman\nwhite\n68\n1871\n\n\nPA1C\nwoman\nwhite\n68\n1871\n\n\nPA1E\nman\nwhite\n68\n1871\n\n\nPA1D\nman\nwhite\n66\n1873\n\n\nPA1F\nman\nwhite\n62\n1877\n\n\nPA1H\nwoman\nwhite\n59\n1880\n\n\nPA1B\nman\nwhite\n43\n1896\n\n\n\nI’m not going to have much more to say about these LAMSAS speakers, though, because they were all transcribed as saying [wɔɾɚ], which would be the same vowel as in wall or walk, not as in wood. In fact, I’ve done some more combing through the LAMSAS records from counties surrounding Philadelphia in both Pennsylvania and New Jersey, and there are no records of [wʊɾɚ].\nWooderless?\nOn the one hand, I was really surprised by this, but I don’t know why I should have been. There’s really no reason why [wʊɾɚ] should have been a long-standing feature of the dialect as long as a century ago. Moreover, Tucker (1944) is often cited as the earliest description of the Philadelphia dialect,1 and he includes a few word-by-word descriptions that ring true based on my own Philly upbringing like:\n\n“spigot, pronounced spicket and commonly used for ‘faucet.’”\n“taffy ‘lollipop,’ known in some places as a ‘sucker.’”\n“yo […] used especially by children in calling one another”\n\nOthers must’ve fallen out of use before my time, and I’m completely unfamiliar with them, such as\n\n“yakers (or yakes or yacks), or yakers on it! ‘I claim it’; also Goods (on it)I ‘I claim a share.’”\n“this after, short for ‘this afternoon.’”\n\nThe point, though, is that he does not mention anything about the word “water” at all. So sometime around the early 1940s, the pronunciation [wʊɾɚ] was either non-existent or used too infrequently for either Tucker or Lowman to note it.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#acoustics",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#acoustics",
    "title": "Rising Wooders: Part 2",
    "section": "Acoustics",
    "text": "Acoustics\nThis is where I would normally turn to acoustic data in the Philadelphia Neighborhood Corpus, but there are a few complications there. First and foremost, I really don’t trust any automated system to be able to place a firm boundary between where the /w/ ends and the vowel begins in a word like “water”, especially because I wouldn’t really trust myself to be able to do so. So, while I do have access to acoustic data for all tokens of “water”, I don’t have a high degree of trust in them as they are.\nThe exciting news is that the longest ago born person in the PNC (1890) overlaps with the latest born person in LAMSAS (1896), meaning we have some degree of data continuity in year of births ranging from 1869 through to 1990. But, the fact that the LAMSAS data only exists in transcription form means I need to do things the old fashioned way and listen to all of these tokens and impressionistically code them.\nSo that’s what I did, starting with the speaker born in 1890.\nWorder!\nI’ve really lucked out that the speaker born in 1890 (interviewed in 1973) talked about water a lot. And, to my surprise, l coded most of his pronunciations as [wɔɾɚ], just like Guy Lowman’s informants from 1939. However, while he never said [wʊɾɚ], there were a bunch of tokens where I could hear an [ɹ] before the flap, and could see a falling F3, characteristic of [ɹ]s as well. Here’s a representative spectrogram.\n\nlibrarieslibrary(tidyverse)\nlibrary(khroma)\nlibrary(scales)\nlibrary(geomtextpath)\nlibrary(ggnewscale)\n\nlibrary(tuneR)\nlibrary(seewave)\n\nlibrary(readtextgrid)\n\nsource(here::here(\"_defaults.R\"))\n\n\n\nloading datawater_wav &lt;- readWave(\"data/water.wav\")\nwater_formant &lt;- read_csv(\"data/water.csv\")\nwater_tg &lt;- read_textgrid(\"data/water.TextGrid\")\n\n\n\nspectrogram processingwater_wav |&gt; \n  spectro(\n    # window length, in terms of samples\n    wl = 0.005 * water_wav@samp.rate, \n    # window overlap\n    ovlp = 90, \n    plot = F\n    ) -&gt; spect\n\n# \"dynamic range\"\ndyn &lt;- -50\n\ncolnames(spect$amp) &lt;- spect$time\nrownames(spect$amp) &lt;- spect$freq\n\nspect_df &lt;- \n  spect$amp |&gt; \n  as_tibble(rownames = \"freq\") |&gt; \n  pivot_longer(-freq, names_to = \"time\", values_to = \"amp\") |&gt; \n  mutate(freq = as.numeric(freq),\n           time = as.numeric(time)) |&gt; \n  # floor at the dynamic range\n  mutate(amp = case_when(amp &lt;= dyn ~ dyn,\n                         TRUE ~ amp)) \n\n\n\ntextgrid processingwater_labels &lt;- \n  water_tg |&gt; \n  filter(tier_num == 1) |&gt; \n  mutate(ipa_label = case_when(text == \"W\" ~ \"wɔ(ɹ)\",\n                               text == \"AO1\" ~ \"wɔ(ɹ)\",\n                               text == \"T\" ~ \"ɾ\",\n                               text == \"ER0\" ~ \"ɚ\")) |&gt; \n  group_by(ipa_label) |&gt; \n  summarise(\n    xmin = min(xmin),\n    xmax = max(xmax)\n    ) |&gt; \n  arrange(xmin) |&gt; \n  rowwise() |&gt; \n  mutate(midpoint = median(c(xmin, xmax)))\n\nwater_boundaries &lt;- \n  water_labels |&gt; \n  select(-midpoint) |&gt; \n  pivot_longer(-ipa_label) |&gt; \n  select(value) |&gt; \n  distinct()\n\n\n\nformant track processingformant_df &lt;- \n  water_formant |&gt; \n  select(time, f1, f2, f3) |&gt; \n  pivot_longer(\n    -time, \n    names_to = \"formant\",\n    values_to = \"hz\"\n    ) |&gt; \n  mutate(\n    formant = toupper(formant),\n    formant_num = str_extract(formant, \"\\\\d\"),\n    formant_num = as.numeric(formant_num)\n    )\n\n\n\nplotting codespect_df |&gt; \n  ggplot(aes(time, freq*1000))+\n    stat_contour(\n      aes(z = amp, fill = after_stat(level)),\n      geom = \"polygon\",\n      bins = 500\n      )+\n    scale_fill_grayC(reverse = T, guide = \"none\")+\n    geom_vline(\n      data = water_boundaries, \n      aes(xintercept = value),\n      linetype = 2, \n      color = \"grey70\"\n      )+\n    geom_text(\n      data = water_labels,\n      aes(\n        x = midpoint, \n        label = ipa_label\n        ),\n      family = \"Fira Sans\",\n      y = 5000,\n      size = 6\n    )+\n    new_scale_fill()+\n    geom_labelline(\n      data = formant_df,\n      aes(\n        x = time,\n        y = hz,\n        fill = formant,\n        color = formant,\n        label = formant,\n        hjust = formant_num/10\n        ),\n      textcolor = \"white\",\n      linewidth = 1.5\n      )+\n    scale_fill_manual(\n      values = c(\"#33BBEE\", \"#EE3377\", \"#009988\"),\n      guide = \"none\"\n    )+\n    scale_color_manual(\n      values = c(\"#33BBEE\", \"#EE3377\", \"#009988\"),\n      guide = \"none\"\n    )+\n    scale_y_continuous(expand = expansion(mult = 0),\n                       labels = label_comma())+\n    coord_cartesian(ylim = c(0, 5500))+\n    labs(\n      x = \"time (s)\", \n      y = \"freq (hz)\",\n      title = '\"water\" spectrogram',\n      subtitle = str_wrap(\n        \"sample token from a speaker born in 1890\n        with a dropping F3 prior to the flap\",\n        width = 40\n        )\n      )+\n    theme(aspect.ratio = 5/8,\n          text = element_text(size = 16),\n          plot.subtitle = element_text(size = 10, color = \"grey80\"),\n          plot.caption = element_text(size = 10, color = \"grey80\"))\n\n\n\n\n\n\nFigure 1: Spectrogram of a token of ‘water’ from the PNC, illustrating a dropping F3 prior to the flap.\n\n\n\n\nThe coding scheme\nThis pronunciation with an [ɹ] hasn’t really been discussed by anyone, but it is reminiscent of how daughter is sometimes pronounced in Philly, as they recently (kinda2) portrayed in the SNL satirization of Mare of Easttown, Murder Durder.\nI had to take this variant into account in my coding. In fact, I think it’s actually key to understanding where wooder came from. So my coding scheme had the following values:\n\nʊ - any high, rounded realization without a [ɹ]\nɔɹ - any mid to high-ish rounded realization with a [ɹ]\nɔ - any lowish to mid rounded realization without an [ɹ]\nɑ - any low, unrounded realization\n\nThis unrounded variant mostly shows up when people are talking about the pronunciation of water, but not exclusively.\nThe model\nSince this is already getting to be a pretty long post, I’ll just skip to the model results! I fit a multilevel multinomial logistic regression model3 with these 4 variants as outcomes.\n\nlibrarieslibrary(brms)\nlibrary(tidybayes)\n\n\n\nload brms modelmodel &lt;- read_rds(\"data/multinomial_linear.rds\")\n\n\n\nget model estimatesnewdat &lt;- tibble(\n  dob = 1869:1990,\n  decade = (dob - 1950)/10\n  )\n\nests &lt;- \n  newdat |&gt; \n  add_epred_draws(model, re_formula = NA) |&gt; \n  group_by(dob, .category) |&gt; \n  median_hdci(.epred, .width = seq(0.2, 0.8, by = 0.1)) |&gt; \n  mutate(code = case_when(.category == \"0_a\" ~ \"ɑ\",\n                          .category == \"1_oh\" ~ \"ɔ\",\n                          .category == \"2_ohr\" ~ \"ɔr\",\n                          .category == \"3_u\" ~ \"ʊ\"), \n         code = as.factor(code) |&gt; fct_rev()) \n\n\n\nplot codeests |&gt; \n ggplot(aes(dob))+\n    geom_ribbon(\n      aes(\n        ymin = .lower, \n        ymax = .upper, \n        group = paste(.width, .category), \n        fill = .category\n        ),\n      alpha = 0.2)+\n    scale_fill_bright(guide = \"none\")+\n    facet_wrap(~code)+\n    scale_y_continuous(\n      expand = expansion(mult = 0), \n      limits = c(0,1), \n      breaks = c(0, 0.5, 1)\n      )+\n    labs(x = \"year of birth\",\n         y = \"vowel probability\",\n         title = \"variant probability\",\n         subtitle = str_wrap(\n           \"the probability of one of four vowel variants in water'\n           being used for different year of birth cohorts, as modeled by a\n           multinomial logistic regression\",\n           width = 60\n           )\n         )+\n    theme(aspect.ratio = 5/8,\n          text = element_text(size = 16),\n          plot.subtitle = element_text(size = 8, color = \"#ebebeb\"),\n         ) \n\n\n\n\n\n\nFigure 2: Modeled estimates of the probability of one of the 4 vowel variants being used in water.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#wooder-on-the-rise",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#wooder-on-the-rise",
    "title": "Rising Wooders: Part 2",
    "section": "Wooder on the rise",
    "text": "Wooder on the rise\nSo it looks like the shift from [wɔɾɚ] to [wʊɾɚ] was shift that occurred in Philadelphia during the 20th century alongside many others. But it also seems like it was not as simple as a categorical pronunciation jump nor a gradual shift from [ɔ] to [ʊ] because we also have this rhotic variant [wɔɹɾɚ] in the mix.\nI actually think the [wɔɹɾɚ] variant is key to figuring out how we got from [wɔɾɚ] to [wʊɾɚ], specifically because of another pattern in Philadelphia English, r-dissimilation, which I’ll hopefully write up for a post tomorrow!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#footnotes",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#footnotes",
    "title": "Rising Wooders: Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\nAlthough, he offers no information for the basis of how he came to describe the dialect. As in, this is more like his collected observations, but he doesn’t even really say that either.↩︎\nWhat’s off about the SNL sketch is they pronounce the first syllable of daughter with the Nurse vowel, [ɚ], when it should be something ranging between [ɔɹ] to [ʊɹ].↩︎\n\nMore specifically, I fit the model using brms::brm like so\nbrm(\n    data = data,\n    family = categorical(\n      link = logit,\n      refcat = \"1_oh\"\n      ),\n    code_param ~ decade + (1|idstring),\n  )\n↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html",
    "title": "Building up a complex {gt} table.",
    "section": "",
    "text": "Recently, for class notes on probability/the central limit theorem, I wanted to recreate the table of 2d6 values that I made here. A really cool thing I found between that blog post and now is that gt has afmt_icon() operation that will replace text with its fontawesome icon.\nlibrary(tidyverse)\nlibrary(gt)\ntibble(\n icon = c(\n   \"face-smile\",\n   \"hippo\",\n   \"pizza-slice\"\n )\n) |&gt; \n  gt() |&gt; \n  fmt_icon()\n\n\n\n\n\n\nicon\n\n\n\n\n\nFace Smile\n\n\n\n\n\n\nHippo\n\n\n\n\n\n\nPizza Slice\nAnd font-awesome has icons for each face of a 6 sided die!\nlibrary(english)\n\ntibble(\n  icon = str_glue(\n    \"dice-{as.english(1:6)}\"\n  )\n) |&gt; \n  gt() |&gt; \n  fmt_icon()\n\n\n\n\n\n\nicon\n\n\n\n\n\nDice One\n\n\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Six\nHere’s how I got to a result that I liked. If anyone has suggestions for how to do this more cleanly, I’d love to hear about it!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#the-actual-die-rolls",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#the-actual-die-rolls",
    "title": "Building up a complex {gt} table.",
    "section": "The actual die rolls",
    "text": "The actual die rolls\nGetting the actual die rolls and their total is a simple expand_grid() .\n\nexpand_grid(\n  die_a = 1:6,\n  die_b = 1:6\n) |&gt; \n  mutate(\n    total = die_a + die_b\n  )-&gt;\n  rolls_df\n\nhead(rolls_df)\n\n# A tibble: 6 × 3\n  die_a die_b total\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1     1     2\n2     1     2     3\n3     1     3     4\n4     1     4     5\n5     1     5     6\n6     1     6     7",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#injecting-the-fontawesome-icon-names",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#injecting-the-fontawesome-icon-names",
    "title": "Building up a complex {gt} table.",
    "section": "Injecting the fontawesome icon names",
    "text": "Injecting the fontawesome icon names\nIn my original code, I used some joins here, but I just found the {english} package, which will let me mutate the die_ columns directly.\n\nrolls_df |&gt; \n  mutate(\n    across(\n      starts_with(\"die_\"),\n      ~ str_glue(\n        \"dice-{word}\",\n        word = as.english(.x)\n      )\n    )\n  ) -&gt;\n  rolls_df\n\nhead(rolls_df)\n\n# A tibble: 6 × 3\n  die_a    die_b      total\n  &lt;glue&gt;   &lt;glue&gt;     &lt;int&gt;\n1 dice-one dice-one       2\n2 dice-one dice-two       3\n3 dice-one dice-three     4\n4 dice-one dice-four      5\n5 dice-one dice-five      6\n6 dice-one dice-six       7",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#identifying-combos",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#identifying-combos",
    "title": "Building up a complex {gt} table.",
    "section": "Identifying combos",
    "text": "Identifying combos\nI’m going to start pivoting a bunch, so now is the best time to give an id to each unique combo, as well as the total number of combos per total.\n\nrolls_df |&gt; \n  mutate(\n    .by = total,\n    id = row_number(),\n    n = n()\n  ) -&gt;\n  rolls_df\n\nhead(rolls_df)\n\n# A tibble: 6 × 5\n  die_a    die_b      total    id     n\n  &lt;glue&gt;   &lt;glue&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 dice-one dice-one       2     1     1\n2 dice-one dice-two       3     1     2\n3 dice-one dice-three     4     1     3\n4 dice-one dice-four      5     1     4\n5 dice-one dice-five      6     1     5\n6 dice-one dice-six       7     1     6",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#getting-wide",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#getting-wide",
    "title": "Building up a complex {gt} table.",
    "section": "Getting wide",
    "text": "Getting wide\nI’ll eventually want one column per die, with its combination id appended to it, which means pivoting long, merging the columns together, then pivoting wide.\nPivoting long\n\nrolls_df |&gt; \n  pivot_longer(\n    starts_with(\"die\"),\n    names_to = \"die\",\n    values_to = \"icon\"\n  ) |&gt; \n  unite(\n    c(die, id),\n    col = \"die_id\"\n  )-&gt;\n  rolls_long\n\nhead(rolls_long)\n\n# A tibble: 6 × 4\n  total die_id      n icon      \n  &lt;int&gt; &lt;chr&gt;   &lt;int&gt; &lt;glue&gt;    \n1     2 die_a_1     1 dice-one  \n2     2 die_b_1     1 dice-one  \n3     3 die_a_1     2 dice-one  \n4     3 die_b_1     2 dice-two  \n5     4 die_a_1     3 dice-one  \n6     4 die_b_1     3 dice-three\n\n\nPivoting wide\n\nrolls_long |&gt; \n  pivot_wider(\n    names_from = die_id,\n    values_from = icon\n  ) -&gt;\n  rolls_wide\n\nhead(rolls_wide)\n\n# A tibble: 6 × 14\n  total     n die_a_1  die_b_1   die_a_2 die_b_2 die_a_3 die_b_3 die_a_4 die_b_4\n  &lt;int&gt; &lt;int&gt; &lt;glue&gt;   &lt;glue&gt;    &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt; \n1     2     1 dice-one dice-one  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n2     3     2 dice-one dice-two  dice-t… dice-o… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n3     4     3 dice-one dice-thr… dice-t… dice-t… dice-t… dice-o… &lt;NA&gt;    &lt;NA&gt;   \n4     5     4 dice-one dice-four dice-t… dice-t… dice-t… dice-t… dice-f… dice-o…\n5     6     5 dice-one dice-five dice-t… dice-f… dice-t… dice-t… dice-f… dice-t…\n6     7     6 dice-one dice-six  dice-t… dice-f… dice-t… dice-f… dice-f… dice-t…\n# ℹ 4 more variables: die_a_5 &lt;glue&gt;, die_b_5 &lt;glue&gt;, die_a_6 &lt;glue&gt;,\n#   die_b_6 &lt;glue&gt;\n\n\nNow, I’ve got some well named columns identifying die a and die b, as well as numeric ids for each unique combination. I’ll use these for coloring the dice icons and merging columns.\nBut, I also want to move the n column, and add a proportion column.\n\nrolls_wide |&gt; \n  relocate(\n    n,\n    .after = last_col()\n  ) |&gt; \n  mutate(\n    prop = n/sum(n)\n  )-&gt;\n  rolls_wide",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#formatting-the-icons",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#formatting-the-icons",
    "title": "Building up a complex {gt} table.",
    "section": "Formatting the icons",
    "text": "Formatting the icons\nTo make sure it’s clear I’m working with two die, I want die_a and die_b to be different colors, which I can make happen with two uses of fmt_icon().\n\nrolls_wide |&gt; \n  gt() |&gt;   \n  fmt_icon(\n    starts_with(\"die_a\"),\n    fill_color = \"#CC6677\"\n  ) |&gt;\n  fmt_icon(\n    starts_with(\"die_b\"),\n    fill_color = \"#4477AA\"\n  ) -&gt;\n  out_tbl\n\nout_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal\ndie_a_1\ndie_b_1\ndie_a_2\ndie_b_2\ndie_a_3\ndie_b_3\ndie_a_4\ndie_b_4\ndie_a_5\ndie_b_5\ndie_a_6\ndie_b_6\nn\nprop\n\n\n\n2\n\n\nDice One\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n0.02777778\n\n\n3\n\n\nDice One\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n0.05555556\n\n\n4\n\n\nDice One\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n3\n0.08333333\n\n\n5\n\n\nDice One\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\n4\n0.11111111\n\n\n6\n\n\nDice One\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\nDice One\n\n\nNA\nNA\n5\n0.13888889\n\n\n7\n\n\nDice One\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\nDice One\n\n\n6\n0.16666667\n\n\n8\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\nNA\nNA\n5\n0.13888889\n\n\n9\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\nNA\nNA\nNA\nNA\n4\n0.11111111\n\n\n10\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n3\n0.08333333\n\n\n11\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n0.05555556\n\n\n12\n\n\nDice Six\n\n\n\n\nDice Six\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n0.02777778",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#dropping-the-missing-values",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#dropping-the-missing-values",
    "title": "Building up a complex {gt} table.",
    "section": "Dropping the missing values",
    "text": "Dropping the missing values\nI want to drop out all of the missing values. I found that if I replace them with just \"\", for some reason the row with no NAs winds up being narrower than the rest, but if I replace them with a zero-width space, it turns out more compact.\n\nout_tbl |&gt; \n  sub_missing(missing_text = html(\"&ZeroWidthSpace;\")) -&gt;\n  out_tbl",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#the-ugliest-part",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#the-ugliest-part",
    "title": "Building up a complex {gt} table.",
    "section": "The ugliest part",
    "text": "The ugliest part\nNow, I need to merge the columns together with cols_merge(). This is where the code gets a little ugly, what I want to be able to say is\n\nMerge two columns if they match in the last two characters\n\nMaybe there’s a way to express this with tidyselect verbs that I’m just not good enough with. In the original code, I just used cols_merge() 6 times, which would look like:\n\nout_tbl |&gt; \n  cols_merge(ends_with(\"1\")) |&gt; \n  cols_merge(ends_with(\"2\")) |&gt; \n  cols_merge(ends_with(\"3\")) |&gt;\n  cols_merge(ends_with(\"4\")) |&gt;\n  cols_merge(ends_with(\"5\")) |&gt;\n  cols_merge(ends_with(\"6\"))\n\nIt just occurred to me that something from purrr might be the right tool, and refreshed myself on purrr::reduce().\n\nreduce(\n  as.character(1:6),\n  \\(acc, nxt){\n    acc |&gt; \n      cols_merge(\n        ends_with(nxt)\n      )\n  },\n  .init = out_tbl\n) -&gt;\n  out_tbl\n\nout_tbl \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal\ndie_a_1\ndie_a_2\ndie_a_3\ndie_a_4\ndie_a_5\ndie_a_6\nn\nprop\n\n\n\n2\n\n\n\nDice One\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.02777778\n\n\n3\n\n\n\nDice One\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.05555556\n\n\n4\n\n\n\nDice One\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08333333\n\n\n5\n\n\n\nDice One\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n4\n0.11111111\n\n\n6\n\n\n\nDice One\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice One\n\n\n\n​ ​\n5\n0.13888889\n\n\n7\n\n\n\nDice One\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice One\n\n\n\n6\n0.16666667\n\n\n8\n\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\n\n​ ​\n5\n0.13888889\n\n\n9\n\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\n\n​ ​\n​ ​\n4\n0.11111111\n\n\n10\n\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08333333\n\n\n11\n\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.05555556\n\n\n12\n\n\n\nDice Six\n\n\n\n\nDice Six\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.02777778\n\n\n\n\n\n\nTo be honest, even though the reduce() approach is more programmery, writing out each cols_merge() individually is more readable…\nMaybe if I wanted to expand this out to 3d6, the reduce() approach would be better. But at that point, I’d also be creating a table of 27 columns, and at that point the illustrative nature of the table would probably be lost.\n\n\n\n\n\n\n27 Columns\n\n\n\n\n\n\n# dice roll package\nlibrary(droll)\n\nd6 &lt;- d(6)\n\none_combo_p &lt;- droll(3, 3*d6)\ntotal_combo &lt;- 1/one_combo_p\n\nmax_combo_p &lt;- droll(10, 3*d6)\n\ntotal_combo * max_combo_p\n\n[1] 27",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#final-table-finessing",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#final-table-finessing",
    "title": "Building up a complex {gt} table.",
    "section": "Final table finessing",
    "text": "Final table finessing\nNow, I want to\n\nremove the column names from the dice columns\nadd a grand summary row\nformat the probabilities down to 2 digits\nadd some css so that the table will match lightmode/darkmode settings\n\n\nout_tbl |&gt; \n  # no dice column label\n  cols_label(\n    starts_with(\"die\") ~ \"\"\n  ) |&gt; \n  # grand summary\n  grand_summary_rows(\n    columns = c(n, prop),\n    fns = list(total ~ sum(.)),\n    missing_text = \"\"\n  ) |&gt; \n  # two digits max \n  fmt_number(\n    columns = prop,\n    decimals = 2\n  ) |&gt; \n  # font setting\n  opt_table_font(\n    font = list(\n      google_font(name = \"Public Sans\"),\n      default_fonts()\n    )\n  ) |&gt;     \n  # light/darkmode matching\n  tab_style(\n    style = \"\n      background-color: var(--bs-body-bg);\n      color: var(--bs-body-color)\n    \",\n    locations = list(\n      cells_column_labels(),\n      cells_column_spanners(),\n      cells_row_groups(),\n      cells_body(),\n      cells_grand_summary(),\n      cells_stub_grand_summary(),\n      cells_stub(),\n      cells_stubhead()\n    )\n  )  -&gt;\n  out_tbl\n\nout_tbl  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal\n\n\n\n\n\n\nn\nprop\n\n\n\n\n2\n\n\n\nDice One\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.03\n\n\n\n3\n\n\n\nDice One\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.06\n\n\n\n4\n\n\n\nDice One\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08\n\n\n\n5\n\n\n\nDice One\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n4\n0.11\n\n\n\n6\n\n\n\nDice One\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice One\n\n\n\n​ ​\n5\n0.14\n\n\n\n7\n\n\n\nDice One\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice One\n\n\n\n6\n0.17\n\n\n\n8\n\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\n\n​ ​\n5\n0.14\n\n\n\n9\n\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\n\n​ ​\n​ ​\n4\n0.11\n\n\n\n10\n\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08\n\n\n\n11\n\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.06\n\n\n\n12\n\n\n\nDice Six\n\n\n\n\nDice Six\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.03\n\n\ntotal\n\n\n\n\n\n\n\n36\n1",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-20_poisson-ranef/index.html",
    "href": "posts/2024/11/2024-11-20_poisson-ranef/index.html",
    "title": "Random Effects and Overdispersion",
    "section": "",
    "text": "Today in my stats class, my students saw me realize, in real-time, that you can include random intercepts in poisson models that you couldn’t in ordinary gaussian models, and this might be a nicer way to deal with overdisperssion than moving to a negative binomial model.\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(distributional)\nlibrary(ggdist)\nlibrary(palmerpenguins)\nlibrary(gt)\nlibrary(marginaleffects)",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "11",
      "Random Effects and Overdispersion"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-20_poisson-ranef/index.html#negative-binomial",
    "href": "posts/2024/11/2024-11-20_poisson-ranef/index.html#negative-binomial",
    "title": "Random Effects and Overdispersion",
    "section": "Negative Binomial?",
    "text": "Negative Binomial?\nThe stats notes I linked to above turned to a negative binomial model to use in a case of over-dispersion like this. I’m not quite in a place to evaluate the pros and cons of the negative binomial vs this random effects approach in general. But for this case I like the random effects better because\n\nIt lines up with how I think about this data as having a population level trend, with individual divergences off of it.\nIt’s easier for me to explain and understand than whatever the shape parameter is for the negative binomial.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "11",
      "Random Effects and Overdispersion"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-05/index.html",
    "href": "posts/2023/02/2023-02-05/index.html",
    "title": "A handy dplyr function for linguistics",
    "section": "",
    "text": "One of the new functions in dplyr v1.1.0 is dplyr::consecutive_id(), which strikes me as having a few good use cases for linguistic data. The one I’ll illustrate here is for processing transcriptions.\nlibrary(tidyverse)\nlibrary(gt)\n\nsource(here::here(\"_defaults.R\"))\n\n# make sure its &gt;= v1.1.0\npackageVersion(\"dplyr\")\n\n[1] '1.1.2'\nI’ll use a sample transcription extract from LANCS, where the audio has been chunked into “breath groups” and transcribed, along with an identifier of who was speaking, and beginning and end times.\ntranscription &lt;- \n  read_csv(\"data/KY25A_1.csv\")\nspeaker\nstart\nend\ntranscript\n\n\n\nIVR\n192.110\n194.710\nwell uh, I have a number of uh\n\n\nIVR\n195.530\n198.620\nthings I'd like to ask you about. I wonder if you'd just mind uh.\n\n\nIVR\n199.110\n200.900\nanswering questions uh\n\n\nIVR\n202.130\n203.610\none after another if you\n\n\nKY25A\n203.295\n204.405\nyeah\n\n\nKY25A\n204.745\n205.225\nwell\n\n\nIVR\n204.740\n205.805\nif I remind you of a\n\n\nKY25A\n205.510\n207.930\nnow you might start that\n\n\nKY25A\n208.440\n209.570\nI was born in\n\n\nKY25A\n210.420\n212.120\neighteen sixty seven\n\n\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n\n\nKY25A\n215.780\n216.600\nninety three\n\n\nIVR\n216.665\n217.455\nninety three\nOne thing we might want to do is indicate which sequences of transcription chunks belong to one speaker, corresponding roughly to their speaking turns. I’ve hacked my way through this kind of coding before, but now we can easily add turn numbers with dplyr::consecutive_id(), which will add a column of numbers that increment every time the value in the indicated column changes.\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  )\nspeaker\nstart\nend\ntranscript\nturn\n\n\n\nIVR\n192.110\n194.710\nwell uh, I have a number of uh\n1\n\n\nIVR\n195.530\n198.620\nthings I'd like to ask you about. I wonder if you'd just mind uh.\n1\n\n\nIVR\n199.110\n200.900\nanswering questions uh\n1\n\n\nIVR\n202.130\n203.610\none after another if you\n1\n\n\nKY25A\n203.295\n204.405\nyeah\n2\n\n\nKY25A\n204.745\n205.225\nwell\n2\n\n\nIVR\n204.740\n205.805\nif I remind you of a\n3\n\n\nKY25A\n205.510\n207.930\nnow you might start that\n4\n\n\nKY25A\n208.440\n209.570\nI was born in\n4\n\n\nKY25A\n210.420\n212.120\neighteen sixty seven\n4\n\n\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n5\n\n\nKY25A\n215.780\n216.600\nninety three\n6\n\n\nIVR\n216.665\n217.455\nninety three\n7\nNow we can do things like group the data by turn, and get a new dataframe summarized by turn.\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  ) |&gt; \n  summarise(\n    .by = c(turn, speaker),\n    start = min(start),\n    end = max(end),\n    transcript = str_c(transcript, collapse = \" \"),\n  )\nturn\nspeaker\nstart\nend\ntranscript\n\n\n\n1\nIVR\n192.110\n203.610\nwell uh, I have a number of uh things I'd like to ask you about. I wonder if you'd just mind uh. answering questions uh one after another if you\n\n\n2\nKY25A\n203.295\n205.225\nyeah well\n\n\n3\nIVR\n204.740\n205.805\nif I remind you of a\n\n\n4\nKY25A\n205.510\n212.120\nnow you might start that I was born in eighteen sixty seven\n\n\n5\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n\n\n6\nKY25A\n215.780\n216.600\nninety three\n\n\n7\nIVR\n216.665\n217.455\nninety three\nAnd then you can start moving onto other analyses, like what the lag was between one speaker’s end and the next’s beginning.\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  ) |&gt; \n  summarise(\n    .by = c(turn, speaker),\n    start = min(start),\n    end = max(end),\n    transcript = str_c(transcript, collapse = \" \"),\n  ) |&gt; \n  mutate(overlapping = start &lt; lag(end))\nturn\nspeaker\nstart\nend\ntranscript\nlag\n\n\n\n1\nIVR\n192.110\n203.610\nwell uh, I have a number of uh things I'd like to ask you about. I wonder if you'd just mind uh. answering questions uh one after another if you\nNA\n\n\n2\nKY25A\n203.295\n205.225\nyeah well\n-0.315\n\n\n3\nIVR\n204.740\n205.805\nif I remind you of a\n-0.485\n\n\n4\nKY25A\n205.510\n212.120\nnow you might start that I was born in eighteen sixty seven\n-0.295\n\n\n5\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n1.230\n\n\n6\nKY25A\n215.780\n216.600\nninety three\n0.330\n\n\n7\nIVR\n216.665\n217.455\nninety three\n0.065\nThis was just the first example that came to mind, but there’s probably a lot of data processing tasks that can be made a lot less annoying with dplyr::consecutive_id().",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "A handy dplyr function for linguistics"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-05/index.html#extra",
    "href": "posts/2023/02/2023-02-05/index.html#extra",
    "title": "A handy dplyr function for linguistics",
    "section": "Extra",
    "text": "Extra\nI’ll throw the duration of within-turn pauses in there.\n\nlibrary(glue)\n\n\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  ) |&gt; \n  mutate(\n    .by = turn,\n    pause_dur = start - lag(end),\n    transcript = case_when(\n      .default = transcript,\n      is.finite(pause_dur) ~ glue(\n        \"&lt;{round(pause_dur, digits = 2)} second pause&gt; {transcript}\"\n      )\n    )\n  ) |&gt; \n  summarise(\n    .by = c(turn, speaker),\n    start = min(start),\n    end = max(end),\n    transcript = str_c(transcript, collapse = \" \"),\n  ) |&gt; \n  mutate(lag = start - lag(end)) |&gt; \n  relocate(lag,  .before = start)\n\n\n\n\n\n\n\nturn\nspeaker\nlag\nstart\nend\ntranscript\n\n\n\n1\nIVR\nNA\n192.110\n203.610\nwell uh, I have a number of uh &lt;0.82 second pause&gt; things I'd like to ask you about. I wonder if you'd just mind uh. &lt;0.49 second pause&gt; answering questions uh &lt;1.23 second pause&gt; one after another if you\n\n\n2\nKY25A\n-0.315\n203.295\n205.225\nyeah &lt;0.34 second pause&gt; well\n\n\n3\nIVR\n-0.485\n204.740\n205.805\nif I remind you of a\n\n\n4\nKY25A\n-0.295\n205.510\n212.120\nnow you might start that &lt;0.51 second pause&gt; I was born in &lt;0.85 second pause&gt; eighteen sixty seven\n\n\n5\nIVR\n1.230\n213.350\n215.450\nmhm and that makes you how old?\n\n\n6\nKY25A\n0.330\n215.780\n216.600\nninety three\n\n\n7\nIVR\n0.065\n216.665\n217.455\nninety three",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "02",
      "A handy dplyr function for linguistics"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "",
    "text": "I’ve been playing a lot of Baldur’s Gate 3 lately. My party has gotten to max level, and we’re running around, casting spells and taking names.\nOne of the spells my wizard has is called Disintegrate, which gets summarized like this\nThe line of information under “Damage” means that to calculate how much damage you do when you cast the spell, you need to roll 10 6-sided dice (“d6”), then add 40. The “(50~100)” part summarizes the worst vs best outcomes.\nIf you just glance at the description saying that the spell will do between 50 and 100 damage, you might think that you’re equally likely to roll any amount of damage between these two values.\nBut, as you play, you’ll find that hitting something right in the middle, like 75, is very likely, and that hitting either 50 or 100 is vanishingly rare. The distribution of rolls wind up looking like this:\nIn fact, 95% of the rolls are going to be between 64 and 86. The rest of this post is about these two questions:",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#d6",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#d6",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "1d6",
    "text": "1d6\nWe should start things off easy, and look at what happens when we roll 1d6. As long as the die is fair, any value between 1 and 6 is possible. And for each value between 1 and 6, there’s only 1 possible way to get that value.\n\n\n\n\n\n\ntotal\nrolls\nn\n\n\n\n1\n1\n1\n\n\n2\n2\n1\n\n\n3\n3\n1\n\n\n4\n4\n1\n\n\n5\n5\n1\n\n\n6\n6\n1",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-1",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-1",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "2d6",
    "text": "2d6\nThings are more complicated when we start rolling 2d6 and summing them together. Let’s name each of our die “Alpha” and “Beta” to keep them straight.\nThere’s only one way for us to get a total roll of 2: both Alpha and Beta need to come up 1. Same thing for a total roll of 12: both Alpha and Beta need to come up 6.\nBut to roll a value of 3, there are two ways.\n\nAlpha rolls 1, and Beta rolls 2.\nAlpha rolls 2, and Beta rolls 1.\n\nIf we expand this out to look at all possible values between 2 and 12, we’ll find that there’s six unique ways to roll 7, with each value above and below having fewer unique ways.\n\n\n\n\n\n\ntotal\nrolls\nn\n\n\n\n2\n1 1\n1\n\n\n3\n1 2; 2 1\n2\n\n\n4\n1 3; 2 2; 3 1\n3\n\n\n5\n1 4; 2 3; 3 2; 4 1\n4\n\n\n6\n1 5; 2 4; 3 3; 4 2; 5 1\n5\n\n\n7\n1 6; 2 5; 3 4; 4 3; 5 2; 6 1\n6\n\n\n8\n2 6; 3 5; 4 4; 5 3; 6 2\n5\n\n\n9\n3 6; 4 5; 5 4; 6 3\n4\n\n\n10\n4 6; 5 5; 6 4\n3\n\n\n11\n5 6; 6 5\n2\n\n\n12\n6 6\n1\n\n\n\n\n\n\nThere’s just more different ways for a very large roll on Alpha to get balanced out by a smaller roll on Beta.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-2",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-2",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "10d6",
    "text": "10d6\nIf we expand this out to the 10d6 situation, where there’s only one way to roll 10 (all six die roll 1), there are 4,395,456 different ways to roll 35. The massively larger number of ways to get 35 results in it showing up that much more often than 10 (or 60).",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#unfair-dice",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#unfair-dice",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "Unfair dice",
    "text": "Unfair dice\nThe nifty thing about the Central Limit Theorem is that it works no matter what the shape of the original distribution was. Let’s say we had a 10 sided die, but 3 of the sides have “1” painted, 3 of the sides have “6” painted on them, and the rest have 2 through 5. With this die, we’re a lot more likely to roll a 1 or a 6.\n\n\n\n\n\n\n\n\nIf we used this unfair die, to roll damage for the Disintegrate spell, how will that affect the outcome?\n\n\n\n\n\n\n\n\nIt might be hard to see the difference, but this unfair die is slightly more spread out than the fair d6\n\n\n\n\n\n\n\n\nBut, even though it’s more spread out, it’s still approximating a normal distribution!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html",
    "title": "Making a spectrogram in R",
    "section": "",
    "text": "I might flesh this out as a more detailed tutorial for LingMethodsHub, but for now this is going to be a rough-around-the-edges post about making spectrograms in R. My goal will be to get as close as possible to recreating a spectrogram like you might get from Praat.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#pre-processing.",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#pre-processing.",
    "title": "Making a spectrogram in R",
    "section": "Pre-processing.",
    "text": "Pre-processing.\nTo keep things simple, I grabbed vowel audio clip from the Wikipedia IPA vowel chat with audio (audio info and license).\nYour browser does not support the audio element. \nExplanation of a hacky thing I had to do.\nI know {tuneR} package has a readWave() function, and I couldn’t figure out how to read in an Oog file, so step 1 was converting the Oog to a wav file. Since I’m writing this in a quarto notebook, I thought I should be able to drop in a ```{sh} code chunk, but it seems like doesn’t have access to my PATH. Long story short, that’s why I’ve got this kind of goofy R code chunk with system() and the full path to sox.\n\nlibrary(glue)\n\nsource(here::here(\"_defaults.R\"))\n\n\nogg_file &lt;- \"assets/Close_front_unrounded_vowel.ogg\"\nwav_file &lt;- \"assets/Close_front_unrounded_vowel.wav\"\nsystem(glue(\"/opt/homebrew/bin/sox {ogg_file} {wav_file}\"))",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#loading-the-audio-file",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#loading-the-audio-file",
    "title": "Making a spectrogram in R",
    "section": "Loading the audio file",
    "text": "Loading the audio file\nThe seewave package, which I’m using to make the spectrogram, takes the sound objects created by {tuneR}, so that’s what I’ll use for reading in the audio file.\n\nlibrary(tuneR)\nlibrary(seewave)\n\n\ni_wav &lt;- readWave(\"assets/Close_front_unrounded_vowel.wav\")\n\nTo get a sense of what information is in the wav file, you can use str()\n\nstr(i_wav)\n\nFormal class 'Wave' [package \"tuneR\"] with 6 slots\n  ..@ left     : int [1:26524] -2 16 42 24 33 53 56 68 51 55 ...\n  ..@ right    : num(0) \n  ..@ stereo   : logi FALSE\n  ..@ samp.rate: int 44100\n  ..@ bit      : int 16\n  ..@ pcm      : logi TRUE\n\n\nSince I’m going to be zooming in to 0 to 5,000 Hz on the spectrogram, I’ll downsample the audio to 10000.\n\ni_wav_d &lt;- downsample(i_wav, 10000)",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#computing-the-spectrogram",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#computing-the-spectrogram",
    "title": "Making a spectrogram in R",
    "section": "Computing the spectrogram",
    "text": "Computing the spectrogram\nThe function to compute the spectrogram is seewave::spectro(). Its argument names are formatted in a way I find a bit grating. A lot of them are compressed down to single characters or other abbreviations that require having the docs constantly open.\nAnyway, the arguments that seem most important are:\n\nwl\n\nwindow length in samples\n\nwn\n\nwindow function, defaulting to Hanning\n\novlp\n\nWindow overlap, in percentage. That is, a 25% overlap between analysis windows should be passed to ovlp as 25.\n\n\nPraat defaults\nLet’s have a look at the Praat spectrogram defaults\n\n\n\n\n\nFigure 2: Praat defaults for spectrograms\n\n\nHere’s a quick illustration of what these defaults correspond to. It takes an analysis window that’s 0.005 seconds long, and moves it over time by 0.002 second increments. Also, the data coming into the analysis window is weighted by a Gaussian distribution.\n\nlibrary(tidyverse)\n\n\nPlot Codewin_len &lt;- 0.005\ntime_step &lt;- 0.002\ntibble(\n  center = seq(win_len/2, 0.02, by = time_step),\n  left_edge = center - (win_len/2),\n  right_edge = center + (win_len/2),\n  win_num = seq_along(center)\n) -&gt; window_fig\nwindow_fig |&gt; \n  ggplot(aes(x = center, y = win_num))+\n    geom_pointrange(\n      aes(\n        xmin = left_edge,\n        xmax = right_edge\n      ),\n      size = 2,\n      linewidth = 2\n    )+\n  labs(x = \"time\",\n       y = NULL,\n       title = \"Overlapping spectrogram window illustration\")+\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n\n\nFigure 3: An illustration of overlapping analysis windows that produce a spectrogram\n\n\n\n\nThe seewave::spectro() function defines this same relationship, except instead of the time step or window hop length, we need to define by what % the windows overlap. We also need to express how wide the windows are in terms of audio sample, rather than in terms of time, but that just requires multiplying the desired time width by the sampling rate.\n\nwin_len &lt;- 0.005 * i_wav_d@samp.rate\nhop_len &lt;- 0.002 * i_wav_d@samp.rate\noverlap &lt;- ((win_len - hop_len) / win_len) * 100\n\nThe one thing that I can’t recreate for now is the Gaussian window function. seewave doesn’t have it implemented, so I’ll just stick with its default (Hamming)\nComputing the spectrogram\nNow, it’s a pretty straightforward call to spectro().\n\nspect &lt;-\n  i_wav_d |&gt;\n  spectro(\n    # window length, in terms of samples\n    wl = win_len,\n    # window overlap\n    ovlp = overlap,\n    # don't plot the result\n    plot = F\n    )\n\nThe spect object is a list with three named items\n\n$time\n\na vector corresponding to the time domain\n\n$freq\n\na vector corresponding to the frequency domain\n\n$amp\n\na matrix of amplitudes across the time and frequency domains\n\n\n\nglimpse(spect)\n\nList of 3\n $ time: num [1:299] 0 0.00202 0.00404 0.00606 0.00807 ...\n $ freq: num [1:25] 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 ...\n $ amp : num [1:25, 1:299] -35.1 -39.5 -51.3 -49.9 -51.7 ...\n\n\nTidying up\nIn order to make a plot of the spectrogram in ggplot2, we need to do some tidying up. I’ll go about this by setting the row and column names of the spect$amp matrix to the frequency and time domain values, converting it to a data frame, then doing some pivoting.\n\n# set the colnames and rownames\ncolnames(spect$amp) &lt;- spect$time\nrownames(spect$amp) &lt;- spect$freq\n\n\nspect_df &lt;-\n  spect$amp |&gt;\n  # coerce the row names to a column\n  as_tibble(rownames = \"freq\") |&gt;\n  # pivot to long format\n  pivot_longer(\n    # all columns except freq\n    -freq, \n    names_to = \"time\", \n    values_to = \"amp\"\n  ) |&gt;\n  # since they were names before,\n  # freq and time need conversion to numeric\n  mutate(\n    freq = as.numeric(freq),\n    time = as.numeric(time)\n  )\n\n\nsummary(spect_df)\n\n      freq          time             amp        \n Min.   :0.0   Min.   :0.0000   Min.   :-89.94  \n 1st Qu.:1.2   1st Qu.:0.1494   1st Qu.:-45.17  \n Median :2.4   Median :0.3008   Median :-35.85  \n Mean   :2.4   Mean   :0.3008   Mean   :-35.43  \n 3rd Qu.:3.6   3rd Qu.:0.4521   3rd Qu.:-26.69  \n Max.   :4.8   Max.   :0.6015   Max.   :  0.00  \n\n\n“Dynamic Range”\nFrequency data is represented in terms of kHz, which I’ll leave alone for now. One last thing we need to re-recreate from Praat is the “dynamic range”. All values below some cut off (by default, 50 below the maximum) are plotted with the same color. We can do that with some data frame operations here.\n\ndyn = -50\nspect_df_floor &lt;- \n  spect_df |&gt; \n  mutate(\n    amp_floor = case_when(\n      amp &lt; dyn ~ dyn,\n      TRUE ~ amp  \n    )\n  )",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#plotting-the-spectrogram",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#plotting-the-spectrogram",
    "title": "Making a spectrogram in R",
    "section": "Plotting the spectrogram",
    "text": "Plotting the spectrogram\nNow what’s left is to plot the thing. I’ll load the khroma package in order to get some nice color scales.\n\nlibrary(khroma)\n\nBasic raster plot\nAs a first step, we can plot the time by frequency data as a raster plot, with little rectangles for each position filled in with their amplitude.\n\nspect_df_floor |&gt; \n  ggplot(aes(time, freq))+\n    geom_raster(aes(fill = amp_floor))+\n    guides(fill = \"none\")+\n    labs(\n      x = \"time (s)\",\n      y = \"frequency (kHz)\",\n      title = \"spectrogram raster plot\"\n    )\n\n\n\n\n\n\nFigure 4: A raster spectrogram plot\n\n\n\n\nSpectrogram contour plot\nTo get closer to the Praat output, though, we need to make a contour plot instead. Here’s where I’m getting a bit stymied. I wind up with these weird diagonal “shadows” on the right and left hand side of the spectrogram, which I think are a result of how the stat_contour() is being computed and plotted, rather than anything to do with the actual spectrogram.\n\nspect_df_floor |&gt; \n  ggplot(aes(time, freq))+\n    stat_contour(\n      aes(\n        z = amp_floor,\n        fill = after_stat(level)\n      ),\n      geom = \"polygon\",\n      bins = 300\n    )+\n    scale_fill_batlow()+\n    guides(fill = \"none\")+\n    labs(\n      x = \"time (s)\",\n      y = \"frequency (kHz)\",\n      title = \"spectrogram contour plot\"\n    )\n\n\n\n\n\n\nFigure 5: A contour spectrogram plot\n\n\n\n\nOne way around this I’ve found is to compute the spectrogram on the higher sampling rate audio, and then zoom into the frequency range you want.\n\nre-running the spectrogramwin_len &lt;- 0.005 * i_wav@samp.rate\nhop_len &lt;- 0.002 * i_wav@samp.rate\noverlap &lt;- ((win_len - hop_len) / win_len) * 100\n\nspect2 &lt;-\n  i_wav |&gt;\n  spectro(\n    # window length, in terms of samples\n    wl = win_len,\n    # window overlap\n    ovlp = overlap,\n    # don't plot the result\n    plot = F\n    )\n\n# set the colnames and rownames\ncolnames(spect2$amp) &lt;- spect2$time\nrownames(spect2$amp) &lt;- spect2$freq\n\nspect2_df &lt;-\n  spect2$amp |&gt;\n  # coerce the row names to a column\n  as_tibble(rownames = \"freq\") |&gt;\n  # pivot to long format\n  pivot_longer(\n    # all columns except freq\n    -freq, \n    names_to = \"time\", \n    values_to = \"amp\"\n  ) |&gt;\n  # since they were names before,\n  # freq and time need conversion to numeric\n  mutate(\n    freq = as.numeric(freq),\n    time = as.numeric(time)\n  )\n\ndyn = -50\nspect2_df_floor &lt;- \n  spect2_df |&gt; \n  mutate(\n    amp_floor = case_when(\n      amp &lt; dyn ~ dyn,\n      TRUE ~ amp  \n    )\n  )\n\n\n\nspect2_df_floor |&gt; \n  ggplot(aes(time, freq))+\n    stat_contour(\n      aes(\n        z = amp_floor,\n        fill = after_stat(level)\n      ),\n      geom = \"polygon\",\n      bins = 300\n    )+\n    scale_fill_batlow()+\n    guides(fill = \"none\")+\n    labs(\n      x = \"time (s)\",\n      y = \"frequency (kHz)\",\n      title = \"spectrogram contour plot\"\n    )+\n    coord_cartesian(ylim = c(0, 5))\n\n\n\n\n\n\nFigure 6: A contour spectrogram plot\n\n\n\n\nI think this might not be necessary if I had a better handle on stat_contour() but for now, it does the trick!",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#update",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#update",
    "title": "Making a spectrogram in R",
    "section": "Update",
    "text": "Update\nDue to popular request, another attempt at showing the degree of window overlap.\n\n\n\nThis is fantastic, jo, thx! One quick q, I’m wondering if there might be a better way to visualize overlapping windows. The image maybe implies some y dimension (windows increasing in some vertical space). I’m just putting myself in a 1st yrs shoes in my speech class\n\n— Chandan (@GutStrings) January 22, 2023\n\nI’ve folded the code here, just because it’s medium gnarly. The short version is you can get the weights for the window functions from seewave::ftwindow(), and then I used geom_area() to plot it.\n\nthe data and plotting codewindow_fig |&gt; \n  group_by(win_num) |&gt; \n  nest() |&gt; \n  mutate(\n    dists = map(\n      data, \n      \\(df){\n        tibble(\n          time = seq(df$left_edge, df$right_edge, len = 512),\n          weight = ftwindow(512)\n        )\n      }\n    )\n  ) |&gt; \n  unnest(dists) -&gt; window_weights\n\nwindow_weights |&gt; \n  ggplot(aes(time, weight, group = win_num)) +\n    geom_area(\n      aes(\n        group = win_num, \n        fill = win_num,\n        color = win_num\n      ), \n      position = \"identity\", \n      alpha = 0.5)+\n    scale_fill_hawaii(guide = \"none\")+\n    scale_color_hawaii(guide = \"none\")+\n  labs(title = \"Hamming window functions\",\n       subtitle = \"window length = 0.005, time step =0.002\")\n\n\n\n\n\n\nFigure 7: Hamming window functions",
    "crumbs": [
      "Homepage",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html",
    "title": "Setting default ggplot2 colors",
    "section": "",
    "text": "This might be a “everyone else already knew about this” thing, but I’ve finally gotten to a place of understanding about setting default colors scales for ggplot2, so I thought I’d share.",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#discrete-colors",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#discrete-colors",
    "title": "Setting default ggplot2 colors",
    "section": "Discrete colors",
    "text": "Discrete colors\nWithout doing anything\nHere’s how things look by default:\n\nggplot(\n  sample_data\n)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = level\n    )\n  ) -&gt;\n  discrete3_p\n\nggplot(\n  sample_data\n)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = a &lt; 0\n    )\n  ) -&gt;\n  discrete2_p\n\ndiscrete2_p\ndiscrete3_p\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting the default with a list\nIf we set the default colors by setting options(ggplot2.discrete.colour = ) a list of color values, ggplot will use those colors if there’s enough, and if there’s not enough, it’ll fall back to the default scale_color_hue().\nwithr::with_options(\n  list(\n    ggplot2.discrete.colour = list(\n      c(\"#AA4499\", \"#117733\")\n    )\n  ),\n  \n  {\n    print(discrete2_p)\n    print(discrete3_p)\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou could even set a completely different vector of values for 3 colors.\nwithr::with_options(\n  list(\n    ggplot2.discrete.colour = list(\n      c(\"#AA4499\", \"#117733\"),\n      c(\"#4477AA\", \"#88CCEE\", \"#DDCC77\")\n    )\n  ),\n  \n  {\n    print(discrete2_p)\n    print(discrete3_p)\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I set up in my actual _defaults.R files is to generate all of the possible palettes from ggthemes::ptol_pal(), because I like it.\n\nmy_discrete_list &lt;- lapply(\n  1:12, \n  ggthemes::ptol_pal()\n)\n\nscales::show_col(\n  my_discrete_list[[10]],\n  ncol = 5,\n  cex_label = 0.8\n)\n\n\n\n\n\n\n\nI could set ggplot2.discrete.colour to ggthemes::scale_color_ptol(). But by setting it to the progessively larger list of colors, if if make the decision1 to map a factor with 13 labels to color, instead of erroring or just not plotting some points, it will fall back to the built in scale_color_hue().\n\noptions(\n  ggplot2.discrete.colour = lapply(\n    1:12,\n    ggthemes::ptol_pal()\n  ),\n  ggplot2.discrete.fill = lapply(\n    1:12,\n    ggthemes::ptol_pal()\n  )\n)\n\nHere’s an example showing using that 13+ levels example\n\nSetting up base plotsggplot(sample_data)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = cut(a, 10)\n    )\n  )+\n  guides(\n    color = \"none\"\n  )-&gt;\n  base_10_p\n\nggplot(sample_data)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = cut(a, 15)\n    )\n  )+\n  guides(\n    color = \"none\"\n  )-&gt;\n  base_15_p\n\n\nPlotting codebase_10_p +\n  labs(\n    title = \"10 cuts, list\"\n  )\nbase_15_p +\n  labs(\n    title = \"15 cuts, list\"\n  )\nbase_10_p +\n  ggthemes::scale_color_ptol()+\n  labs(\n    title = \"10 cuts, scale\"\n  )\nbase_15_p +\n  ggthemes::scale_color_ptol()+\n  labs(\n    title = \"15 cuts, scale\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError in colors[[n]]: subscript out of bounds",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#continuous-colors",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#continuous-colors",
    "title": "Setting default ggplot2 colors",
    "section": "Continuous colors",
    "text": "Continuous colors\nThe continuous color scales need to be set more straightforwardly with a function that returns a scale_color_*(). I’m kind of bouncing around continuous color scales I like, but for now, I’m defaulting to the batlow palette from scico.\nSince getting the specific batlow palette requires passing arguments to scico::scale_color_scico(), I need to pass ggplot2.continuous.colour an anonymous function.\n\nggplot(sample_data)+\n  geom_point(\n    aes(\n      a, b,\n      color = a\n    )\n  )-&gt;\n  continuous_base\n\n\noptions(\n  ggplot2.continuous.colour = \\(...){\n    scico::scale_color_scico(\n      palette = \"batlow\", \n      ...\n    )\n  },\n  ggplot2.continuous.fill = \\(...){\n    scico::scale_fill_scico(\n      palette = \"batlow\", \n      ...\n    )\n  }\n)\n\n\ncontinuous_base",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#ordinal-colors",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#ordinal-colors",
    "title": "Setting default ggplot2 colors",
    "section": "Ordinal colors",
    "text": "Ordinal colors\nFor the longest time, I only had settings for continuous and discrete color scales, and I kept getting frustrated when an occasional plot would show up with neither of my options showing up.\n\n(\n  ggplot(sample_data) +\n    geom_hdr_points(\n      aes(a, b),\n      probs = rev(ppoints(10))\n    )+\n    guides(color = \"none\") -&gt;\n    density_points\n)\n\n\n\n\n\n\n\nI could not, for the life of me, figure out what option I had to set to change the default here! I eventually just searched the ggplot2 github for getOption and found ggplot2.ordinal.colour! This is definitely not documented anywhere in the actual ggplot2 docs!\nAnyway, l kind of like the mako viridis palette for this, so that’s what I’m using:\n\noptions(\n  ggplot2.ordinal.colour = \\(...){\n    scale_color_viridis_d(\n      option = \"G\", \n      direction = -1, \n      ...\n    )\n  },\n  ggplot2.ordinal.fill = \\(...){\n    scale_fill_viridis_d(\n      option = \"G\", \n      direction = -1, \n      ...\n    )\n  }\n)\n\n\ndensity_points",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#footnotes",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#footnotes",
    "title": "Setting default ggplot2 colors",
    "section": "Footnotes",
    "text": "Footnotes\n\nbad↩︎",
    "crumbs": [
      "Homepage",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  }
]