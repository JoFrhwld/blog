[
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "",
    "text": "I just saw a really interesting paper by Scott Nelson and Jeffrey Heinz (Nelson and Heinz 2025) that proposes a model of phonology and phonetics as complex function application that maintains a discrete phonology while also alowing for things like incomplete neutralization. I myself am always sort of able to follow formal notation, but get a better understanding if I try rewriting it in a programming language of some sort. So I’m giving it a go here in Python.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#my-approach-to-the-functions",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#my-approach-to-the-functions",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "My approach to the functions",
    "text": "My approach to the functions\nIn their paper they say\n\nFor every \\(n\\)­ary function, there is an equivalent (\\(n+1\\))-ary relation. Since phonology is a unary function (i.e., it has one input, a UR), it can also be envisioned as a binary relation consisting of UR and SR pairs \\(\\langle\\) UR, SR \\(\\rangle\\).\n\nTo capture that fact, and also to make my python functions operate in a way that I feel is principled, I’m going to have (almost) every function return a tuple of its input and output like so:\n\ndef my_fun(x:str) -&gt; tuple[str, str]:\n  return (x, x[0])\n\nmy_fun(\"hello!\")\n\n('hello!', 'h')",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#typing",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#typing",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "Typing",
    "text": "Typing\nI’m also including typing on most of the functions as well. If you’re not familiar, x:str means that this function has one parameter x, and that parameter is a string. The -&gt; tuple[str, str] part means that the function returns a tuple with two values, both of them strings. By including typing, I was able to rely on automatic type checking in my IDE to tell me if the values I was actually returning were what I thought I was returning. They eventually get a little illegible… but they’re still mechanically useful.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#getting-started-with-the-lexicon",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#getting-started-with-the-lexicon",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "Getting started with the lexicon",
    "text": "Getting started with the lexicon\nNelson & Heinz weren’t very explicit about what the structure of the lexicon was, and I think that’s cause it doesn’t really matter. So I just need to make a decision and say that each item in the lexicon will be a list with two values. The first example they work through was word-final devoicing, so I’ll say the first will either be \"+\" or \"-\" for [+voice] or [-voice]. The second value will be either \"#\" for “word final” or \".\" for “everything else”.\n\nimport numpy as np\nimport numpy.typing as npt\nfrom typing import Callable\n\n\nL = [\n  [\"+\", \"#\"],\n  [\"-\", \"#\"],\n  [\"+\", \".\"],\n  [\"-\", \".\"]\n]",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#the-phonology",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#the-phonology",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "The Phonology",
    "text": "The Phonology\nI’m going to write the phonology (P()) to map every underlying representation (UR) in the lexicon to its surface representation (SR). So I really want the function to look like P(L).\nFor the actual mapping of a specific UR to a specific SR, I’ll define a function internal to P() called neut(). I guess this neut() function is what’s “encapsulated” within P(), and it’s the closest to what feels familiar as a “phonological rule”.\n\ndef P(L:list[list[str]])-&gt;list[tuple[list[str], list[str]]]:\n  \"\"\"\n  Map all URs to SRs\n  \"\"\"\n  def neut(UR: list[str]) -&gt; tuple[list[str], list[str]]:\n    \"\"\"\n    Neutralize '+' to '-' when a '#' follows.\n    \"\"\"\n    if UR[0] == \"+\" and UR[1] == \"#\":\n      return (UR, [\"-\", \"#\"])\n    return (UR, UR)\n  return list(map(neut, L))\n\nHere’s how the phonology looks when applied to the lexicon.\n\nP(L)\n\n[(['+', '#'], ['-', '#']),\n (['-', '#'], ['-', '#']),\n (['+', '.'], ['+', '.']),\n (['-', '.'], ['-', '.'])]",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#phonetics",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#phonetics",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "Phonetics",
    "text": "Phonetics\nThe Phonetics (which they call ‘A’ in the paper) maps the Phonology (P) to phonetic targets. They get incomplete neutralization out of this by saying both the UR and the SR get targets mapped.\nHere’s where I needed to make some decisions I wasn’t sure about, specifically in what the output of A was. One possibility is that it should just be a list of the targets\n\\[\n\\left[\\begin{array}{c}\nx_1, y_1\\\\\n\\ldots\\\\\nx_i, y_i\n\\end{array}\\right]\n\\]\nBut the input was a \\(\\langle\\) UR, SR \\(\\rangle\\) tuple, and in keeping with treating \\(n\\) ary functions as \\((n+1)\\)-ary relations, maybe the output should also be a list of tuples.\n\\[\n\\left[\\begin{array}{ll}\n\\langle\\langle\\text{UR},\\text{SR}\\rangle, & [x_1, y_1]\\rangle\\\\\n\\ldots\\\\\n\\langle\\langle\\text{UR},\\text{SR}\\rangle, & [x_i, y_i]\\rangle\n\\end{array}\\right]\n\\]\nI decided to go this second route, since it felt more principled than changing how these functions work midway.\nI also had to make a few decisions about the internal functions of A(). Nelson & Heinz weren’t very specific about how the cue assignment worked, or whether the process was identical for URs and SRs. I feel like the model would get very unconstrained if it was different, so I wrote just one internal target() function that maps over UR, SR pairs. This target() function is most similar to what I think of as the “Phonology-Phonetics Interface”.\nThen, inside the targeting function, I have an internal voicing() function that maps feature values to specific cue values. This is the most similar to what I think of as a “phonetic implementation rule”.\n\ndef A(P:list[tuple[list[str], list[str]]]) -&gt; list[tuple[tuple[list[str],list[str]],npt.NDArray]]:\n  \"\"\"\n  Assign targets to P(L)\n  \"\"\"\n  def target(UR_SR:tuple[list[str], list[str]]) -&gt; tuple[tuple[list[str],list[str]],npt.NDArray]:\n    \"\"\"\n    Assign targets to each representation in &lt;UR, SR&gt;\n    \"\"\"\n    def voicing(rep: list[str]) -&gt; float|None:\n      \"\"\"\n      Return cue value for each feature value.\n      \"\"\"\n      if rep[0] == \"+\":\n        return 2.0\n      if rep[0] == \"-\":\n        return 1.0\n      \n    return (UR_SR, np.array(list(map(voicing, UR_SR))))\n  \n  return list(map(target, P))\n\nThe typing is pretty incomprehensible, and we’re dealing with two levels of function embedding, but in the end we get the kind of output we’re looking for.\n\nA(P(L))\n\n[((['+', '#'], ['-', '#']), array([2., 1.])),\n ((['-', '#'], ['-', '#']), array([1., 1.])),\n ((['+', '.'], ['+', '.']), array([2., 2.])),\n ((['-', '.'], ['-', '.']), array([1., 1.]))]",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#intent",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#intent",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "Intent",
    "text": "Intent\nFinally, there’s the “Intent” (I) function that takes in the cue values from A() and weights & combines them according to an intention to keep underlying features distinct.\nNow, a function that looks like I(A(P(L))) won’t do the trick, because the intention value varies. I could let I() take a second parameter like I(A(P(L)), i), but none of the other functions I’ve written so far have done that, so I don’t want start now. Instead, I wrote an I_factory() function which returns a parameterized I() function. The I() function then weights and sums the target values to return the Phonetic Realization.\nAt this point, I decided that I wasn’t going to bother with returning a tuple of \\(\\langle\\) input, output \\(\\rangle\\) because\n\nThis is the last step.\nThe typing would be monstrous.\n\n\ndef I_factory(i:float = 0.0) -&gt; Callable:\n  \"\"\"\n  Return a parameterized Intent function\n  \"\"\"\n  def I(A:list[tuple[tuple[list[str],list[str]],npt.NDArray]]) -&gt; list[float]:\n    \"\"\"\n    Map A() to a phonetic cue, according to distinctness intent.\n    \"\"\"\n    weights = np.array([i, 1-i])\n    def PR(ur_sr_target):\n      \"\"\"\n      Weight and combine cues.\n      \"\"\"\n      return np.dot(ur_sr_target[1], weights)\n    return list(map(PR, A))\n  return(I)\n\nHere it is in action!\n\n# Full Neutralization\nI_factory(i = 0)(A(P(L)))\n\n[np.float64(1.0), np.float64(1.0), np.float64(2.0), np.float64(1.0)]\n\n\n\n# Incomplete Neutralization\nI_factory(i = 0.1)(A(P(L)))\n\n[np.float64(1.1), np.float64(1.0), np.float64(2.0), np.float64(1.0)]",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#thoughts",
    "href": "posts/2025/08/2025-08-12_blueprint-phonology-in-python/index.html#thoughts",
    "title": "Blueprint Model of Production, Pythonically",
    "section": "Thoughts",
    "text": "Thoughts\nA kind of interesting thing to note is that the Intention function only looked at and weighted the cues from A(), but the full \\(\\langle\\) UR, SR \\(\\rangle\\) tuples were also there. I don’t know what I() could have done with them, but they were right there.\nOverall, even if I got things wrong, this has felt like an enlightening exercise. It’s definitely an approach to Phonology and Phonetics I’ll be noodling over for a bit.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "Blueprint Model of Production, Pythonically"
    ]
  },
  {
    "objectID": "posts/2025/09/2025-09-11_ggplot-4-0-and-dark-mode/index.html",
    "href": "posts/2025/09/2025-09-11_ggplot-4-0-and-dark-mode/index.html",
    "title": "ggplot2 4.0 and Dark Mode",
    "section": "",
    "text": "Back in April, Quarto v1.7 introduced the ability to display multiple outputs from a single code chunk depending on whether or not the side is in dark mode. This was really exciting for me, because even though I prefer to keep sites in dark mode for personal use, I’ve found that in some presentation and teaching settings, the projector and lighting conditions make dark mode illegible.\nTo make it work, you add renderings: [light, dark] to your code chunk yaml, then plot the same data twice: the first time with light mode color settings, and the second time with dark mode color settings. If you toggle between light & dark mode on this post, the plot should toggle between a light mode and a dark mode version.\n```{r}\n#| renderings: \n#|   - light\n#|   - dark\n#| fig-width: 5\n#| fig-height: 3\nlibrary(withr)\n\nplot(1:10)\n\nwith_par(\n  list(bg = \"#222\", \n       col = \"white\", \n       fg = \"white\",\n       col.axis = \"white\",\n       col.lab = \"white\"\n       \n       ),\n  plot(1:10)\n)\n```\nWith ggplot, I’ve already semi-customized my default theme for when I source my _defaults.R that looks like this:\nlibrary(ggplot2)\n\ntheme_set(\n  theme_minimal(base_size = 16) +\n    theme(\n      text = element_text(family = \"Public Sans\"),\n      panel.grid = element_blank(),\n      legend.key = element_blank(),\n      axis.ticks = element_blank(),\n      axis.line = element_line(color = \"grey60\", linewidth = 0.2),\n      legend.background = element_blank()\n    )\n)\npenguins |&gt;\n  ggplot(\n    aes(bill_len, bill_dep)\n  )+\n    geom_point(\n      aes(color = species)\n    ) -&gt;\n  plot1\n\nplot1\nThen, I had an additional theme_darkmode() I’d add:\ntheme_darkmode &lt;- function(){\n  theme(\n    text = element_text(family = \"Public Sans\", colour = \"white\"),\n    axis.text = element_text(colour = \"white\"),\n    rect = element_rect(colour = \"#222\", fill = \"#222\"),\n    plot.background = element_rect(fill = \"#222\", colour = NA),\n    panel.background = element_rect(fill = \"#424952\"),\n    strip.background = element_rect(fill=\"#3d3d3d\"),\n    strip.text = element_text(color = \"white\")\n  )\n}\nplot1 + theme_darkmode()\nThe issue arose in darkmode when I hadn’t mapped the color aesthetic to any data.\npenguins |&gt;\n  ggplot(\n    aes(bill_len, bill_dep)\n  )+\n    geom_point() -&gt;\n  plot2\n\nplot2 + theme_darkmode()\nThe default point colors weren’t settable by a theme, and I had to use some unpleasant dark rendering functions to get things working nicely.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "09",
      "ggplot2 4.0 and Dark Mode"
    ]
  },
  {
    "objectID": "posts/2025/09/2025-09-11_ggplot-4-0-and-dark-mode/index.html#redoing-it-with-ggplot2-v4",
    "href": "posts/2025/09/2025-09-11_ggplot-4-0-and-dark-mode/index.html#redoing-it-with-ggplot2-v4",
    "title": "ggplot2 4.0 and Dark Mode",
    "section": "Redoing it with ggplot2 v4",
    "text": "Redoing it with ggplot2 v4\nSo, I’m going to rewrite my theme settings to take account of the fact that ggplot2 themes now have “ink” and “paper” color settings you can define. I’ll re-define my default theme with the new theme_sub_*() layers.\n\ntheme_set(\n  theme_minimal(base_size = 16) +\n    theme(\n      text = element_text(family = \"Public Sans\")\n    ) +\n    theme_sub_panel(\n      grid = element_blank(),\n    ) +\n    theme_sub_legend(\n      key = element_blank(),\n      background = element_blank()\n    ) +\n    theme_sub_axis(\n      ticks = element_blank(),\n      line = element_line(color = \"grey60\", linewidth = 0.2)\n    )\n)\n\n\nplot2\n\n\n\n\n\n\n\nAnd now I’ll define my darkmode theme:\n\ntheme_darkmode &lt;- function(){\n  theme_minimal(\n    base_size = 16,\n    paper = \"#222\",\n    ink = \"white\"\n  ) +\n    theme(\n      text = element_text(family = \"Public Sans\")\n    ) +\n    theme_sub_panel(\n      background = element_rect(\n        fill = \"#424952\", color = NA\n      ),\n      grid = element_blank()\n    ) +\n    theme_sub_legend(\n      key = element_blank(),\n      background = element_blank()\n    ) +\n    theme_sub_axis(\n      line = element_line(\n        color = \"grey60\", linewidth = 0.2\n      )\n    )\n}\n\n\nplot2 +\n  theme_darkmode()\n\n\n\n\n\n\n\nSo now to have one code chunk with different outputs in light vs dark mode, it’s just\n\n```{r}\n#| renderings: \n#|   - light\n#|   - dark\nplot2\nplot2 + theme_darkmode()\n```",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "09",
      "ggplot2 4.0 and Dark Mode"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html",
    "href": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html",
    "title": "Setting Up Rogue Scholar",
    "section": "",
    "text": "I’m not sure when I first came across his stuff, but I’ve really admired Andrew Heiss’ public scholarship, including his blog, which is full of handy stuff. At some point I noticed he had post-level DOIs minted. I knew you could mint repository-level DOIs with GitHub and Zenodo, but maintaining each post as a repository and adding them as submodules to a blog is… a lot.\nTurns out, Rogue Scholar is a science blogging platform that will archive your posts and mint a DOI for them! As I learned from reading (and in communicating with the Rogue Scholar Team), the requirements are\nAdditionally, it’s possible to pre-assign your post a DOI using the commonmeta tool. So, this post is in part a description about how I spruced up my quarto infrastructure to enforce some of these requirements, and is also partially a test to make sure I did it right!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Setting Up Rogue Scholar"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html#enforcing-metadata",
    "href": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html#enforcing-metadata",
    "title": "Setting Up Rogue Scholar",
    "section": "Enforcing metadata",
    "text": "Enforcing metadata\nI sometimes have a bad habit of creating a new quarto file for a post, and forgetting to add a date, which would be no good for the RSS feed. And if I had to go back and forth between the Rogue Scholar archive and my blog files to error-free copy-paste DOIs, this was never going to work. My approach was to put together a quarto pre-render script, which you can look at in full here.\nThe upshot is that I use pathlib to walk through and find all of my post-level directories, which I have (maybe annoyingly) organized like so:\nblog/\n└── posts/\n    └── YYYY/\n        └── MM/\n            └── YYYY-MM-DD_slug/\n                └── index.qmd\nI can take the YYYY-MM-DD portion of the post directory to get the date, and I can generate a DOI using commonmeta-py like so\n\nfrom commonmeta import encode_doi\nencode_doi(\n  \"10.59350\" #the Rogue Scholar prefix\n)\n\n'https://doi.org/10.59350/fdgm0-evb66'\n\n\nThen, I drop this info into a post-level _metadata.yml file. If you look at the actual function I wrote to update _metadata.yml, I have a lot of conditionals because\n\nIf _metadata.yml already has a doi field, I don’t want to overwrite it!\nAnd if _metadata.yml already has an accurate date, I don’t want waste the time on writing to it either.\n\nA nice thing about how I set this up, though, is if I create a post directory and don’t wind up publishing the post until a few days later, the date metadata ought to get automatically updated if I just change the post’s directory name.\nWith my pre-render script written, I just added it to the pre-render list in my quarto configuration file.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Setting Up Rogue Scholar"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html#adjusting-my-github-action",
    "href": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html#adjusting-my-github-action",
    "title": "Setting Up Rogue Scholar",
    "section": "Adjusting my GitHub action",
    "text": "Adjusting my GitHub action\nFor the most part, my GitHub publication action isn’t running code chunks because I’m freezing execution locally. But the pre-render script runs every time, and needs to point to the right python environment that has commonmeta-py and pyyaml installed. I’m using renv to manage all of my dependencies, so getting this to work involved adding the following step before Quarto publish:\n- name: Set Quarto Python\n  run: |\n    echo \"QUARTO_PYTHON=renv/python/virtualenvs/renv-python-3.12/bin/python\" &gt;&gt; $GITHUB_ENV",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Setting Up Rogue Scholar"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html#wrapping-up",
    "href": "posts/2025/07/2025-07-31_setting-up-rogue-scholar/index.html#wrapping-up",
    "title": "Setting Up Rogue Scholar",
    "section": "Wrapping up",
    "text": "Wrapping up\nHopefully I configured this all correctly!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Setting Up Rogue Scholar"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-14_light-color/index.html",
    "href": "posts/2025/07/2025-07-14_light-color/index.html",
    "title": "Light vs Dark <color>",
    "section": "",
    "text": "So now I can finally get to visualizing the effect of “light” and other modifiers on colors! When I eventually get to the plotly code, there’s nothing tidy going on, so I’ll be code-folding most of this stuff.\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(tinytable)\nlibrary(plotly)\nlibrary(ggdist)\n\nlibrary(patchwork)\nset.seed(2025-07-08)\nCodemax_text_contrast &lt;- function(bgs){\n  w_contrast &lt;- abs(\n    contrast_ratio(\n      \"white\", bgs,\n      algorithm = \"APCA\"\n    )[,1]\n  )\n  b_contrast &lt;- abs(\n    contrast_ratio(\n      \"black\", bgs,\n      algorithm = \"APCA\"\n    )[,1]\n  )\n  out_cols &lt;- c(\"white\", \"black\")\n\n  out_cols[\n    cbind(w_contrast, b_contrast) |&gt;\n      apply(1, which.max)\n  ]\n  \n}\n# eval: false\n# downloading & saving to avoid \n# downloading on every quarto render\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-08')\n\nfs::dir_create(\"data\")\nwrite_rds(tuesdata, \"data/tuesdata.rds\")\ntuesdata &lt;- read_rds(\"data/tuesdata.rds\")\n\nanswers &lt;- tuesdata$answers\ncolor_ranks &lt;- tuesdata$color_ranks\nusers &lt;- tuesdata$users\ncolor_ranks |&gt; \n  pull(hex) |&gt;\n  hex2RGB() |&gt; \n  as(\"HLS\") |&gt; \n  slot(\"coords\") |&gt; \n  as_tibble() |&gt; \n  bind_cols(\n    color_ranks\n  ) -&gt;\n  color_hls",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Light vs Dark &lt;color&gt;"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-14_light-color/index.html#the-goal",
    "href": "posts/2025/07/2025-07-14_light-color/index.html#the-goal",
    "title": "Light vs Dark <color>",
    "section": "The goal",
    "text": "The goal\nLet’s take “blue” and “light blue” as a quick example. Here are these two labels the “average” hex code associated with these labels, as well as the Hue, Lightness, Saturation values.\n\nCodecolor_hls |&gt; \n  filter(\n    color %in% c(\"blue\", \"light blue\")\n  ) |&gt; \n  mutate(\n    hls = str_glue(\n      \"HLS({round(H)}, {round(L, digits = 2)}, {round(S, digits = 2)})\"\n    )\n  ) |&gt; \n  select(color, hex, hls) -&gt;\n  blues\n\nblues |&gt; \n  tt() |&gt; \n  style_tt(\n    i = 1:2,\n    j = 2,\n    background = blues$hex,\n    color = max_text_contrast(blues$hex)\n  )\n\n\n\n    \n\n      \n\ncolor\n                hex\n                hls\n              \n\n\nblue\n                  #0343df\n                  HLS(223, 0.44, 0.97)\n                \n\nlight blue\n                  #95d0fc\n                  HLS(206, 0.79, 0.94)\n                \n\n\n\n\n\n\nUnsurprisingly, blue ➝ light blue involves an increase in the lightness (from 0.44 to 0.79), but there’s also about a 20 degree rotation of the hue towards green. The saturation stays about constant, but that’s not true for every “&lt;color&gt;”, “light &lt;color&gt;” pair. “Indigo” desaturates quite a bit when it becomes “light indigo.”\n\nCodecolor_hls |&gt; \n  filter(\n    color %in% c(\"indigo\", \"light indigo\")\n  ) |&gt; \n  mutate(\n    hls = str_glue(\n      \"HLS({round(H)}, {round(L, digits = 2)}, {round(S, digits = 2)})\"\n    )\n  ) |&gt; \n  select(color, hex, hls) -&gt;\n  indigos\n\nindigos |&gt; \n  tt() |&gt; \n  style_tt(\n    i = 1:2,\n    j = 2,\n    background = indigos$hex,\n    color = max_text_contrast(indigos$hex)\n  )\n\n\n\n    \n\n      \n\ncolor\n                hex\n                hls\n              \n\n\nindigo\n                  #380282\n                  HLS(265, 0.26, 0.97)\n                \n\nlight indigo\n                  #6d5acf\n                  HLS(250, 0.58, 0.55)\n                \n\n\n\n\n\n\nThe path these colors follow when they move from “&lt;color&gt;” to”&lt;modifier&gt; &lt;color&gt;” is what I want to visualize.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Light vs Dark &lt;color&gt;"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-14_light-color/index.html#modifiers",
    "href": "posts/2025/07/2025-07-14_light-color/index.html#modifiers",
    "title": "Light vs Dark <color>",
    "section": "Modifiers",
    "text": "Modifiers\nSingle word terms\nFirst of all, I need to identify every one word color term so that I can go on to identify their modified versions. This involves some regex work\n\ncolor_hls |&gt; \n  filter(\n    # filter out any labels\n    # containing a space, slash or hyphen\n    str_detect(color, \"[ /-]\", negate = T),\n    # forcing this by hand\n    !color %in% c(\"dark\", \"pale\")\n  ) -&gt;\n  single_terms\n\nnrow(single_terms)\n\n[1] 248\n\n\nLet’s get a look at 5 randomly sampled terms.\n\nCodesingle_terms |&gt; \n  slice_sample(n = 5) |&gt; \n  select(color, rank, hex) |&gt; \n  arrange(rank) -&gt;\n  example_single\n\nexample_single |&gt; \n  tt() |&gt; \n  style_tt(\n    i = 1:5,\n    j = 3,\n    background = example_single$hex,\n    color = max_text_contrast(example_single$hex)\n  )\n\n\n\n    \n\n      \n\ncolor\n                rank\n                hex\n              \n\n\nblack\n                  36\n                  #000000\n                \n\npoop\n                  266\n                  #7f5e00\n                \n\nblurple\n                  295\n                  #5539cc\n                \n\ngrapefruit\n                  774\n                  #fd5956\n                \n\nbruise\n                  884\n                  #7e4071\n                \n\n\n\n\n\n\nI don’t study color terms, and am only vaguely aware of the debates in the area. My shallow investigation into it turns up a 1969 book, Basic color terms: their universality and evolution by Berlin & Kay (third edition: Berlin and Kay (1999)), that seems to be a touchstone work. I couldn’t quickly get an electronic copy, but I was able to find a 2001 encyclopedia entry by Kay (Kay 2001) that includes in the definition of “basic color term”\n\nmust be monolexemic\nmust not be the name of a kind of object\n\nSo from the 5 sampled color terms here, I think these criteria would exclude “blurple” for being a portmanteau, and “poop”, “grapefruit” and “bruise” for being some kind of object. I’m still going to treat all of them as single color terms for this post though.\nI’ll concatenate all single color words into a big regular expression.\n\nsingle_terms |&gt; \n  pull(color) |&gt; \n  str_c(collapse = \"|\") -&gt;\n  single_word_regex\n\ncolor_hls |&gt; \n  mutate(\n    n_color = str_count(\n      color, single_word_regex\n    )\n  ) -&gt;\n  color_hls\n\nLet’s get a sense of how many color terms show up in the labels.\n\nCodecolor_hls |&gt; \n  count(\n    n_color\n  ) |&gt;\n  tt() |&gt;\n  style_tt(\n    j = 2, align = \"r\"\n  )\n\n\n\n    \n\n      \n\nn_color\n                n\n              \n\n\n0\n                  9\n                \n\n1\n                  587\n                \n\n2\n                  353\n                \n\n\n\n\n\n\nThe nine color labels that had 0 matches for any of the single color terms was interesting.\n\nCodecolor_hls |&gt; \n  filter(n_color == 0) -&gt;\n  zeros\n\nzeros |&gt; \n  select(\n    color, rank, hex\n  ) |&gt; \n  tt(caption = \"0 single color words\") |&gt; \n  style_tt(\n    i = 1:nrow(zeros),\n    j = 3,\n    background = zeros$hex,\n    color = max_text_contrast(zeros$hex)\n  )\n\n\n\n    \n\n      \n\n0 single color words\n              \ncolor\n                rank\n                hex\n              \n\n\n\nterra cotta\n                  278\n                  #c9643b\n                \n\nrobin's egg\n                  448\n                  #6dedfd\n                \n\nlight urple\n                  581\n                  #b36ff6\n                \n\npale\n                  669\n                  #fff9d0\n                \n\ndark\n                  736\n                  #1b2431\n                \n\nburnt siena\n                  849\n                  #b75203\n                \n\nlight lavendar\n                  867\n                  #efc0fe\n                \n\negg shell\n                  900\n                  #fffcc4\n                \n\nmacaroni and cheese\n                  934\n                  #efb435\n                \n\n\n\n\n\n\n\nTwo of them (“pale” and “dark”) are modifiers used on their own that I specifically excluded from the single color terms because I wanted to investigate them as modifiers.\nFour of them (“terra cotta”, “robin’s egg”, “egg shell”, and “macaroni and cheese”) are multi-word descriptions that contain no words that were used on their own as color terms (although, the alternative spelling “terracotta” was included).\nThe remaining three (“light urple”, “burnt siena”, and “light lavendar”) misspell the single color terms that they contain.\n\nIt also seems interesting that the maximum number of color terms included in a label is 2! There weren’t any “blueish green yellow” or anything.\nGetting the Modifiers\nI’ll be identifying modified colors labels as\n\nlabels that include just 1 color term,\nand have just 2 words,\nand the second word is a color term\n\nAnd I’ll be defining “modifiers” as the first word in these labels.\n\ncolor_hls |&gt; \n  filter(\n    # 1 color term\n    n_color == 1,\n    # 2 words\n    str_count(color, \" \") == 1,\n    # second word is color\n    str_detect(\n      color,\n      str_c(\" \", single_word_regex,\"$\")\n    )\n  ) |&gt; \n  separate_wider_delim(\n    color,\n    delim = \" \",\n    names = c(\"modifier\", \"base\")\n  ) -&gt;\n  modified_hls\n\nLet’s get a look at the most common modifiers\n\nCodemodified_hls |&gt; \n  count(modifier) |&gt; \n  arrange(desc(n)) |&gt; \n  slice_head(n = 6) -&gt;\n  top_mod\n\ntop_mod |&gt; \n  tt() |&gt; \n  style_tt(\n    j = \"n\",\n    align = \"r\"\n  )\n\n\n\n    \n\n      \n\nmodifier\n                n\n              \n\n\ndark\n                  41\n                \n\nlight\n                  38\n                \n\npale\n                  24\n                \n\nbright\n                  17\n                \n\ndeep\n                  15\n                \n\ndull\n                  9\n                \n\n\n\n\n\n\nI’ll also need to get the base color data for making the comparisons.\n\ncolor_hls |&gt; \n  filter(\n    n_color == 1,\n    str_count(color, \" \") == 0\n  )  |&gt; \n  rename(\n    base = color\n  ) -&gt;\n  base_colors\n\nAnd then a join will get me the modifier, base color in one data frame.\n\nmodified_hls |&gt; \n  left_join(\n    base_colors,\n    by = \"base\",\n    suffix = c(\"_mod\", \"_base\")\n  ) |&gt; \n  filter(is.finite(L_base)) -&gt;\n  comp_hls",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Light vs Dark &lt;color&gt;"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-14_light-color/index.html#general-modifier-analysis",
    "href": "posts/2025/07/2025-07-14_light-color/index.html#general-modifier-analysis",
    "title": "Light vs Dark <color>",
    "section": "General modifier analysis",
    "text": "General modifier analysis\nThe plotly code gets gnarly, so this is all going to be folded.\n\nCodemake_mod_segments &lt;- function(comp_hls, mod) {\n  comp_hls |&gt; \n    filter(\n      modifier == mod\n    ) |&gt; \n    mutate(\n      H_mod = case_when(\n        H_mod - H_base &gt; 180 ~ H_mod - 360,\n        H_mod - H_base &lt; -180 ~ 360 + H_mod,\n        .default = H_mod\n      )\n    ) |&gt; \n    reframe(\n      .by = base,\n      H = seq(H_base, H_mod, length = 50),\n      L = seq(L_base, L_mod, length = 50),\n      S = seq(S_base, S_mod, length = 50),\n      point = 1:50\n    ) |&gt; \n    mutate(\n      hex = HLS(H, L, S) |&gt; hex(),\n      H_radian = H * (pi/180),\n      z = (L*2) - 1,\n      max_S = sqrt(abs((z^2) - 1)),\n      weighted_S = S * max_S,\n      x = sin(H_radian) * weighted_S,\n      y = cos(H_radian) * weighted_S,\n      color = base\n    ) |&gt; \n    nest(\n      .by = base,\n      data = c(x, y, z, hex, color)\n    ) \n}\n\nbase_sphere &lt;- function(){\n  plot_ly(\n    type = \"scatter3d\",\n    mode = \"lines\"\n  ) |&gt;   \n  add_trace(\n    x = sin(seq(0, pi*2, length = 100)),\n    y = cos(seq(0, pi*2, length = 100)),\n    z = rep(0, 100),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  add_trace(\n    x = sin(seq(0, pi*2, length = 100)),\n    z = cos(seq(0, pi*2, length = 100)),\n    y = rep(0, 100),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  add_trace(\n    y = sin(seq(0, pi*2, length = 100)),\n    z = cos(seq(0, pi*2, length = 100)),\n    x = rep(0, 100),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  add_trace(\n    x = sin(seq(0, pi*2, length = 100)) * 0.5,\n    y = cos(seq(0, pi*2, length = 100)) * 0.5,\n    z = rep(0, 100),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  add_trace(\n    x = sin(seq(0, pi*2, length = 100)) * 0.5,\n    z = cos(seq(0, pi*2, length = 100)) * 0.5,\n    y = rep(0, 100),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  add_trace(\n    y = sin(seq(0, pi*2, length = 100)) * 0.5,\n    z = cos(seq(0, pi*2, length = 100)) * 0.5,\n    x = rep(0, 100),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  add_trace(\n    x = rep(0, 5),\n    y = rep(0, 5),\n    z = seq(-1, 1, length = 5),\n    mode = \"lines\",\n    line = list(color = \"grey\"),\n    hoverinfo = \"none\"\n  ) |&gt; \n  layout(\n    showlegend = F,\n    autosize = F,\n    scene = list(\n      xaxis = list(\n        domain = c(-1, 1),\n        color = \"white\"\n      ),\n      yaxis = list(\n        domain = c(-1, 1),\n        color = \"white\"\n      ),\n      zaxis = list(\n        domain = c(-1, 1),\n        color = \"white\"\n      )      \n    )\n  ) \n}\n\nmake_mod_plot &lt;- function(comp_hls, mod, mode  = \"light\") {\n  \n  segments &lt;- make_mod_segments(comp_hls, mod)\n  \n  p &lt;- base_sphere()\n  \n  for(i in seq_along(segments$data)){\n    d &lt;- segments$data[[i]]\n    col &lt;- segments$base[i]\n    markers &lt;- c(rep(\"circle\", length(d$x)-1), \"diamond\")\n    sizes &lt;- c(10, rep(1, length(d$x)-2), 5)\n    p |&gt;\n      add_trace(\n        type = \"scatter3d\",\n        mode = \"markers+lines\",\n        x = ~x,\n        y = ~y,\n        z = ~z,\n        name = col,\n        data = d,\n        mode = \"lines\",\n        line = list(\n          color = d$hex,\n          width = 5\n        )\n        ,marker = list(\n          size = sizes,\n          symbol =  markers,\n          color = d$hex\n        )\n      ) -&gt;\n      p\n  }\n  p |&gt; \n    layout(title = list(text = mod))-&gt;\n    p\n  if (mode == \"dark\") {\n    p |&gt; \n      layout(\n        paper_bgcolor = \"#222\",\n        scene = list(\n          xaxis = list(\n            domain = c(-1, 1),\n            color = \"#222\"\n          ),\n          yaxis = list(\n            domain = c(-1, 1),\n            color = \"#222\"\n          ),\n          zaxis = list(\n            domain = c(-1, 1),\n            color = \"#222\"\n          )      \n        ),\n        title = list(text = mod, font = list(color = \"white\"))\n      ) -&gt;\n      p\n  }\n  p\n}\n\n\n“Light”\nLet’s start with “light”. The location of the base color is indicated with a circle, the modified color with a diamond, with a line connecting the two.\n\n\nCodemake_mod_plot(comp_hls, \"light\")\n\n\n\n\n\n\n\n\nCodemake_mod_plot(comp_hls, \"light\", mode = \"dark\")\n\n\n\n\n\n\nA few cool things here!\n\nEven if very dark base colors shot straight upwards to lighten, this would still involve some degree of relative desaturation (they’d be deeper within the sphere). As it is, though, it looks like they tend to arc inward.\nDesaturated colors that started out in the light hemisphere (like “sage” or “rose”) look like they make a b-line for the surface of the sphere, which involves saturating them.\nOther highly saturated colors look like they stay close to the saturation surface, rather than cutting through the sphere (and desaturating).\nA lot of colors look like they’ve got a bit of a twist towards 90°, greening a little.\n“dark”\n\n\nCodemake_mod_plot(comp_hls, \"dark\")\n\n\n\n\n\n\n\n\nCodemake_mod_plot(comp_hls, \"dark\", mode = \"dark\")\n\n\n\n\n\n\nA lot of this looks like the reverse pattern of “light.”\n\nVery light colors really dive downwards and toward the center (desaturating).\nRelative dark and highly saturated colors stay closer to the surface (keeping their saturation).\n“pale”\n\n\nCodemake_mod_plot(comp_hls, \"pale\")\n\n\n\n\n\n\n\n\nCodemake_mod_plot(comp_hls, \"pale\", mode = \"dark\")\n\n\n\n\n\n\nThis looks really similar to “light”. Maybe the main difference is where the base colors are originating from? Looks like “pale” doesn’t combine with as many colors from the dark hemisphere as “light” did.\n“bright”\n\n\nCodemake_mod_plot(comp_hls, \"bright\")\n\n\n\n\n\n\n\n\nCodemake_mod_plot(comp_hls, \"bright\", mode = \"dark\")\n\n\n\n\n\n\nThis one’s really cool: Everything heads towards the equator!\n“deep”\n\n\nCodemake_mod_plot(comp_hls, \"deep\")\n\n\n\n\n\n\n\n\nCodemake_mod_plot(comp_hls, \"deep\", mode = \"dark\")\n\n\n\n\n\n\nThis looks a lot like “dark”. This might be down to there being fewer colors described as “dark &lt;color&gt;”, but there’s no base colors in the light hemisphere between 0° and 270°.\n“dull”\n\n\nCodemake_mod_plot(comp_hls, \"dull\")\n\n\n\n\n\n\n\n\nCodemake_mod_plot(comp_hls, \"dull\", mode = \"dark\")\n\n\n\n\n\n\nThis looks exactly like you’d expect: everything’s diving towards the core!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Light vs Dark &lt;color&gt;"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-14_light-color/index.html#wrapping-up",
    "href": "posts/2025/07/2025-07-14_light-color/index.html#wrapping-up",
    "title": "Light vs Dark <color>",
    "section": "Wrapping up",
    "text": "Wrapping up\nI think figuring out all of the math to plot these trajectories through color spheres was totally worth it!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Light vs Dark &lt;color&gt;"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-09_color-survey/index.html",
    "href": "posts/2025/07/2025-07-09_color-survey/index.html",
    "title": "tidytuesday color survey",
    "section": "",
    "text": "When I saw that the TidyTuesday dataset was the the XKCD color survey this week, I had to jump in!\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(tinytable)\nlibrary(mgcv)\nlibrary(marginaleffects)\nlibrary(ggdist)\nlibrary(ggdensity)\nlibrary(geomtextpath)\nset.seed(2025-07-08)\n# eval: false\n# downloading & saving to avoid \n# downloading on every quarto render\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-08')\n\nfs::dir_create(\"data\")\nwrite_rds(tuesdata, \"data/tuesdata.rds\")\ntuesdata &lt;- read_rds(\"data/tuesdata.rds\")\n\nanswers &lt;- tuesdata$answers\ncolor_ranks &lt;- tuesdata$color_ranks\nusers &lt;- tuesdata$users\nI first started digging into the answers dataframe.\nsummary(answers)\n\n    user_id           hex                 rank      \n Min.   :     1   Length:1058211     Min.   :1.000  \n 1st Qu.: 39387   Class :character   1st Qu.:2.000  \n Median : 77661   Mode  :character   Median :2.000  \n Mean   : 77261                      Mean   :2.499  \n 3rd Qu.:115068                      3rd Qu.:3.000  \n Max.   :152401                      Max.   :5.000\nThe rank column is a unique ID for color labels, which means these are all of the responses for the top 5 provided color labels.\ncolor_ranks |&gt; \n  slice_head(n = 5) -&gt;\n  top5\n\ntop5 |&gt; \n  tt() |&gt; \n  style_tt(\n    i = 1:5,\n    j = 3,\n    background = top5$hex\n  )\n\n\n\n    \n\n      \n\ncolor\n                rank\n                hex\n              \n\n\npurple\n                  1\n                  #7e1e9c\n                \n\ngreen\n                  2\n                  #15b01a\n                \n\nblue\n                  3\n                  #0343df\n                \n\npink\n                  4\n                  #ff81c0\n                \n\nbrown\n                  5\n                  #653700",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "tidytuesday color survey"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-09_color-survey/index.html#colorspaces",
    "href": "posts/2025/07/2025-07-09_color-survey/index.html#colorspaces",
    "title": "tidytuesday color survey",
    "section": "Colorspaces",
    "text": "Colorspaces\nTo make some plots, I’ll use functions from colorspace to convert the hex codes to HLS (hue, lightness, saturation) values.\n\nlibrary(colorspace)\n\n\n\nH: A value in degrees from 0 to 360.\n\n0/360 ≈ red\n90 ≈ yellow to green\n180 ≈ green to blue\n270 ≈ blue to purple\n\n\nlightness: A value ranging from 0 to 1\nsaturation: A value ranging from 0 to 1\n\nColors become somewhat indistinct at both very high and very low Lightness\n\n# an example color space grid\n# with a fixed saturation\nexpand_grid(\n  H = 0:360,\n  L = seq(0, 1, length = 50)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    hls = HLS(H, L, 0.75) |&gt; list(),\n    hex = hex(hls)\n  ) |&gt; \n  ungroup() -&gt;\n  hl_demo\n\n\nCodehl_demo |&gt; \n  ggplot(\n    aes(H, L)\n  ) +\n    geom_tile(\n      aes(fill = hex)\n    ) +\n    scale_x_continuous(expand = expansion(0)) + \n    scale_y_continuous(expand = expansion(0)) +\n    scale_fill_identity() -&gt;\n    hl_rect_plot\n\n# I'm using the new quarto renderings option\nhl_rect_plot\nhl_rect_plot + theme_dark()\n\n\n\n\n\n\n\nThe indistinctness at the top and bottom is why the colorspace is often visualized as a colorwheel.\n\nCodehl_demo |&gt; \n  ggplot(\n    aes(H, L)\n  ) +\n  geom_tile(\n    aes(\n      fill = hex, \n      color = hex\n    )\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 90, 180, 270),\n    expand = expansion(0)\n  ) +\n  scale_y_continuous(\n    expand = expansion(0)\n  ) +\n  scale_fill_identity() +\n  scale_color_identity() +\n  coord_radial()-&gt;\n  color_wheel\n\ncolor_wheel\ncolor_wheel + theme_dark()\n\n\n\n\n\n\n\nBut it’s probably best thought of as a color sphere with a darker and a lighter hemisphere\n\nCodehl_demo |&gt; \n  mutate(\n    eq_dist = 1 - abs(L - 0.5),\n    hemisphere = case_when(\n      L &gt; 0.5 ~ \"lighter\",\n      .default = \"darker\"\n    ),\n    H2 = case_when(\n      hemisphere == \"lighter\" ~ abs(H - 360),\n      .default = H\n    )\n  ) |&gt; \n  ggplot(\n    aes(H2, eq_dist)\n  ) +\n  geom_tile(\n    aes(\n      fill = hex, \n      color = hex\n    )\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 90, 180, 270),\n    expand = expansion(0)\n  ) +\n  scale_y_continuous(\n    expand = expansion(0)\n  ) +\n  scale_fill_identity() +\n  scale_color_identity() +\n  facet_wrap(~hemisphere, labeller = label_both) +\n  coord_radial() +\n  theme_no_y() +\n  theme_no_x() -&gt;\n  color_sphere\n\ncolor_sphere\ncolor_sphere + theme_dark()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "tidytuesday color survey"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-09_color-survey/index.html#looking-at-blue",
    "href": "posts/2025/07/2025-07-09_color-survey/index.html#looking-at-blue",
    "title": "tidytuesday color survey",
    "section": "Looking at blue",
    "text": "Looking at blue\nI’ll convert all of the answers hex codes to HLS for plotting.\n\nanswers |&gt; \n  pull(hex) |&gt; \n  hex2RGB() |&gt; \n  as(\"HLS\") -&gt;\n  answers_hsl_obj\n\n# the colorspace objects are S4 classes, so...\nanswers_hsl_obj@coords |&gt; \n  as_tibble() -&gt;\n  ans_hsl_cols\n\nanswers |&gt; \n  bind_cols(ans_hsl_cols) |&gt; \n  left_join(\n    color_ranks |&gt; select(rank, color)\n  ) -&gt;\n  answers_hsl\n\nWe can plot a subset of blue to see how it looks:\n\nCodeanswers_hsl |&gt; \n  filter(color == \"blue\") |&gt; \n  # thin to deal with overplotting\n  slice_sample(n = 500) |&gt; \n  # slice up by saturation\n  mutate(\n    saturation = case_when(\n      S &lt;= (1/3) ~ \"low\",\n      S &lt;= (2/3) ~ \"med\",\n      S &lt;= 1 ~ \"high\"\n    ) |&gt; \n      fct_reorder(S)\n  ) |&gt; \n  ggplot(\n    aes(H, L)\n  ) +\n  geom_point(\n    aes(\n      color = hex\n    )\n  ) +\n  scale_x_continuous(\n    limits = c(0, 360),\n    breaks = c(0, 90, 180, 270),\n    expand = expansion(0)\n  ) +\n  scale_y_continuous(\n    expand = expansion(0),\n    limits = c(0, 1)\n  ) +\n  scale_color_identity() +\n  coord_radial() +\n  facet_wrap(\n    ~saturation, \n    labeller = label_both\n  ) -&gt;\n  blues_plot\n\n\nblues_plot\nblues_plot + theme_dark()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "tidytuesday color survey"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-09_color-survey/index.html#blue-vs-green",
    "href": "posts/2025/07/2025-07-09_color-survey/index.html#blue-vs-green",
    "title": "tidytuesday color survey",
    "section": "Blue vs green",
    "text": "Blue vs green\nThere was a “find your boundary between green and blue” quiz that went a little viral recently, and we could probably recreate it here.\n\n# get all green & blue answers\nanswers_hsl |&gt; \n  filter(color %in% c(\"green\", \"blue\")) |&gt; \n  mutate(is_blue = color == \"blue\") -&gt;\n  gb_hsl\n\n# subsample to not overwhelm my computer\ngb_hsl |&gt; \n  slice_sample(prop = 0.1) -&gt; \n  gb_hsl_subset\n\nIf I was doing this for real for real, I’d fit a big bad Bayesian model, but I’ll go for a simpler gam here. But what I will do (because I’ve never had cause to do it before!) is fit a “splines on a sphere” smooth! I’ll need to prepare the data by converting the H and L columns into degrees in radians.\nHere I’ll be honest and say I’m not 100% sure how the lightness dimension works. I’m guessing that it’s actually describing the distance along the center axis of the sphere, and I had to make some notes about how that’d translate into an angle across the surface of the sphere.\n\n\n\n\nConverting the H scale to radians is easier, but looking at the help page for smooth.construct.sos.smooth.spec(), it looks like I’ll need to convert it into east and west hemispheres.\n\nhl_demo |&gt; \n  select(-hls) |&gt; \n  mutate(\n    L_rad = asin((L - 0.5)/0.5),\n    H_rad = (H * (pi/180)) - pi\n  ) -&gt;\n  hl_demo_rad\n\n\n\n\n\n\n\nNoteHemispheres again\n\n\n\n\n\nI just want to see how the conversion to radians affects the color sphere plot I did before.\n\nCodehl_demo_rad |&gt;\n  slice(.by = hex, 1) |&gt; \n  mutate(\n    hemi = case_when(\n      L_rad &gt; 0 ~ \"lighter\",\n      .default = \"darker\"\n    ),\n    dist = abs(L_rad),\n    H2 = case_when(\n      hemi == \"lighter\" ~ abs(H  - 360),\n      .default = H\n    )\n  ) |&gt;\n  ggplot(\n    aes(H2, dist)\n  ) +\n  geom_point(\n    aes(color = hex)\n  ) +\n  scale_color_identity()+\n  scale_y_reverse(\n    expand = expansion(0)\n  ) +\n  scale_x_continuous(\n    expand = expansion(0),\n    limits = c(0, 360),\n    breaks = c(0, 90, 180, 270)\n  ) +\n  facet_wrap(~hemi) +\n  coord_polar() +\n  theme_no_y() +\n  theme_no_x() -&gt;\n  hl_sphere2\n\nhl_sphere2\nhl_sphere2 + theme_dark()\n\n\n\n\n\n\n\n\n\n\nOk! Time for splines on the sphere!\n\ngb_hsl_subset |&gt; \n  mutate(\n    H_rad = (H * (pi/180)) - pi,\n    L_rad = asin((L - 0.5)/0.5)\n  ) -&gt;\n  gb_hsl_subset\n\n\n# eval: false\n# this took a while to fit\ngb_mod &lt;- bam(\n  is_blue ~ s(L_rad, H_rad, bs = \"sos\"),\n  family = binomial,\n  data = gb_hsl_subset\n)\n\nwrite_rds(gb_mod, \"gb_mod.rds\")\n\n\ngb_mod &lt;- read_rds(\"gb_mod.rds\")\n\nJust as a first glance:\n\nplot(gb_mod)\n\n\n\n\n\n\n\nThis is pretty cool!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "tidytuesday color survey"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-09_color-survey/index.html#plotting-the-boundaries",
    "href": "posts/2025/07/2025-07-09_color-survey/index.html#plotting-the-boundaries",
    "title": "tidytuesday color survey",
    "section": "Plotting the boundaries",
    "text": "Plotting the boundaries\nFor plotting the boundaries, I’m going to make a dense grid in the polar coordinate space and then convert that to radians. I’ll use the “sphere” grid to get predictions from the model. This is, admittedly, a lot of math just to get the figure just like I want it.\n\nA lot of π# this creates a single hemisphere\nexpand_grid(\n  x = seq(-pi/2, pi/2, length = 100),\n  y = seq(-pi/2, pi/2, length = 100),\n) |&gt;\n  mutate(\n    dist =  sqrt((x^2) + (y^2))\n  ) |&gt; \n  filter(\n   dist &lt; (pi/2)\n  ) |&gt; \n  mutate(\n    L_rad = (pi/2)-dist,\n    H_rad = atan2(x,y),\n    H_rad = case_when(\n      H_rad &lt; 0 ~ H_rad + (2*pi),\n      .default = H_rad\n    ),\n    H_rad = H_rad - pi\n  ) -&gt;\n  hemi_1\n\n# creating the second hemisphere\nhemi_1 |&gt; \n  mutate(\n    L_rad = -L_rad\n  )-&gt;\n  hemi_2\n\n\nbind_rows(\n  hemi_1, \n  hemi_2\n) -&gt;\n  sphere\n\n# finalizing the predictions grid\nsphere |&gt; \n  mutate(\n    H = (H_rad + pi) / (pi/180),\n    L = (sin(L_rad)/2) + 0.5,\n    hemi = case_when(\n      L_rad &lt; 0 ~ \"darker\",\n      .default = \"lighter\"\n    )\n  )  |&gt; \n  rowwise() |&gt; \n  mutate(\n    hls = HLS(H, L, 0.75) |&gt; list(),\n    hex = hex(hls)\n  ) |&gt; \n  select(-hls) |&gt; \n  ungroup() -&gt;\n  sphere_pred\n\n\nGrabbing the predictions\n\n# eval: false\n# this took a while to run\ngb_mod |&gt; \n  predictions(\n    newdata = sphere_pred\n  ) -&gt;\n  gb_pred\n\nwrite_rds(gb_pred, \"gb_pred.rds\")\n\n\ngb_pred &lt;- read_rds(\"gb_pred.rds\")\n\nAnd plotting\n\nCodegb_pred |&gt; \n  mutate(\n    x = case_when(\n      hemi == \"lighter\" ~ -x,\n      .default = x\n    )\n  ) |&gt; \n  ggplot(\n    aes(x, y)\n  ) + \n  geom_raster(\n    aes(fill = hex)\n  ) +\n  geom_textcontour(\n    aes(z = estimate, color = hemi),\n    breaks = c(0.2, 0.5, 0.8),\n    hjust = 0.2\n  ) +\n  scale_fill_identity() +\n  scale_color_manual(\n    values = c(\"white\", \"black\")\n  ) +\n  guides(color = \"none\") +\n  facet_wrap(~hemi)+\n  coord_fixed()+\n  theme_no_x() + \n  theme_no_y()-&gt;\n  pred_plot\n\npred_plot\npred_plot + theme_dark()\n\n\n\n\n\n\n\nIt looks like on the darker side of the sphere, green has an “advantage” where more colors rotated towards blue, relative to the equator, are classified as “green”, but on the lighter side of the sphere, it goes the other way.\nI’d also started messing around with how labeling a color “light X” vs “dark X” shifts its location across the sphere, but I think this was enough for one post.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "tidytuesday color survey"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html",
    "title": "Introducing tidynorm",
    "section": "",
    "text": "TipThe upshot\n\n\n\nThe tidynorm package has convenience functions for normalizing\n\n\n\nPoint measurements\n\n\nnorm_barkz()\nnorm_deltaF()\nnorm_lobanov()\nnorm_nearey()\nnorm_wattfab()\n\n\n\n\nFormant Tracks\n\n\nnorm_track_barkz()\nnorm_track_deltaF()\nnorm_track_lobanov()\nnorm_track_nearey()\nnorm_track_wattfab()\n\n\n\n\nDCT coefficients\n\n\nnorm_dct_barkz()\nnorm_dct_deltaF()\nnorm_dct_lobanov()\nnorm_dct_nearey()\nnorm_dct_wattfab()\n\n\n\n\n\nAs well as generic functions to implement your own normalization method.\nYou can install tidynorm in your preferred way from CRAN.\n\ninstall.packages(\"tidynorm\")\nLoading Packageslibrary(tidynorm)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggtext)\nlibrary(gt)\nsource(here::here(\"_defaults.R\"))",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#what-is-speaker-vowel-normalization",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#what-is-speaker-vowel-normalization",
    "title": "Introducing tidynorm",
    "section": "What is speaker vowel normalization?",
    "text": "What is speaker vowel normalization?\nImagine a very tall person from London speaking to you. You can probably imagine what their accent sounds like. Now imagine a very short person speaking to you in the same accent. In reality, if you heard these two people speaking in what you perceive to be identical accents, the acoustics of their speech will be different due to their (likely) different vocal tract lengths (VTL).\nUsing some rough heuristics and assumptions, the overall vowel spaces of these two speakers might look something like this:\n\nguestimates functionsvtl_2_formant &lt;- function(vtl, f = 1){\n  dF = 34300/(2*vtl)\n  dF * (f * 0.5)\n}\n\nvowel_polygon &lt;- function(F1, F2){\n  tibble(\n    F1 = c(\n      F1 * 0.6,\n      F1 * 1.45,\n      F1 * 0.6\n      ),\n    F2 = c(\n      F2 * 1.5,\n      F2,\n      F2 * 0.52\n    )\n  )\n}\n\n\n\nvowel-space-plottibble(\n  vtl = seq(14, 17, length = 2)\n) |&gt; \n  mutate(\n    F1 = vtl_2_formant(vtl, 1),\n    F2 = vtl_2_formant(vtl, 2),\n    F3 = vtl_2_formant(vtl, 3)\n  ) |&gt;  mutate(\n    vtl = factor(vtl, labels = c(\"short\", \"long\"))\n  ) -&gt;\n  speaker_formants\n\nspeaker_formants |&gt; \n  reframe(\n    .by = vtl,\n    vowel_polygon(F1, F2)\n  ) |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_polygon(\n    aes(group = vtl, fill = vtl, color = vtl),\n    linewidth = 1,\n    alpha = 0.6\n  ) +\n  scale_y_reverse() +\n  scale_x_reverse() +\n  labs(fill = \"VTL\", color = \"VTL\") +\n  coord_fixed() -&gt; p\n\np\np+theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese speakers’ overall vowel spaces have different center points, and cover different areas.\n\nplotting codespeaker_formants |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_point(\n    aes(color = vtl),\n    size = 5\n  ) +\n  scale_y_reverse() +\n  scale_x_reverse() +\n  labs(title = \"vowel space center\") -&gt; center_p\n\nspeaker_formants |&gt; \n  reframe(\n    .by = vtl,\n    vowel_polygon(F1, F2)\n  ) |&gt; \n  summarise(\n    .by = vtl,\n    b = diff(range(F1)),\n    h = diff(range(F2))\n  ) |&gt; \n  mutate(\n    a = (b/100 * h/100)*2\n  ) |&gt; \n  ggplot(\n    aes(\n      vtl, a\n    )\n  )+\n  geom_col(aes(fill = vtl)) +\n  labs(\n    y = \"area (kHz&lt;sup&gt;2&lt;/sup&gt;)\",\n    title = \"vowel space area\"\n  )+\n  theme(\n    axis.title.y = element_markdown()\n  ) -&gt;\n  area_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of any speaker vowel normalization method is to try to line up speakers’ vowel spaces so that speaker A’s highest, frontest vowels are lined up with speaker B’s, so that speaker A’s lowest, backest vowels are lined up with speaker B’s, etc. Once we have their vowel spaces aligned in a such way that we know similarities between them are matched, we can start investigating any differences.\n\n\n\n\n\n\nTipVowel Space ≠ Pitch\n\n\n\nOne really important thing to keep in mind is that vowel space differences due to different vocal tract lengths are not the same as differences in speakers’ pitch. Differences in speakers’ pitch are caused by differences in how their vocal folds vibrate. You can have two speakers’ with the same exact pitch, but very different vocal tract lengths (& vowel spaces), and vice versa.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#normalization-methods",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#normalization-methods",
    "title": "Introducing tidynorm",
    "section": "Normalization methods",
    "text": "Normalization methods\nAll normalization methods involve some kind of shift in the location of a speaker’s vowel space by some value \\(L\\), scaling the size of a speaker’s vowel space by some value \\(S\\), or both.\n\\[\nF' = \\frac{F-L}{S}\n\\]\nThey way normalization methods mainly differ is in terms of\n\nTransformations applied to the original formant values before calculating \\(L\\) and \\(S\\) (e.g. log, bark).\nThe exact functions used to calculate \\(L\\) and \\(S\\) (e.g. mean, standard deviation)\nThe scope over which \\(L\\) and \\(S\\) are calculated (e.g. across all formants at once, or one formant at a time).\n\nFor example, the logic behind Nearey normalization (Nearey 1978) is that after log transforming vowel spaces, they should really only differ in terms of the location of the centers, not in terms of their area.\n\nplotting codespeaker_formants |&gt; \n  reframe(\n    .by = vtl,\n    vowel_polygon(F1, F2)\n  ) |&gt; \n  mutate(across(F1:F2, log)) |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_polygon(\n    aes(\n      group = vtl, \n      fill = vtl, \n      color = vtl\n      ),\n    alpha = 0.6,\n    linewidth = 1\n  ) +\n  scale_y_reverse(\"log(F1)\") +\n  scale_x_reverse(\"log(F2)\") +\n  coord_fixed() -&gt; p\n\np\np+theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo what Nearey normalization does is\n\nlog transform the data\ntake the average value across all formants\nsubtracts that value from each token\n\nSo where \\(i\\) is the formant number, and \\(j\\) is the token number:\n\\[\nF_{ij}' = \\frac{\\log F_{ij}-L}{1}\n\\]\n\\[\nL = \\frac{1}{MN} \\sum_{i=1}^3\\sum_{j=1}^N \\log F_{ij}\n\\]\nIt’s possible to do this yourself using some tidyverse verbs, but it involves some pivoting between wide and long. This, combined with my work on normalizing vowel formant tracks motivated me to create the tidynorm package.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#normalizing-with-tidynorm",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#normalizing-with-tidynorm",
    "title": "Introducing tidynorm",
    "section": "Normalizing with {tidynorm}\n",
    "text": "Normalizing with {tidynorm}\n\n\nlibrary(tidynorm)\nlibrary(ggdensity)\n\nLet’s start with two speakers’ unnormalized data\n\nplotting codespeaker_data |&gt; \n  ggplot(\n    aes(F2, F1, color = speaker)\n  ) +\n  stat_hdr(\n    probs = 0.95,\n    aes(fill = speaker),\n    linewidth = 1\n  )+\n  scale_x_reverse() +\n  scale_y_reverse() +\n  guides(\n    alpha = \"none\"\n  )+\n  coord_fixed()-&gt;p_unnorm\n\np_unnorm\np_unnorm + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspeaker\nvowel\nplt_vclass\nipa_vclass\nword\nF1\nF2\nF3\n\n\n\n1\ns01\nEY\neyF\nejF\nOKAY\n764\n2088\n2931\n\n\n2\ns01\nAH\nuh\nʌ\nUM\n700\n1881\n3248\n\n\n3\ns01\nAY\nay\naj\nI'M\n889\n1934\n3120\n\n\n4\ns01\nIH\ni\nɪ\nLIVED\n556\n1530\n3462\n\n\n5\ns01\nIH\ni\nɪ\nIN\n612\n2323\n3359\n\n\n6..10696\n\n\n\n\n\n\n\n\n\n\n10697\ns03\nAH\n@\nə\nTHE\n429\n1218\n2352\n\n\n\n\n\n\nWe can implement the logic of Nearey normalization in tidynorm’s function norm_generic().\n\nspeaker_nearey &lt;- speaker_data |&gt;\n  norm_generic(\n    # the formants to normalize\n    F1:F3,\n    \n    # provide the speaker id column\n    .by = speaker,\n    \n    # pre calculation transformation function\n    .pre_trans = log,\n    \n    # location calculation\n    .L = mean(.formant, na.rm = T),\n    \n    # scope\n    .by_formant = F,\n    .by_token = F\n  )\n\nNormalization info\n• normalized with `tidynorm::norm_generic()`\n• normalized `F1`, `F2`, and `F3`\n• normalized values in `F1_n`, `F2_n`, and `F3_n`\n• grouped by `speaker`\n• within formant: FALSE\n• (.formant - mean(.formant, na.rm = T))/(1)\n\n\nI tried to include a helpful message describing what kind of normalization just happened. Here’s how the normalized data looks.\n\nplotting codespeaker_nearey |&gt; \n  ggplot(\n    aes(F2_n, F1_n, color = speaker)\n  ) +\n  stat_hdr(\n    probs = 0.95,\n    aes(fill = speaker),\n    linewidth = 1\n  )+\n  scale_x_reverse() +\n  scale_y_reverse() +\n  guides(\n    alpha = \"none\"\n  )+\n  coord_fixed()-&gt;p_nearey\n\np_nearey\np_nearey + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Lobanov\nThe Lobanov normalization technique (Lobanov 1971) essentially z-scores each formant (\\(L\\) = the mean, \\(S\\) = the standard deviation). We can see how that logic can be implemented in norm_generic() as well.\n\nspeaker_lobanov &lt;- speaker_data |&gt; \n  norm_generic(\n    # the formants to normalize\n    F1:F3,\n    \n    # provide the speaker id column\n    .by = speaker,\n    \n    # location calculation\n    .L = mean(.formant, na.rm = T),\n    \n    # scale calculation\n    .S = sd(.formant, na.rm = T),\n    \n    # scope\n    .by_formant = T,\n    .by_token = F\n  )\n\nNormalization info\n• normalized with `tidynorm::norm_generic()`\n• normalized `F1`, `F2`, and `F3`\n• normalized values in `F1_n`, `F2_n`, and `F3_n`\n• grouped by `speaker`\n• within formant: TRUE\n• (.formant - mean(.formant, na.rm = T))/(sd(.formant, na.rm = T))\n\n\n\nplotting codespeaker_lobanov |&gt; \n  ggplot(\n    aes(F2_n, F1_n, color = speaker)\n  ) +\n  stat_hdr(\n    probs = 0.95,\n    aes(fill = speaker),\n    linewidth = 1\n  )+\n  scale_x_reverse() +\n  scale_y_reverse() +\n  guides(\n    alpha = \"none\"\n  )+\n  coord_fixed()-&gt;p_lobanov\n\np_lobanov\np_lobanov + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvenience functions\nInstead of needing to write out the centering and scaling functions yourself every time, I’ve included convenience functions for some established normalization methods, including\n\nnorm_lobanov()\nnorm_nearey()\nnorm_deltaF()\nnorm_wattfab()\nnorm_barkz()\n\nThey’re all just wrappers around norm_generic(), so if you’re looking for some inspiration implementing your own normalization method, have a look at the source to see how I implemented these.\nWe can apply multiple normalization methods to the same data set by chaining them.\n\nspeaker_multi &lt;- speaker_data |&gt; \n  norm_nearey(\n    F1:F3,\n    .by = speaker,\n    .silent = TRUE\n  ) |&gt; \n  norm_lobanov(\n    F1:F3,\n    .by = speaker, \n    .silent = TRUE\n  ) |&gt; \n  norm_deltaF(\n    F1:F3,\n    .by = speaker, \n    .silent = T\n  )\n\nIf you’ve lost track of which normalization methods you’ve used, and where the normalized values have gone, you can print an information message with check_norm().\n\ncheck_norm(speaker_multi)\n\nNormalization Step\n• normalized with `tidynorm::norm_nearey()`\n• normalized `F1`, `F2`, and `F3`\n• normalized values in `F1_lm`, `F2_lm`, and `F3_lm`\n• grouped by `speaker`\n• within formant: FALSE\n• (.formant - mean(.formant, na.rm = T))/(1)\n\n\nNormalization Step\n• normalized with `tidynorm::norm_lobanov()`\n• normalized `F1`, `F2`, and `F3`\n• normalized values in `F1_z`, `F2_z`, and `F3_z`\n• grouped by `speaker`\n• within formant: TRUE\n• (.formant - mean(.formant, na.rm = T))/(sd(.formant, na.rm = T))\n\n\nNormalization Step\n• normalized with `tidynorm::norm_deltaF()`\n• normalized `F1`, `F2`, and `F3`\n• normalized values in `F1_df`, `F2_df`, and `F3_df`\n• grouped by `speaker`\n• within formant: FALSE\n• (.formant - 0)/(mean(.formant/(.formant_num - 0.5), na.rm = T))",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#normalizing-formant-tracks",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#normalizing-formant-tracks",
    "title": "Introducing tidynorm",
    "section": "Normalizing Formant Tracks",
    "text": "Normalizing Formant Tracks\nWhile I think the advice I have for normalizing formant tracks is good, I admit it’s fairly complex. So I’ve also implemented formant track normalization methods:\n\nnorm_track_generic()\nnorm_track_lobanov()\nnorm_track_nearey()\nnorm_track_wattfab()\nnorm_track_barkz()\n\nLet’s look at one of them in action on formant track data.\n\nplotting codespeaker_tracks |&gt;   \n  filter(\n    .by = c(speaker, id),\n    !any(F1 &gt; 1200)\n  ) -&gt;\n  speaker_tracks\n\nspeaker_tracks |&gt; \n  ggplot(\n    aes(F2, F1, color = speaker)\n  )+\n  geom_path(\n    alpha = 0.2,\n    aes(group = interaction(speaker, id))\n  )+\n  guides(\n    color = guide_legend(override.aes = list(alpha = 1))\n  )+\n  scale_x_reverse()+\n  scale_y_reverse() -&gt; p\np\np + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspeaker\nid\nvowel\nplt_vclass\nword\nt\nF1\nF2\nF3\n\n\n\n1\ns01\n0\nEY\neyF\nOKAY\n32.39\n754\n2145\n2913\n\n\n2\ns01\n0\nEY\neyF\nOKAY\n32.40\n719\n2155\n2913\n\n\n3\ns01\n0\nEY\neyF\nOKAY\n32.41\n752\n2115\n2914\n\n\n4\ns01\n0\nEY\neyF\nOKAY\n32.42\n762\n2087\n2931\n\n\n5\ns01\n0\nEY\neyF\nOKAY\n32.43\n738\n2088\n2933\n\n\n6..19159\n\n\n\n\n\n\n\n\n\n\n\n19160\ns03\n818\nUW\nTuw\nTO\n406.66\n275\n1336\n2364\n\n\n\n\n\n\nIn addition to identifying the speaker ID column, we also need to provide a column that uniquely identifies each token (in this data set, id) and we can provide an optional column of time information.1\n\nspeaker_track_lobanov &lt;- speaker_tracks |&gt; \n  norm_track_lobanov(\n    # the formants to normalize\n    F1:F3,\n    \n    # provide the speaker id column\n    .by = speaker,\n    \n    # provide the token id column\n    .token_id_col = id,\n    \n    # provide a time column\n    .time_col = t\n  )\n\nNormalization info\n• normalized with `tidynorm::norm_track_lobanov()`\n• normalized `F1`, `F2`, and `F3`\n• normalized values in `F1_z`, `F2_z`, and `F3_z`\n• token id column: `id`\n• time column: `t`\n• grouped by `speaker`\n• within formant: TRUE\n• (.formant - mean(.formant, na.rm = T))/sd(.formant, na.rm = T)\n\n\n\nplotting codespeaker_track_lobanov |&gt; \n  ggplot(\n    aes(F2_z, F1_z, color = speaker)\n  )+\n  geom_path(\n    alpha = 0.2,\n    aes(group = interaction(speaker, id))\n  )+\n  guides(\n    color = guide_legend(override.aes = list(alpha = 1))\n  )+\n  scale_x_reverse()+\n  scale_y_reverse() -&gt; p\n\np\np + theme_dark()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#extending-tidynorm",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#extending-tidynorm",
    "title": "Introducing tidynorm",
    "section": "Extending {tidynorm}\n",
    "text": "Extending {tidynorm}\n\nIf there is a normalization method you really like, or are just interested in, and aren’t sure how to implement it in tidynorm, add an issue (ideally with a reference or some math) on the github issues page.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#closing-thoughts",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#closing-thoughts",
    "title": "Introducing tidynorm",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nThis was a complex, but really enjoyable package to write. In addition to wrapping my head around “tidy evaluation”, there was a lot of conceptual work in figuring out how to implement one consistent data processing workflow in norm_generic() that could carry out the normalization methods that have been described in the literature, each in their own way. Something like\n\nTake each formant column and z-score it.\n\nis pretty straightforward, but something like\n\nTransform Hz into Bark, then for each token, subtract the third formant from the first and second formants.\n\nis a little trickier to include in the same workflow.\nThe tidynorm method most different from the method as described in the literature is norm_wattfab(). As described by Watt and Fabricius (2002), the method involves calculating the means of corner vowels. Doing that inside of a tidynorm workflow isn’t impossible, but would sacrifice a lot of generality, and would require users to provide a vowel-class column name every time (since I can’t assume what everyone’s data columns are called). Instead, I went for the shortcut method also used by Johnson (2020), and calculated their \\(S\\) values based on the mean across each formant.\nI might revisit that in the future, but it would require a much more hands-on approach from the user than the other convenience functions currently do.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#footnotes",
    "href": "posts/2025/06/2025-06-16_introducing-tidynorm/index.html#footnotes",
    "title": "Introducing tidynorm",
    "section": "Footnotes",
    "text": "Footnotes\n\nBy default, these track normalization methods will also slightly smooth the formant tracks. But if you don’t want that, you can pass it .order = NA.↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Introducing tidynorm"
    ]
  },
  {
    "objectID": "posts/2024/12/2024-12-17_in-remembrance/index.html",
    "href": "posts/2024/12/2024-12-17_in-remembrance/index.html",
    "title": "In Remembrance",
    "section": "",
    "text": "Weinreich, Labov, and Herzog (1968) is a foundational text in my subfield of linguistics. Entitled “Empirical foundations for a theory of language change,” it’s both a comprehensive review of the field at the time, and a programmatic outlook for the future, laying down problems that researchers are still grappling with today. It’s one of those papers that’s worth dusting off and revisiting every 4 or 5 years as you’ve accumulated more knowledge and experience to see what new glimmers of wisdom you’ll pick up on this time.\nBut, I always have a hard time starting a new read. I get completely sidetracked by the opening lines:\n\nUriel Weinreich died on March 30, 1967. Those who knew him, friends and colleagues in many fields of research, find it difficult to contain their grief. He was not yet forty-one years old. In the last weeks of his life he devoted his major effort to the final revision of this paper, and worked actively on it until two days before his death.\n\nIt seems clear that Weinreich left a long and lasting impact on Bill Labov. The dedication pages of all three volumes of Principles of Linguistic Change (Labov 1994, 2001, 2010) read simply “For Uriel Weinreich”. In his essay “How I got into linguistics, and what I got out of it”, he says\n\nWeinreich was the perfect academic: passionately interested in the ideas of others, brimming over with intellectual honesty, vigor and originality. He protected me from every academic evil. […] Going through his papers in later years, I found that he had written up projects for research that anticipated most of the things I wanted to do. So to this day, I do not know how many of my ideas I brought to linguistics, and how many I got from Weinreich. I would like to think that my students are as lucky as I was, but I know better than that.\n\nThis is a sentiment he repeats in the preface to the second edition of The social stratification of English in New York City (Labov 2006)\n\nI find it very hard to say where his [Weinreich’s] influence is to be found, since it has merged so deeply with my own approach to language, so I must assume that it is everywhere.\n\nI even remember Weinreich coming up in an advising meeting with Bill, as I was talking about the metaphysical nature of language. I said something like, “Maybe Language only exists as the individualized grammars inside each of our heads, but a baby doesn’t know that when they’re trying to learn it.” And Bill said something like, “It seems like you’re really picking up the ideas of my professor, Uriel Weinreich.”\nI obviously never knew Uriel Weinreich. But I do know that whether the loss of a mentor was tragic and untimely, or after a long and fruitful and inspiring career, it is still keenly felt. And I know that everything Bill had to say about Weinreich is what I have to say about Bill.\n\n\n\n\n\n\n\nAfter my parents, the people who have most bent the arc of my life are Bill Labov and his wife & Linguistics Lab co-director, Gillian Sankoff. I went to college being good at school, but without any goals, and without my wits about me for the elite environment at Penn. I am indescribably lucky to have wandered into the Linguistics Lab where Bill and Gillian introduced me to a passion, and showed me that I could meaningfully contribute to our collective understanding of language.\nI wouldn’t be an academic today if not for Bill Labov. Not just for his literal signature on my dissertation, but because some of the best years of my life were spent doing my PhD, and I know this is far from the norm. I know that there can be joy in the in the process, given the right mixture of patience, encouragement & flexibility, and I would like to pay that forward. Since finishing my PhD, my career has had some twists and turns that, quite frankly, nearly led to me calling it quits, but for that guiding light of knowing how things could be better.\nMay his memory be a blessing.\n\n\n\n\nReferences\n\nLabov, William. 1994. Principles of Linguistic Change. Volume 1: Internal Factors. Blackwell.\n\n\n———. 2001. Principles of Linguistic Change. Volume 2: Social Factors. Language in Society. Blackwell.\n\n\n———. 2006. The Social Stratification of English in New York City. Second. Cambridge University Press.\n\n\n———. 2010. Principles of Linguistic Change. Volume 3: Cognitive and Cultural Factors. Blackwell.\n\n\nWeinreich, Uriel, William Labov, and Marvin Herzog. 1968. “Empirical Foundations for a Theory of Language Change.” In Directions for Historical Linguistics, edited by W Lehmann and Y Malkiel. U. of Texas Press.\n\nReuseCC-BY 4.0CitationBibTeX citation:@online{fruehwald2024,\n  author = {Fruehwald, Josef},\n  title = {In {Remembrance}},\n  series = {Væl Space},\n  date = {2024-12-17},\n  url = {https://jofrhwld.github.io/blog/posts/2024/12/2024-12-17_in-remembrance/},\n  doi = {10.59350/c2ebd-vkv37},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. 2024. “In Remembrance.” Væl Space.\nDecember 17, 2024. https://doi.org/10.59350/c2ebd-vkv37.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "12",
      "In Remembrance"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-19_logit-priors-again/index.html",
    "href": "posts/2024/11/2024-11-19_logit-priors-again/index.html",
    "title": "Random effect priors, redo",
    "section": "",
    "text": "For me, teaching stats this semester has turned into a journey of discovering what the distributional and ggdist packages can do for me. The way I make illustrative figures will never be the same. So I thought I’d revisit my post about hierarchical variance priors, this time implementing the figures using these two packages.\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scico)\nlibrary(ggdist)\nlibrary(distributional)\nCustom y theme and scaletheme_no_y &lt;- function(){\n\n  out_theme &lt;- theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank()\n  )\n\n  out_theme  \n}\n\nscale_y_tight &lt;- function(...) {\n  scale_y_continuous(expand = expansion(0), ...)\n}",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "11",
      "Random effect priors, redo"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-19_logit-priors-again/index.html#looking-for-a-more-neutral-distribution",
    "href": "posts/2024/11/2024-11-19_logit-priors-again/index.html#looking-for-a-more-neutral-distribution",
    "title": "Random effect priors, redo",
    "section": "Looking for a more neutral distribution",
    "text": "Looking for a more neutral distribution\nLet’s see what different \\(\\sigma\\)s look like in the probability space.\n\npossible_dists &lt;- tibble(\n  sigma = seq(\n    1, 2.1, by = 0.1\n  ),\n  dist = dist_normal(0, sigma),\n  p_dist = dist_transformed(\n    dist, plogis, qlogis\n  )\n)\n\nggplot(\n  possible_dists,\n  aes(\n    xdist = p_dist,\n    fill = sigma\n  )\n)+\n  stat_slab(\n    color = \"black\",\n    linewidth = 0.5\n  )+\n  scale_fill_scico(\n    palette = \"devon\",\n    guide = \"none\"\n  )+\n  scale_y_tight()+\n  facet_wrap(~sigma)+\n  theme_no_y()+\n  theme_no_x()\n\n\n\n\n\n\n\nIt looks like somewhere between 1.3 and 1.4 is the sweet spot for a maximally flat random effects distribution in the probability space.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "11",
      "Random effect priors, redo"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-19_logit-priors-again/index.html#really-honing-in-on-it",
    "href": "posts/2024/11/2024-11-19_logit-priors-again/index.html#really-honing-in-on-it",
    "title": "Random effect priors, redo",
    "section": "Really honing in on it",
    "text": "Really honing in on it\nI can try getting even more precise by looking at a vectorized version of these distributions, and finding the largest sigma what still has its density peak at 0.5.\n\nlibrary(purrr)\n\n# a vector of sigmas\nsigmas = seq(1.3, 1.5, length = 100)\n\n# a vectorized normal\nvec_dist &lt;- dist_normal(\n  mu = 0,\n  sigma = sigmas\n) \n\n# a vectorized ilogit(normal)\nvec_p_dist &lt;- dist_transformed(\n  vec_dist,\n  plogis,\n  qlogis\n)\n\n# the density function\n# from 0 to 0.5\np_densities &lt;- density(\n  vec_p_dist, \n  seq(0, 0.5, length = 100)\n)\n\n# The index of the max\n# density\nwhere_is_max &lt;- p_densities |&gt; \n  map_vec(\n    which.max\n  ) \n\n# if where_is_max == 100\n# peak density was at 0.5\nflat_idx &lt;- (where_is_max == 100) |&gt; \n  which() |&gt; \n  max()\n\nflattest_sigma &lt;- sigmas[flat_idx]\n\nflattest_sigma\n\n[1] 1.413131\n\n\nLet’s take a look at it:\n\nflat_pdist &lt;- dist_normal(0, flattest_sigma) |&gt; \n  dist_transformed(plogis,qlogis)\n  \nggplot()+\n  stat_slab(\n    aes(\n      xdist = flat_pdist\n    ),\n    fill = \"#EE6677\"\n  )+\n  scale_y_tight()+\n  theme_no_y()",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "11",
      "Random effect priors, redo"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html",
    "title": "Building up a complex {gt} table.",
    "section": "",
    "text": "Recently, for class notes on probability/the central limit theorem, I wanted to recreate the table of 2d6 values that I made here. A really cool thing I found between that blog post and now is that gt has afmt_icon() operation that will replace text with its fontawesome icon.\nlibrary(tidyverse)\nlibrary(gt)\ntibble(\n icon = c(\n   \"face-smile\",\n   \"hippo\",\n   \"pizza-slice\"\n )\n) |&gt; \n  gt() |&gt; \n  fmt_icon()\n\n\n\n\n\n\nicon\n\n\n\n\n\nFace Smile\n\n\n\n\n\n\nHippo\n\n\n\n\n\n\nPizza Slice\nAnd font-awesome has icons for each face of a 6 sided die!\nlibrary(english)\n\ntibble(\n  icon = str_glue(\n    \"dice-{as.english(1:6)}\"\n  )\n) |&gt; \n  gt() |&gt; \n  fmt_icon()\n\n\n\n\n\n\nicon\n\n\n\n\n\nDice One\n\n\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Six\nHere’s how I got to a result that I liked. If anyone has suggestions for how to do this more cleanly, I’d love to hear about it!",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#the-actual-die-rolls",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#the-actual-die-rolls",
    "title": "Building up a complex {gt} table.",
    "section": "The actual die rolls",
    "text": "The actual die rolls\nGetting the actual die rolls and their total is a simple expand_grid() .\n\nexpand_grid(\n  die_a = 1:6,\n  die_b = 1:6\n) |&gt; \n  mutate(\n    total = die_a + die_b\n  )-&gt;\n  rolls_df\n\nhead(rolls_df)\n\n# A tibble: 6 × 3\n  die_a die_b total\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1     1     2\n2     1     2     3\n3     1     3     4\n4     1     4     5\n5     1     5     6\n6     1     6     7",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#injecting-the-fontawesome-icon-names",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#injecting-the-fontawesome-icon-names",
    "title": "Building up a complex {gt} table.",
    "section": "Injecting the fontawesome icon names",
    "text": "Injecting the fontawesome icon names\nIn my original code, I used some joins here, but I just found the {english} package, which will let me mutate the die_ columns directly.\n\nrolls_df |&gt; \n  mutate(\n    across(\n      starts_with(\"die_\"),\n      ~ str_glue(\n        \"dice-{word}\",\n        word = as.english(.x)\n      )\n    )\n  ) -&gt;\n  rolls_df\n\nhead(rolls_df)\n\n# A tibble: 6 × 3\n  die_a    die_b      total\n  &lt;glue&gt;   &lt;glue&gt;     &lt;int&gt;\n1 dice-one dice-one       2\n2 dice-one dice-two       3\n3 dice-one dice-three     4\n4 dice-one dice-four      5\n5 dice-one dice-five      6\n6 dice-one dice-six       7",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#identifying-combos",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#identifying-combos",
    "title": "Building up a complex {gt} table.",
    "section": "Identifying combos",
    "text": "Identifying combos\nI’m going to start pivoting a bunch, so now is the best time to give an id to each unique combo, as well as the total number of combos per total.\n\nrolls_df |&gt; \n  mutate(\n    .by = total,\n    id = row_number(),\n    n = n()\n  ) -&gt;\n  rolls_df\n\nhead(rolls_df)\n\n# A tibble: 6 × 5\n  die_a    die_b      total    id     n\n  &lt;glue&gt;   &lt;glue&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 dice-one dice-one       2     1     1\n2 dice-one dice-two       3     1     2\n3 dice-one dice-three     4     1     3\n4 dice-one dice-four      5     1     4\n5 dice-one dice-five      6     1     5\n6 dice-one dice-six       7     1     6",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#getting-wide",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#getting-wide",
    "title": "Building up a complex {gt} table.",
    "section": "Getting wide",
    "text": "Getting wide\nI’ll eventually want one column per die, with its combination id appended to it, which means pivoting long, merging the columns together, then pivoting wide.\nPivoting long\n\nrolls_df |&gt; \n  pivot_longer(\n    starts_with(\"die\"),\n    names_to = \"die\",\n    values_to = \"icon\"\n  ) |&gt; \n  unite(\n    c(die, id),\n    col = \"die_id\"\n  )-&gt;\n  rolls_long\n\nhead(rolls_long)\n\n# A tibble: 6 × 4\n  total die_id      n icon      \n  &lt;int&gt; &lt;chr&gt;   &lt;int&gt; &lt;glue&gt;    \n1     2 die_a_1     1 dice-one  \n2     2 die_b_1     1 dice-one  \n3     3 die_a_1     2 dice-one  \n4     3 die_b_1     2 dice-two  \n5     4 die_a_1     3 dice-one  \n6     4 die_b_1     3 dice-three\n\n\nPivoting wide\n\nrolls_long |&gt; \n  pivot_wider(\n    names_from = die_id,\n    values_from = icon\n  ) -&gt;\n  rolls_wide\n\nhead(rolls_wide)\n\n# A tibble: 6 × 14\n  total     n die_a_1  die_b_1   die_a_2 die_b_2 die_a_3 die_b_3 die_a_4 die_b_4\n  &lt;int&gt; &lt;int&gt; &lt;glue&gt;   &lt;glue&gt;    &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt;  &lt;glue&gt; \n1     2     1 dice-one dice-one  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n2     3     2 dice-one dice-two  dice-t… dice-o… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n3     4     3 dice-one dice-thr… dice-t… dice-t… dice-t… dice-o… &lt;NA&gt;    &lt;NA&gt;   \n4     5     4 dice-one dice-four dice-t… dice-t… dice-t… dice-t… dice-f… dice-o…\n5     6     5 dice-one dice-five dice-t… dice-f… dice-t… dice-t… dice-f… dice-t…\n6     7     6 dice-one dice-six  dice-t… dice-f… dice-t… dice-f… dice-f… dice-t…\n# ℹ 4 more variables: die_a_5 &lt;glue&gt;, die_b_5 &lt;glue&gt;, die_a_6 &lt;glue&gt;,\n#   die_b_6 &lt;glue&gt;\n\n\nNow, I’ve got some well named columns identifying die a and die b, as well as numeric ids for each unique combination. I’ll use these for coloring the dice icons and merging columns.\nBut, I also want to move the n column, and add a proportion column.\n\nrolls_wide |&gt; \n  relocate(\n    n,\n    .after = last_col()\n  ) |&gt; \n  mutate(\n    prop = n/sum(n)\n  )-&gt;\n  rolls_wide",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#formatting-the-icons",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#formatting-the-icons",
    "title": "Building up a complex {gt} table.",
    "section": "Formatting the icons",
    "text": "Formatting the icons\nTo make sure it’s clear I’m working with two die, I want die_a and die_b to be different colors, which I can make happen with two uses of fmt_icon().\n\nrolls_wide |&gt; \n  gt() |&gt;   \n  fmt_icon(\n    starts_with(\"die_a\"),\n    fill_color = \"#CC6677\"\n  ) |&gt;\n  fmt_icon(\n    starts_with(\"die_b\"),\n    fill_color = \"#4477AA\"\n  ) -&gt;\n  out_tbl\n\nout_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal\ndie_a_1\ndie_b_1\ndie_a_2\ndie_b_2\ndie_a_3\ndie_b_3\ndie_a_4\ndie_b_4\ndie_a_5\ndie_b_5\ndie_a_6\ndie_b_6\nn\nprop\n\n\n\n2\n\n\nDice One\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n0.02777778\n\n\n3\n\n\nDice One\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n0.05555556\n\n\n4\n\n\nDice One\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n3\n0.08333333\n\n\n5\n\n\nDice One\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\nDice One\n\n\nNA\nNA\nNA\nNA\n4\n0.11111111\n\n\n6\n\n\nDice One\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\nDice One\n\n\nNA\nNA\n5\n0.13888889\n\n\n7\n\n\nDice One\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\nDice One\n\n\n6\n0.16666667\n\n\n8\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\nNA\nNA\n5\n0.13888889\n\n\n9\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\nNA\nNA\nNA\nNA\n4\n0.11111111\n\n\n10\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n3\n0.08333333\n\n\n11\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n0.05555556\n\n\n12\n\n\nDice Six\n\n\n\n\nDice Six\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n0.02777778",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#dropping-the-missing-values",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#dropping-the-missing-values",
    "title": "Building up a complex {gt} table.",
    "section": "Dropping the missing values",
    "text": "Dropping the missing values\nI want to drop out all of the missing values. I found that if I replace them with just \"\", for some reason the row with no NAs winds up being narrower than the rest, but if I replace them with a zero-width space, it turns out more compact.\n\nout_tbl |&gt; \n  sub_missing(missing_text = html(\"&ZeroWidthSpace;\")) -&gt;\n  out_tbl",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#the-ugliest-part",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#the-ugliest-part",
    "title": "Building up a complex {gt} table.",
    "section": "The ugliest part",
    "text": "The ugliest part\nNow, I need to merge the columns together with cols_merge(). This is where the code gets a little ugly, what I want to be able to say is\n\nMerge two columns if they match in the last two characters\n\nMaybe there’s a way to express this with tidyselect verbs that I’m just not good enough with. In the original code, I just used cols_merge() 6 times, which would look like:\n\nout_tbl |&gt; \n  cols_merge(ends_with(\"1\")) |&gt; \n  cols_merge(ends_with(\"2\")) |&gt; \n  cols_merge(ends_with(\"3\")) |&gt;\n  cols_merge(ends_with(\"4\")) |&gt;\n  cols_merge(ends_with(\"5\")) |&gt;\n  cols_merge(ends_with(\"6\"))\n\nIt just occurred to me that something from purrr might be the right tool, and refreshed myself on purrr::reduce().\n\nreduce(\n  as.character(1:6),\n  \\(acc, nxt){\n    acc |&gt; \n      cols_merge(\n        ends_with(nxt)\n      )\n  },\n  .init = out_tbl\n) -&gt;\n  out_tbl\n\nout_tbl \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal\ndie_a_1\ndie_a_2\ndie_a_3\ndie_a_4\ndie_a_5\ndie_a_6\nn\nprop\n\n\n\n2\n\n\n\nDice One\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.02777778\n\n\n3\n\n\n\nDice One\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.05555556\n\n\n4\n\n\n\nDice One\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08333333\n\n\n5\n\n\n\nDice One\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n4\n0.11111111\n\n\n6\n\n\n\nDice One\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice One\n\n\n\n​ ​\n5\n0.13888889\n\n\n7\n\n\n\nDice One\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice One\n\n\n\n6\n0.16666667\n\n\n8\n\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\n\n​ ​\n5\n0.13888889\n\n\n9\n\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\n\n​ ​\n​ ​\n4\n0.11111111\n\n\n10\n\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08333333\n\n\n11\n\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.05555556\n\n\n12\n\n\n\nDice Six\n\n\n\n\nDice Six\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.02777778\n\n\n\n\n\n\nTo be honest, even though the reduce() approach is more programmery, writing out each cols_merge() individually is more readable…\nMaybe if I wanted to expand this out to 3d6, the reduce() approach would be better. But at that point, I’d also be creating a table of 27 columns, and at that point the illustrative nature of the table would probably be lost.\n\n\n\n\n\n\nNote27 Columns\n\n\n\n\n\n\n# dice roll package\nlibrary(droll)\n\nd6 &lt;- d(6)\n\none_combo_p &lt;- droll(3, 3*d6)\ntotal_combo &lt;- 1/one_combo_p\n\nmax_combo_p &lt;- droll(10, 3*d6)\n\ntotal_combo * max_combo_p\n\n[1] 27",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-15_gt-table/index.html#final-table-finessing",
    "href": "posts/2024/09/2024-09-15_gt-table/index.html#final-table-finessing",
    "title": "Building up a complex {gt} table.",
    "section": "Final table finessing",
    "text": "Final table finessing\nNow, I want to\n\nremove the column names from the dice columns\nadd a grand summary row\nformat the probabilities down to 2 digits\nadd some css so that the table will match lightmode/darkmode settings\n\n\nout_tbl |&gt; \n  # no dice column label\n  cols_label(\n    starts_with(\"die\") ~ \"\"\n  ) |&gt; \n  # grand summary\n  grand_summary_rows(\n    columns = c(n, prop),\n    fns = list(total ~ sum(.)),\n    missing_text = \"\"\n  ) |&gt; \n  # two digits max \n  fmt_number(\n    columns = prop,\n    decimals = 2\n  ) |&gt; \n  # font setting\n  opt_table_font(\n    font = list(\n      google_font(name = \"Public Sans\"),\n      default_fonts()\n    )\n  ) |&gt;     \n  # light/darkmode matching\n  tab_style(\n    style = \"\n      background-color: var(--bs-body-bg);\n      color: var(--bs-body-color)\n    \",\n    locations = list(\n      cells_column_labels(),\n      cells_column_spanners(),\n      cells_row_groups(),\n      cells_body(),\n      cells_grand_summary(),\n      cells_stub_grand_summary(),\n      cells_stub(),\n      cells_stubhead()\n    )\n  )  -&gt;\n  out_tbl\n\nout_tbl  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal\n\n\n\n\n\n\nn\nprop\n\n\n\n\n2\n\n\n\nDice One\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.03\n\n\n\n3\n\n\n\nDice One\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.06\n\n\n\n4\n\n\n\nDice One\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08\n\n\n\n5\n\n\n\nDice One\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice One\n\n\n\n​ ​\n​ ​\n4\n0.11\n\n\n\n6\n\n\n\nDice One\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice One\n\n\n\n​ ​\n5\n0.14\n\n\n\n7\n\n\n\nDice One\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Two\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Two\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice One\n\n\n\n6\n0.17\n\n\n\n8\n\n\n\nDice Two\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Three\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Three\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Two\n\n\n\n​ ​\n5\n0.14\n\n\n\n9\n\n\n\nDice Three\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Four\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Four\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Three\n\n\n\n​ ​\n​ ​\n4\n0.11\n\n\n\n10\n\n\n\nDice Four\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Five\n\n\n\n\nDice Five\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Four\n\n\n\n​ ​\n​ ​\n​ ​\n3\n0.08\n\n\n\n11\n\n\n\nDice Five\n\n\n\n\nDice Six\n\n\n\n\n\n\nDice Six\n\n\n\n\nDice Five\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n2\n0.06\n\n\n\n12\n\n\n\nDice Six\n\n\n\n\nDice Six\n\n\n\n​ ​\n​ ​\n​ ​\n​ ​\n​ ​\n1\n0.03\n\n\ntotal\n\n\n\n\n\n\n\n36\n1",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Building up a complex `{gt}` table."
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "",
    "text": "I’m teaching a class on quantitative methods in linguistics, and after getting tired, over the years, of debugging everyone’s computer before we could get to the content of learning R (much less quantitative methods), I decided to seize control. I’m running the course through GitHub Classroom which comes with an educational allowance for using GitHub codespaces. There have been pros and cons to running things through there:\nAfter working through some issues today, I thought I’d start a running document of decisions I’ve already made, leaving space for updates down the line. This is for my own sake, to refer back to, but also for any other poor souls out there.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-devcontainers-can-take-a-long-time-to-load.",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-devcontainers-can-take-a-long-time-to-load.",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: Devcontainers can take a long time to load.",
    "text": "Problem: Devcontainers can take a long time to load.\nLaunching a codespace can take a long time, especially if you are doing a lot of configuration at the creation time. My solution was to create my own devcontainer image, which should speed some things up.\n\nI forked the rocker-org/devcontainer-images repo\nI made some tweaks to the to the build/args.json file and the build/matrix.json file, which mostly have the effect of changing the name of the resulting devcontainer image, and the paths to source files used to generate it.\n\nI also put in a text file called .rpackages in a sensible place in the devcontainer repo and added the following lines to the dockerfile to get them installed on the image.\n\ndockerfile\n\nCOPY assets/.rpackages /home/rstudio/.rpackages\nRUN install2.r --error --skipinstalled $(cat /home/rstudio/.rpackages)",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-dont-want-a-login-screen-to-rstudio",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-dont-want-a-login-screen-to-rstudio",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: I don’t want a login screen to Rstudio",
    "text": "Problem: I don’t want a login screen to Rstudio\nBy default, if you just launch rserver from the devcontainer image (which has RStudio Server installed), you’ll get a login screen when you visit the forwarded port. I could have left this, but needing to explain “It looks like a login screen, but the username and password are just rstudio/rstudio” is too sharp an edge.\nFortunately, there’s an rstudio server devcontainer feature I can add to the template assignment that enables single sign on mode, and even updates the default working directory to be the workspace, rather than ~.\n\njson\n\n{\n  \"image\": \"ghcr.io/lin611-2024/devcontainer/bayesdevcontainer:4\",\n  \"features\": {\n    \"ghcr.io/rocker-org/devcontainer-features/rstudio-server\": {\n      \"singleUser\": true\n    }\n  }\n}",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-on-launch-rstudio-doesnt-open-a-project",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-on-launch-rstudio-doesnt-open-a-project",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: On launch, RStudio doesn’t open a project",
    "text": "Problem: On launch, RStudio doesn’t open a project\nEven if the default working directory has an .Rproj file in it, RStudio doesn’t default open it as a project on launch, which also means the git pane isn’t available. This was another sharp edge that I knew would cause me problems.\nI found a hook I could add to the system .Rprofile to fix this\n\nr\n\nsetHook(\n  \"rstudio.sessionInit\",\n  function(newSession){\n    if (newSession && is.null(rstudioapi::getActiveProject())) {\n      rstudioapi::openProject(\".\")\n    }\n  },\n  action = \"append\"\n)\nI added this to a sensible place in the devcontainer repository, then I had to make sure to copy it to the image in the Dockerfile.\n\ndockerfile\n\nCOPY --chown=rstudio:rstudio assets/.Rprofile /home/rstudio/",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-there-were-no-git-credentials-in-rstudio-git-pane",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-there-were-no-git-credentials-in-rstudio-git-pane",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: There were no git credentials in RStudio git pane",
    "text": "Problem: There were no git credentials in RStudio git pane\nIf you use the VS Code interface in a codespace, you can push and pull to the original repo, no problem. But I couldn’t do it inside the RStudio Server session. I could have had students go back to VS Code for this, but, again, another sharp edge.\nThe necessary environment variables that are available in the VS Code interface are GITHUB_USER and GITHUB_TOKEN, but they weren’t available to R. This is something that needs to be dealt with after the codespace has built (not in the devcontainer image), so I added the following to assignment template’s devcontainer.json.\n\njson\n\n \"postAttachCommand\": {\n   \"github_user\": \"echo \\\"GITHUB_USER=$GITHUB_USER\\\"&gt;&gt;~/.Renviron && echo \\\"GITHUB_TOKEN=$GITHUB_TOKEN\\\"&gt;&gt;~/.Renviron\"\n}\nThis just copies the values in the environment variables to R’s environment variables.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-wanted-the-most-up-to-date-quarto",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-i-wanted-the-most-up-to-date-quarto",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: I wanted the most up-to-date Quarto",
    "text": "Problem: I wanted the most up-to-date Quarto\nWe’re going to be talking a bit about authoring, sometimes using typst, and the newest update that styles tables from gt was too good to not have. This required going back to the dockerfile.\n\ndockerfile\n\nARG QUARTOVERSION=1.5.56\n\n# stuff\n\nARG QUARTOVERSION\n\nRUN cd /usr/share/ \\\n  && wget https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTOVERSION}/quarto-${QUARTOVERSION}-linux-amd64.tar.gz \\\n  && tar -xvzf quarto-${QUARTOVERSION}-linux-amd64.tar.gz \\\n  && mv quarto-${QUARTOVERSION} .quarto \\\n  && rm /usr/local/bin/quarto \\\n  && ln -s .quarto quarto\n\nENV PATH \"$PATH:/usr/share/quarto/bin\"",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-the-.rproj-file-name-didnt-match-the-repository-name",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-the-.rproj-file-name-didnt-match-the-repository-name",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: The .Rproj file name didn’t match the repository name",
    "text": "Problem: The .Rproj file name didn’t match the repository name\nThis might not have mattered too much, but GitHub classroom creates a new repository for each student called {assignment-name}-{username}. I thought it would be not quite the same as using RStudio projects locally if the project file was called assignment-template.Rproj in all cases.\nThis was another case for adding a post-attach command in the assignment template devcontainer.json.\n\njson\n\n\"postAttachCommand\": {\n  ...\n  \"project-rename\": \"(mv *.Rproj $(basename $(pwd)).Rproj || echo 'moved' )\",\n  ...\n}",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-a-github-classroom-race-condition",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-a-github-classroom-race-condition",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: A GitHub Classroom race condition",
    "text": "Problem: A GitHub Classroom race condition\nSo, when github classroom creates the student’s assignment repo, it adds two buttons to the README.md. But, if the student is quick on the draw and launch their codespace before the bot commits the change to the readme, their codespace will be a commit behind the remote, but they won’t necessarily realize this.\nThen, if they keep working and commit, they’ve got a divergent branch from the remote, and they’ll need to reconcile it, which the RStudio git gui interface doesn’t really have capacity for.\nThis is another item for the devcontainer.json\n\njson\n\n\"postAttachCommand\": {\n  ...\n  \"git-config\": \"git config pull.rebase false && git pull\",\n  ...\n}",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-codespaces-going-idle-too-quickly",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#problem-codespaces-going-idle-too-quickly",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Problem: Codespaces going idle too quickly",
    "text": "Problem: Codespaces going idle too quickly\nI’m still trying to debug this issue (I’ve opened an issue with GH, waiting to hear back). Students’ codespaces seem to be going idle very quickly when we’re working on an in-class assignment. This results in their RStudio session starting to return mysterious 400 errors.\nSome things I’ve already done:\n\nNew codespaces in the organization are owned by the organization.\nI’ve set a custom timeout policy for codespaces within the organization.\n\nThis didn’t seem to fix things. A few more hypotheses I have are:\n\nBecause we’re working in RStudio, largely in Quarto notebooks, and not in the VS Code interface, the codespace isn’t registering any activity.\nThere might be some kind of browser specific memory saving thing going on, but that I’m not 100% on.\n\nTo try to deal with this, I’ve added an awake.sh shell script to the .devcontainer directory.\n\nsh\n\n#!/bin/sh\nwhile true\ndo\n  echo \"Stay Awake\" \n  sleep 20\ndone\nThen added the following to my devcontainer.json\n\njson\n\n \"postAttachCommand\": {\n   ...\n   \"awake\": \"timeout --foreground 90m sh .devcontainer/awake.sh\"\n...\n}\nThis will print Stay Awake every 20 seconds for 90 minutes… hopefully keeping our RStudio sessions active long enough for a class meeting.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#rstudio-prefs",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#rstudio-prefs",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "RStudio Prefs",
    "text": "RStudio Prefs\nSome rstudio preferences I wanted to set were:\n\nRainbow parentheses\nUse of the native pipe operator\nUse of the Menlo font in the editor.\n\n\n\n\n\n\n\nNoteUpdate (2024-09-10)\n\n\n\nI also just found out you can globallychange the default line-wrapping behavior in the source document when working in the Visual Editor. I’ll go with sentence-level wrapping, which will be nice for git diffs.\n\n\nTo get these working, I added the following to the default rstudio-prefs.json which I included in the devcontainer repository\n\njson\n\n{\n  \"save_workspace\": \"never\",\n  \"always_save_history\": false,\n  \"reuse_sessions_for_project_links\": true,\n  \"posix_terminal_shell\": \"bash\",\n  \"initial_working_directory\": \"/workspaces\",\n  \"visual_markdown_editing_is_default\": true,\n  \"editor_theme\": \"Tomorrow\",\n  \"rainbow_parentheses\": true,\n  \"insert_native_pipe_operator\": true,\n  \"server_editor_font_enabled\": true,\n  \"server_editor_font\": \"Menlo\"\n  \"visual_markdown_editing_wrap\": \"sentence\"\n}\nThen added this to the dockerfile\n\ndockerfile\n\nCOPY --chown=rstudio:rstudio assets/rstudio-prefs.json /home/rstudio/.config/rstudio/",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#keybindings",
    "href": "posts/2024/09/2024-09-06_rserver-codespaces/index.html#keybindings",
    "title": "Troubleshooting RStudio for GitHub Classrom and Codespaces",
    "section": "Keybindings",
    "text": "Keybindings\nThere’s really just one custom keybinding that I really like, which is  for inserting the pipe.\n\nTo make this work, I created a keybindings directory in the devcontainer repo with empty addins.json and editor_bindings.json files, and a rstudio_bindings.json with the following:\n\njson\n\n{\n  \"insertPipeOperator\": \"Cmd+.\"\n}\nThen added the following to the dockerfile\n\ndockerfile\n\nCOPY --chown=rstudio:rstudio assets/keybindings/ /home/rstudio/.config/rstudio/keybindings/",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Troubleshooting RStudio for GitHub Classrom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "",
    "text": "I’ve been working a lot with the Discrete Cosine Transform in python, specifically as it’s implemented in scipy. But, I really prefer doing my data munging and stats in R.1 What to do!\nI knew the answer rested in using the reticulate package, which lets you communicate back and forth between python and R, but I hadn’t appreciated how cool reticulate was, which is why I’m making this blog post.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#setup",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#setup",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Setup",
    "text": "Setup\n\n\nr\n\nIrrelevant R setuplibrary(tidyverse)\nlibrary(geomtextpath)\nlibrary(gt)\nsource(here::here(\"_defaults.R\"))\n\n\nIn order to communicate back and forth, I’ll need to load the reticulate package.\n\n\nr\n\nlibrary(reticulate)\n\nI’ll also need to make sure that I’ve got scipy installed for python, which you can do with reticulate::py_install().\n\n\nr\n\nreticulate::py_install(\"scipy\")\n\n\n\n\n\n\n\nNotePython environments\n\n\n\n\n\nPython environments are kind of notorious for being confusing to keep straight, which is why a whole host of tools for managing how python is installed have cropped up. Inside all of my R projects, I already use renv, which has an option to also manage your python environment for a project like so:\n\nr\n\nrenv::use_python()\nIf we pull up the python configuration for reticulate, we can see it’s installed in the local renv project.\n\n\nr\n\nreticulate::py_config()$python\n\n[1] \"/Users/joseffruehwald/Documents/blog/renv/python/virtualenvs/renv-python-3.11/bin/python\"\n\n\nBut, if you have a favorite other way of managing your python environments, there are ways to point reticulate at those too.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#background-what-is-the-dct",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#background-what-is-the-dct",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Background: What is the DCT?",
    "text": "Background: What is the DCT?\nThe Discrete Cosine Transform is very similar to the Fourier Transform (if that helps). It takes in a signal of wiggly data, and re-describes it in terms of weights on cosine functions of increasing frequency. Figure 1 plots the first DCT functions as they’re defined by a particular set of options in scipy.fft.dct.\n\n\n\n\n\n\n\nFigure 1: The first 5 cosine functions of the DCT.\n\n\n\n\nIf you use the same number of cosine functions as you had data points in the original signal, you can fully reconstruct the original signal. Or, if you use just a few (like 5 in this figure), it has the effect of smoothing the signal when you invert the DCT.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#option-1-passing-data-back-and-forth",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#option-1-passing-data-back-and-forth",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Option 1: Passing Data back and forth",
    "text": "Option 1: Passing Data back and forth\nSo, the fasttrackpy package gives you the option of saving DCT parameters to a csv file. I’ll load an example into R, and grab the rows for one example vowel so we can see what it looks like.\n\n\nr\n\n# Reading in the data\ndct_params &lt;- read_csv(                      \n  \"data/josef-fruehwald_speaker_param.csv\",  \n  col_types = cols()                         \n)                                            \n\n# getting the first token's id\nfirst_id &lt;- dct_params$id[1]                 \n\n# subsetting to get just\n# the first token's parameters\nfirst_df &lt;- dct_params |&gt;                    \n  filter(                                    \n    id == first_id                           \n  )                                          \n\n\n\nr\n\nTable Codefirst_df |&gt; \n  select(\n    label,\n    word,\n    param,\n    F1:F3\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(\n    columns = F1:F3\n  )\n\n\n\n\n\nlabel\nword\nparam\nF1\nF2\nF3\n\n\n\nay0\nsunlight\n0\n322.45\n1,178.19\n1,753.24\n\n\nay0\nsunlight\n1\n30.34\n−159.93\n55.47\n\n\nay0\nsunlight\n2\n−0.73\n−11.40\n23.71\n\n\nay0\nsunlight\n3\n2.25\n−15.00\n3.98\n\n\nay0\nsunlight\n4\n−5.07\n4.76\n4.59\n\n\n\n\n\n\nThese DCT parameters don’t look like much on their own. That’s even clearer if we plot them.\n\n\nr\n\nPlotting codefirst_df |&gt; \n  pivot_longer(\n    F1:F3\n  ) |&gt; \n  ggplot(\n    aes(param, value)\n  )+\n    geom_hline(\n      yintercept = 0\n    )+\n    geom_point(\n      aes(color = factor(param)),\n      size = 3\n    )+\n    guides(\n      color = \"none\"\n    )+\n    labs(\n      y = NULL\n    )+\n    facet_wrap(~name)+\n    theme(\n      aspect.ratio = 1\n    )\n\n\n\n\n\n\nFigure 2: DCT parameters for one vowel token\n\n\n\n\nTo get these values back into something interpretable, we need to apply the inverse discrete cosine transform. To do that, we can\n\npass these parameter values over to python,\napply scipy.fft.idct to them to get back formant-like values\npass the results back to R.\n\nPassing data to python\nTo do this, first I’m going to assign each set of parameters to a variable in R.\n\n\nr\n\nF1_param &lt;- first_df$F1\nF2_param &lt;- first_df$F2\nF3_param &lt;- first_df$F3\n\nHaving loaded reticulate before, any variable we’ve created in R are available in Python within an r object.\n\n\npython\n\nr.F1_param\n\n[322.4520974990528, 30.339268532723658, -0.7277856792300109, 2.25340821466954, -5.069135079372835]\n\n\nNow, we just need to import the idct function and apply it to each of these sets of parameters.\nApplying idct\n\n\npython\n\nfrom scipy.fft import idct\n\n\n\npython\n\nF1_expanded = idct(\n  r.F1_param,\n  n = 100,\n  orthogonalize = True,\n  norm = \"forward\"\n)\n\nF2_expanded = idct(\n  r.F2_param,\n  n = 100,\n  orthogonalize = True,\n  norm = \"forward\"\n)\n\nF3_expanded = idct(\n  r.F3_param,\n  n = 100,\n  orthogonalize = True,\n  norm = \"forward\"\n)\n\nPassing data back to R\nNow, we can get these expanded values back in R from an object called py.\n\n\nr\n\nhead(\n  py$F1_expanded\n)\n\n[1] 509.6159 509.6814 509.8102 509.9982 510.2390 510.5247\n\n\nI’ll pop these all into a tibble.\n\n\nr\n\nfirst_expanded &lt;- tibble(\n  F1 = py$F1_expanded,\n  F2 = py$F2_expanded,\n  F3 = py$F3_expanded\n) |&gt; \n  mutate(\n    prop_time = (row_number() - 1)/(n()-1)\n  )\n\n\n\nr\n\nPlotting codefirst_expanded |&gt; \n  pivot_longer(\n    F1:F3,\n    names_to = \"formant\",\n    values_to = \"frequency\"\n  ) |&gt; \n  ggplot(\n    aes(prop_time, frequency, color = formant)\n  )+\n    geom_textpath(\n      aes(label = formant),\n      linewidth = 1\n    )+\n    guides(\n      color = \"none\"\n    )+\n    labs(\n      x = \"proportional time\"\n    )+\n    expand_limits(y = 0)\n\n\n\n\n\n\nFigure 3: Inverse DCT formant results\n\n\n\n\nShortcomings\n\nThat was a lot of code to get back the formant-like values for just one token!\nIt’s not taking advantage of the really cool averaging properties of the DCT.\nIt didn’t fit into my nice tidyverse workflows at all!",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#option-2-using-python-functions-inside-r",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#option-2-using-python-functions-inside-r",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Option 2: Using Python functions inside R",
    "text": "Option 2: Using Python functions inside R\nRather than passing data back and forth directly, instead, I’ll import the scipy function directly into R.\n\n\nr\n\nscipy &lt;- reticulate::import(\"scipy\")\nidct &lt;- scipy$fft$idct\n\nNow, we can use idct() (almost) like an R function. Here’s how it looks on one of the variables we created before.\n\n\nr\n\nnew_F1_expanded &lt;- idct(\n  F1_param,\n  n = 100L,\n  orthogonalize = TRUE,\n  norm = \"forward\"\n)\n  \nhead(new_F1_expanded)\n\n[1] 509.6159 509.6814 509.8102 509.9982 510.2390 510.5247\n\n\nI’ll combine this with the handy-dandy tidyverse functions across and reframe to get average formant trajectories.\nStep 1: Getting the average of the DCT parameters by token.\nWithsummarise() and across(), we’ll get the mean of the parameter values for F1, F2 and F3, grouped by label and parameter.\n\n\nr\n\ndct_params |&gt; \n  summarise(\n    across(\n      F1:F3, mean\n    ),\n    .by = c(label, param)\n  )-&gt;\n  dct_averages\n\nhead(dct_averages)\n\n# A tibble: 6 × 5\n  label param      F1      F2      F3\n  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 ay0       0 338.    1130.   1659.  \n2 ay0       1  33.8   -136.     26.9 \n3 ay0       2  -5.53    -5.95    5.37\n4 ay0       3  -0.580  -11.6    -1.87\n5 ay0       4  -1.20    -2.15   -3.60\n6 ey        0 274.    1380.   1736.  \n\n\nStep 2: Apply idct to the averages\nNow, I’ll use reframe() and across() to get the formant-like values from these averages\n\n\nr\n\ndct_averages |&gt; \n  reframe(\n    across(\n      F1:F3,\n      ~idct(\n        .x, \n        n = 100L,\n        orthogonalize = T, \n        norm = \"forward\"\n      )\n    ),\n    .by = label\n  ) |&gt; \n  mutate(\n    prop_time = (row_number()-1)/(n()-1),\n    .by = label\n  )-&gt;\n  average_smooths\n\nhead(average_smooths)\n\n# A tibble: 6 × 5\n  label        F1        F2        F3 prop_time\n  &lt;chr&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt;     &lt;dbl&gt;\n1 ay0        532.     1287.     2400.    0     \n2 ay0        532.     1287.     2400.    0.0101\n3 ay0        532.     1288.     2400.    0.0202\n4 ay0        532.     1290.     2400.    0.0303\n5 ay0        532.     1292.     2401.    0.0404\n6 ay0        532.     1295.     2401.    0.0505\n\n\nStep 3: Make some good plots\nHere’s a plot of the expanded formant trajectories for some of the more dynamic vowels.\n\n\nr\n\nPlotting codeaverage_smooths |&gt; \n  filter(\n    label %in% c(\n      \"iy\",\n      \"ey\",\n      \"ay\",\n      \"ay0\",\n      \"aw\",\n      \"Tuw\",\n      \"owL\"\n    )\n  ) |&gt; \n  ggplot(\n    aes(F2, F1, color = label)\n  )+\n    geom_textpath(\n      aes(\n        group = label,\n        label = label\n      ),\n      arrow = arrow(\n        type = \"closed\",\n        length = unit(0.2, \"cm\")\n      ),\n      linewidth = 1\n    )+\n    scale_y_reverse()+\n    scale_x_reverse()+\n    guides(\n      color = \"none\"\n    )+\n    theme(aspect.ratio = 1)\n\n\n\n\n\n\nFigure 4: Average formant trajectories",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#a-note",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#a-note",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "A note",
    "text": "A note\nUsually when you see a plot like Figure 4, it’s the result of some fairly complicated model fitting. But take a look through the source code here! Not a gam in sight! Just averaging, and the application of the idct!",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/07/2024-07-19_dct-r/index.html#footnotes",
    "href": "posts/2024/07/2024-07-19_dct-r/index.html#footnotes",
    "title": "Working with the Discrete Cosine Transform in R",
    "section": "Footnotes",
    "text": "Footnotes\n\npolars is growing on me, but I don’t think in it yet.↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "07",
      "Working with the Discrete Cosine Transform in R"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "",
    "text": "I’m currently teaching a computational linguistics course, and for just the second time, I’m using Github Classroom to manage assignments. I might make a few posts about what I’ve learned in the process, but for now, I think I’ll focus on what’s gone right, and what’s been more challenging in using Github Codespaces.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html#github-codespaces",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html#github-codespaces",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "Github Codespaces",
    "text": "Github Codespaces\nA Github Codespace is basically a virtual machine hosted by Github with VS Code as the default interface. First of all, since running a codespace involves data storage and running computation, they usually cost extra money, but you can get an educational discount to $0, as described here.\nWhy Codespaces?\nThe number 1 reason to use some kind of cloud computing environment for teaching a computational course is to avoid the development environment paradox I’ve experienced:\n\n\n\n\n\n\nNoteThe development environment paradox\n\n\n\nIf you are teaching a computational course with \\(N\\) students using their own computers, the number of of development environments to debug is \\(&gt;N\\).\n\n\nWhen I previously taught an NLP course, we used replit, but I increasingly felt the goals of the platform were inappropriate for an educational context (e.g. they have “bounties” with dollar amounts attached to them), and they’re sunsetting their educational program anyway.\nWith Codespaces, I’m still getting the benefits of having full control over the students’ development environment, but the way they interact with it will be very similar to what it’s like to interact with programming in a local environment.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html#how-ive-customized-the-codespaces",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html#how-ive-customized-the-codespaces",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "How I’ve customized the Codespaces",
    "text": "How I’ve customized the Codespaces\nWhen you launch a codespace, the instructions for how it should be configured are located in .devcontainer/devcontainer.json. You can check out how I’ve customized my assignment template repository here. I’ve left most of the configuration options at their default values, but here’s what I’ve customized.\nThe postCreateCommand\n\nAfter initially setting up the codespace, you can have it run shell commands, like installing any python dependencies. I wound up wanting it to do so many things that I’ve moved them into a separate config.sh, which gets run like this\n\njson\n\n{\n \"postCreateCommand\": \"sh .devcontainer/config.sh\"\n}\nInstalling python requirements\nFirst, I need the codespace install any python requirements for the assignment:\n\nsh\n\necho \"Installing Developer Requirements\"\npip3 install --user -r requirements/requirements.txt\nConfigure merge commits\nWe ran into a problem where, if I made some changes to which tests should run for the autograder, it resulted in a divergent history between the remote git repo and the student’s local repo, which VS Code does not present in the most user friendly way.\nSo, in my post-creation config file, I’ve added merge commit configuration\n\nsh\n\necho \"Merge Commits Only\" \ngit config pull.rebase false\nA pre-commit hook\nWe also ran into a problem where I didn’t always remember to run pip freeze after writing an assignment. When students tried to run the code that I said would just work, it didn’t. While learning “module not found” means you should just run pip install is a valuable lesson, I decided to add a pre-commit git hook to run pip freeze and add it to the commit. This also has the benefit that if students install a package and use it, tests should still all pass.\nI saved this hook to .devcontainer/pre-commit:\n\nsh\n\n#!/bin/sh\npip freeze &gt; requirements/requirements.txt\ngit add requirements/requirements.txt\nAnd added the following to .devcontainer/config.sh\n\nsh\n\necho \"pip freeze pre commit\"\nln .devcontainer/pre-commit .git/hooks/\nchmod +x .git/hooks/pre-commit\nTesting\nGithub Classroms lets you write unit tests that will “autograde” submissions. I wanted students to be able to run these tests locally, so they could tell where things weren’t going right. So I added the following VC Code customizations to devcontainer.json.\n\njson\n\n{\n  \"customizations\": {\n        \"vscode\": {\n            \"settings\": {\"python.testing.pytestArgs\": [\n                \"tests\"\n            ],\n            \"python.testing.unittestEnabled\": false,\n            \"python.testing.pytestEnabled\": true}\n        }\n    }\n}\nThis sets up the testing pane in their VS Code automatically.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-14_gh-classroom/index.html#shouldnt-debugging-be-part-of-the-learning-process",
    "href": "posts/2024/02/2024-02-14_gh-classroom/index.html#shouldnt-debugging-be-part-of-the-learning-process",
    "title": "Some lessons in using Github Classroom and Codespaces",
    "section": "Shouldn’t debugging be part of the learning process?",
    "text": "Shouldn’t debugging be part of the learning process?\nMost of these customizations are me trying to foresee and forestall buggy problems students are going to run into. It’s worth asking whether that’s counterproductive. Maybe students should run into these problems and figure out how to fix them, since that’s more realistic to what it’s like to do computational work.\nTo that, I’d say maybe, depending on the level of the course. But if both programming and the course content are all new to many students in the class, I think they just need to experience things working first. When first starting out, you’re faced with a real problem of how to attribute errors. If you click the git Sync button in VS Code and this error pops up:\n\n\n\n\nYou have no idea whether:\n\nYou clicked the wrong button.\nYou’re not supposed to click any button.\nYou actually need to click the button twice.\nThe code you wrote was wrong.\nThe internet is temporarily down.\nYour computer is broken.\nYou need to install an additional program.\nYou’re too stupid to take this course.\nIt actually worked and the error message was an error.\n…\n\nWithout a baseline framework for understanding how things are working, you can’t process and accommodate error messages like this, so, honestly, better for me, as the instructor, to try to make things just work, and present errors and bugs in a planned and structured way.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Some lessons in using Github Classroom and Codespaces"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html",
    "title": "Getting a sense of priors for logistic regression",
    "section": "",
    "text": "This summer, I’ve been spending some time getting more familiar with the nitty-gritty of Bayesian models, and I’m working on some modelling in Stan for logistic regression. Setting up the basic model formula in Stan for logistic regression is pretty straightforward:\nAnd then you “just” need to set a prior for for the intercept and slope terms. But I hadn’t thought about how my impulses for setting priors for ordinary linear models could be pathological for logistic regression!\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(gt)\nsource(here::here(\"_defaults.R\"))",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#simulating-some-intercepts",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#simulating-some-intercepts",
    "title": "Getting a sense of priors for logistic regression",
    "section": "Simulating some intercepts",
    "text": "Simulating some intercepts\nLet’s start by assuming that the intercept value should be about 0, which corresponds to a probability of 0.5.\n\nzero_intercept &lt;- 0\n\n## inverse logit\nplogis(zero_intercept)\n\n[1] 0.5\n\n\nSo, I’ll normal distribution with a mean of 0, but what should its standard deviation be? My impulse from gaussian models is that I don’t want to set the standard deviation too small, cause then I’d be using an overly informative prior. Let’s simulate some intercepts from normal distributions with sds of 1, 3, and 5.\n\ntibble(\n  sd = c(1, 3, 5)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    intercepts = list(tibble(\n      intercept = rnorm(\n        1e6, \n        mean = 0, \n        sd = sd)\n    ))\n  ) |&gt; \n  unnest(intercepts)-&gt;\n  intercept_samples\n\nAnd we can visualize these samples from the prior like so.\n\nintercept_samples |&gt; \n  ggplot(\n    aes(\n      intercept, \n      fill = factor(sd)\n    )\n  )+\n    stat_slab(normalize = \"panels\")+\n    facet_wrap(~sd)+\n    theme_no_y()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\n\nFigure 1: Samples from normal priors.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#how-it-looks-in-the-probability-space",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#how-it-looks-in-the-probability-space",
    "title": "Getting a sense of priors for logistic regression",
    "section": "How it looks in the probability space",
    "text": "How it looks in the probability space\nSo, maybe we should go with the ~normal(0,5) prior, so as to not to be overly informative. Real quick, though, let’s plot these distributions in the probablity space.\n\nintercept_samples |&gt; \n  ggplot(\n    aes(\n      plogis(intercept), \n      fill = factor(sd)\n    )\n  )+\n    stat_slab(normalize = \"panels\")+\n    facet_wrap(~sd)+\n    theme_no_y()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\n\nFigure 2: Inverse logit samples from normal priors.\n\n\n\n\nWait! My uninformative ~normal(0,5) prior is suddenly looking pretty informative. The probability mass for the priors with larger standard deviations is ammassed closer to 0 and 1, rather than mostly toward the middle at 0.5! If we just roughly look at roughly how much probability mass each prior puts below 0.1 versus the middle 0.1, it’s pretty clear.\n\nCodeintercept_samples |&gt; \n  mutate(\n    prob_intercept = plogis(intercept)\n  ) |&gt; \n  group_by(sd) |&gt; \n  summarise(\n    under_5 = mean(\n      prob_intercept &lt; 0.1\n    ),\n    middling = mean(\n      prob_intercept &gt; 0.45 &\n        prob_intercept &lt; 0.55\n    )\n  ) |&gt; \n  gt() |&gt; \n    fmt_number(\n      decimals = 3, \n      columns = 2:3\n    ) |&gt; \n    cols_label(\n      under_5=\"bottom 0.1\",\n      middling = \"middle 0.1\"\n      \n    )\n\n\n\n\n\nsd\nbottom 0.1\nmiddle 0.1\n\n\n\n1\n0.014\n0.159\n\n\n3\n0.231\n0.053\n\n\n5\n0.330\n0.032",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#the-upshot",
    "href": "posts/2023/06/2023-06-19_getting-a-sense-of-priors/index.html#the-upshot",
    "title": "Getting a sense of priors for logistic regression",
    "section": "The Upshot",
    "text": "The Upshot\nThe reason the symmetric and spread out distributions in the logit space turn into sharp bimodal distributions in the probability space is because the (inverse) logit transform is non linear. Basically every value below about -4.5 is going to get squished to 0, and basically every value above 4.5 is going to get squished to 1. If I want the probability distribution over the intercept to be roughly unimodal in the probability space, then the standard deviation in the logit space should be relatively small to avoid extreme values!\nIf we take 0.01 and 0.99 as about most extreme values, in the probability space, that the intercept could be, we can convert that to logit, and divide by 3 to get a good standard deviation for the prior, since almost all data falls within 3sds of the mean.\n\nqlogis(0.99)/3\n\n[1] 1.531707\n\n\nThe prior I had just above, with sd=1, has a slightly narrower range in the probability space.\n\nplogis(1*3)\n\n[1] 0.9525741\n\n\n\nCodetibble(\n  sd = c(1, 1.5)\n) |&gt; \n  rowwise() |&gt; \n  mutate(\n    intercepts = list(tibble(\n      intercept = rnorm(\n        1e6, \n        mean = 0, \n        sd = sd)\n    ))\n  ) |&gt; \n  unnest(intercepts)-&gt;\n  new_intercept_samples\n\n\nThe ~normal(0, 1.5) prior looks almost like a uniform distribution in the probability space.\n\nCodenew_intercept_samples |&gt; \n  ggplot(\n    aes(\n      plogis(intercept), \n      fill = factor(sd)\n    )\n  )+\n    stat_slab(normalize = \"panels\")+\n    facet_wrap(~sd)+\n    theme_no_y()+\n    theme(\n      legend.position = \"none\"\n    )\n\n\n\n\n\n\nFigure 3: Inverse logit samples from normal priors.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Getting a sense of priors for logistic regression"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html",
    "title": "Using Quarto Blogs for Teaching",
    "section": "",
    "text": "This semester I was teaching two R intensive courses: Quantitative Investigations in the Social Sciences and Quantitative Methods in Linguistics. For both courses, I had students maintain Quarto blogs that they pushed to Github Classroom and I thought I’d write up my thoughts on what worked, what didn’t work, and what I wish I’d planned out in advance.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#github-classroom-basics",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#github-classroom-basics",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Github Classroom basics",
    "text": "Github Classroom basics\nFirstly, I spent relatively little time on the actual Github Classroom site. There may be more functionality to it that’s useful, but largely I didn’t need it. Each “Classroom” creates a Github Organization from which you can access every student’s assignment repository.\nCreating assignments\nTo create an assignment, you first have to set up a template repository on Github (could be under your own account or under the classroom organization) that gets associated with the assignment. Github Classroom generates an invite link that you can post to Canvas/Blackboard/whatever that will auto-fork the template for the student in the organization, and name it {name-of-assignment}-{gh-username}.\n\n\n\n\n\n\nNoteRepository Visibility\n\n\n\nThese repositories are, by default, private! In the example repo I show here, I went in and manually changed it to be public\n\n\nFor example, here’s the repository it created when I clicked the invite link to the mapping project:\nhttps://github.com/A-S500/maps-final-JoFrhwld\nThe repository isn’t under my personal account, it’s under the A-S500 organization.\nTip 1: Take your time on the templates\nFor a blog-type assignment that students will be updating across the entire semester, take your time on the template. Get your template blog to be exactly like you want it. Obviously, students can make changes to how the blog will render, but your initial settings will have a major inertia effect.\nHere’s one thing I spent time on in set up that I’ll do differently in the future: I created blank template posts for every post students were going to do that year. For one class, I was having them write weekly reflections, so I created a directory and index.qmd for every week. e.g.\nposts/01_week/index.qmd\nposts/02_week/index.qmd\n...\nposts/15_week/index.qmd\nFor the the other course, I had them write a post for each chapter of the textbook, and the setup was similar.\nI’d do that differently now for a few reasons. First, when looking at the source code and the rendered blog, it makes it harder to eyeball where the students have created new content. It might not seem like a lot, but it’s just a little extra bit of friction to click through to an index.qmd post to find that it’s still just the original template.\nSecond, it facilitated some poor metadata practices for the students. I really couldn’t get everyone to touch the yaml header to update the date of the post if there was already a date: there, so for some people all of their posts were dated January, when I created the template.\nThoughts: The feedback pull request isn’t worth it.\nGithub Classroom will give you the option of automatically creating a feedback branch and open a pull request to leave comments on the code. While this makes sense for some assignments, these quarto blogs with their rendered html are too unwieldy to bother.\nI just opened feedback issues on the blogs referencing the specific lines I was commenting on.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#quarto-things",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#quarto-things",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Quarto Things",
    "text": "Quarto Things\nSome quarto things that I’d recommend:\nTip 2: Set up freeze: auto\n\nTo save yourself and your students’ time be sure to configure freezing in the _quarto.yml file.\nexecute: \n  freeze: auto\nThis means quarto will save the results of any executed R code, and won’t rerun the code when rendering the project unless the content of the file changes. This will save you and your students a lot of time when re-rendering blogs, especially later in the semester.\n\n\n\n\n\n\nImportantcommit `_freeze/`\n\n\n\nBe sure to add and commit the _freeze/ directory though!\n\n\nTip 3: Encourage (enforce!) actual rendering of the blog.\nOne of the positives of any code notebook system in general is that they look nice while you’re writing, and interleave prose, code, & results. A downside for a Quarto blog, though, is that students may not realize that the code results they see in the notebook don’t get saved and committed. You (the reader/grader) will have to re-run all of the code chunks, or re-render the html blog.\nBut, as long as students click the “Render Project” button, the generated html pages will be viewable in _site, with all of the prose, code and code results… if they click “Render Project”.\nI didn’t pick up on the fact many students were just running the code in the notebook and never clicking “Render” until too late in the semester, and by that point we were very busy trying to get through the course content for me to turn that ship. That meant I had to re-render their blogs (takes time) and I had to deal with/fix any inconsistencies between our R environments (more time!)\nTip 4: Set error: true in the quarto project\nI think you should set error: true in _quarto.yml. Usually, if there’s an error in your R code, the project will fail to render. With error: true, the project will render, with the R errors included in the output.\nMaybe you’re thinking “But don’t you want to make sure that students fix the errors?” Well, yes I do, but if there’s errors in their code and they push it anyway, when you try to render the project, you’ll have to hunt down and fix the R errors before you can look at the rendered project!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#rrstudio-things",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#rrstudio-things",
    "title": "Using Quarto Blogs for Teaching",
    "section": "R/RStudio Things",
    "text": "R/RStudio Things\nTip 5: Teach them about {here} early\nSo, no matter how many times I left comments about “please use relative paths”, I would still get posts with code that looked like\n\ndata &lt;- read_csv(\"~/usernamene/Documents/Courses/blog/data/data.csv\")\n\nWhen combined with the fact that many people weren’t clicking “Render” on their projects, this meant I had to go in and fix these global paths in order to read their posts.\nThe here package works within RStudio projects to help construct reproducible paths. here::here() returns the path to the main project directory, and will concatenate any further arguments to that path. So rewriting the example above would look like:\n\nlibrary(here)\ndata &lt;- read_csv(here(\"data\", \"data.csv\"))\n\nOn the one hand, using here in this way does bypass needing to come to grips with paths, which is a crucial component of scientific computing. But on the other, you need to make a decision about where your class time is spent.\nIf you go all in on *nix-style paths and their navigation, you will burn a lot of class time and your own creative energy on a topic that is increasingly conceptually difficult for students before you even get to course content. Moreover, students won’t neatly delimit course content like “paths are utility background information, stats are a collection of theories and methods, R code is an implementation of those theories and methods.” Rather, it’ll all get dumped into one big bucket of “stats is hard.”\nOr, you could teach them to use here.\nTip 6: Figure out {renv}.\nThis is more of a “to-do”. I’m relatively new to using renv and am still trying to work out the kinks of making it work in distributed assignments like this.\nI think step one would be to make sure they’ve installed renv globally, before opening any given project. Then, in your template, commit your renv.lock file, but not the .Rprofile. Then, once they’ve created the project in RStudio, run renv::init() once, restore from the lock file.\nI think…\nTip 7: Remind them to save!\nRemind them that this is not like Google Docs or Word! The document is not auto-saving as you go along! Save save save!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#gitgrading-things",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#gitgrading-things",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Git/Grading Things",
    "text": "Git/Grading Things\nOn the reading/grading side of things, I worked out a git workflow that worked pretty well.\nTip 8: Add the student blogs as git submodules\nI originally had a grand idea of adding every student’s blog to one larger quarto project I would render, and look at all of their posts in one place, but that didn’t work out great. Instead, I created a bare bones RStudio project, and added each student’s blog repository as a submodule in a `blogs/` directory:\ngit submodule add git@github.com:...\nAfter initial setup, this meant I could pull all student’s new posts with\ngit submodule update --remote\nTip 9: Make use of RStudio’s project navigation.\nThe way I’d read and grade the blog posts was by navigating to a student’s blog repository in RStudio’s file browser, then clicking on the blog.Rproj file. This will auto open the student’s blog as an RStudio project which you can render and browse in isolation from all other students’ projects (especially if you’ve initialized renv inside).\nTip 10: Always commit all changes, but don’t push, after grading\n\n\n\n\n\n\nImportantCommit Changes!\n\n\n\nI’m saying the same thing twice because it’s important.\n\n\nAfter re-rendering and reading the student’s blog, be sure to commit all changes, but don’t push them. This is because of a detail of how both quarto and git submodules work.\nEvery time you re-render a project, the “last modified” metadata in the rendered html files gets updated. Meaning even if you just changed one page, all of the html pages get modified.\nAdditionally, if there are uncommitted changes within the git submodule, when you run git submodule update --remote, git won’t pull down the new commits until you’ve committed those changes within then submodule.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#final-thoughts",
    "href": "posts/2023/05/2023-05-03_teaching-quarto-blogs/index.html#final-thoughts",
    "title": "Using Quarto Blogs for Teaching",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOverall, I was pretty happy with the outcomes! The blog format worked well for incremental logging of course progress, and it started socializing students into the practices of collaborative coding projects.\nI should note that both classes were relatively small, roughly seminar size, so I’m not sure how this would scale up to 25+ students.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "05",
      "Using Quarto Blogs for Teaching"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-05/index.html",
    "href": "posts/2023/02/2023-02-05/index.html",
    "title": "A handy dplyr function for linguistics",
    "section": "",
    "text": "One of the new functions in dplyr v1.1.0 is dplyr::consecutive_id(), which strikes me as having a few good use cases for linguistic data. The one I’ll illustrate here is for processing transcriptions.\nlibrary(tidyverse)\nlibrary(gt)\n\nsource(here::here(\"_defaults.R\"))\n\n# make sure its &gt;= v1.1.0\npackageVersion(\"dplyr\")\n\n[1] '1.1.2'\nI’ll use a sample transcription extract from LANCS, where the audio has been chunked into “breath groups” and transcribed, along with an identifier of who was speaking, and beginning and end times.\ntranscription &lt;- \n  read_csv(\"data/KY25A_1.csv\")\nspeaker\nstart\nend\ntranscript\n\n\n\nIVR\n192.110\n194.710\nwell uh, I have a number of uh\n\n\nIVR\n195.530\n198.620\nthings I'd like to ask you about. I wonder if you'd just mind uh.\n\n\nIVR\n199.110\n200.900\nanswering questions uh\n\n\nIVR\n202.130\n203.610\none after another if you\n\n\nKY25A\n203.295\n204.405\nyeah\n\n\nKY25A\n204.745\n205.225\nwell\n\n\nIVR\n204.740\n205.805\nif I remind you of a\n\n\nKY25A\n205.510\n207.930\nnow you might start that\n\n\nKY25A\n208.440\n209.570\nI was born in\n\n\nKY25A\n210.420\n212.120\neighteen sixty seven\n\n\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n\n\nKY25A\n215.780\n216.600\nninety three\n\n\nIVR\n216.665\n217.455\nninety three\nOne thing we might want to do is indicate which sequences of transcription chunks belong to one speaker, corresponding roughly to their speaking turns. I’ve hacked my way through this kind of coding before, but now we can easily add turn numbers with dplyr::consecutive_id(), which will add a column of numbers that increment every time the value in the indicated column changes.\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  )\nspeaker\nstart\nend\ntranscript\nturn\n\n\n\nIVR\n192.110\n194.710\nwell uh, I have a number of uh\n1\n\n\nIVR\n195.530\n198.620\nthings I'd like to ask you about. I wonder if you'd just mind uh.\n1\n\n\nIVR\n199.110\n200.900\nanswering questions uh\n1\n\n\nIVR\n202.130\n203.610\none after another if you\n1\n\n\nKY25A\n203.295\n204.405\nyeah\n2\n\n\nKY25A\n204.745\n205.225\nwell\n2\n\n\nIVR\n204.740\n205.805\nif I remind you of a\n3\n\n\nKY25A\n205.510\n207.930\nnow you might start that\n4\n\n\nKY25A\n208.440\n209.570\nI was born in\n4\n\n\nKY25A\n210.420\n212.120\neighteen sixty seven\n4\n\n\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n5\n\n\nKY25A\n215.780\n216.600\nninety three\n6\n\n\nIVR\n216.665\n217.455\nninety three\n7\nNow we can do things like group the data by turn, and get a new dataframe summarized by turn.\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  ) |&gt; \n  summarise(\n    .by = c(turn, speaker),\n    start = min(start),\n    end = max(end),\n    transcript = str_c(transcript, collapse = \" \"),\n  )\nturn\nspeaker\nstart\nend\ntranscript\n\n\n\n1\nIVR\n192.110\n203.610\nwell uh, I have a number of uh things I'd like to ask you about. I wonder if you'd just mind uh. answering questions uh one after another if you\n\n\n2\nKY25A\n203.295\n205.225\nyeah well\n\n\n3\nIVR\n204.740\n205.805\nif I remind you of a\n\n\n4\nKY25A\n205.510\n212.120\nnow you might start that I was born in eighteen sixty seven\n\n\n5\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n\n\n6\nKY25A\n215.780\n216.600\nninety three\n\n\n7\nIVR\n216.665\n217.455\nninety three\nAnd then you can start moving onto other analyses, like what the lag was between one speaker’s end and the next’s beginning.\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  ) |&gt; \n  summarise(\n    .by = c(turn, speaker),\n    start = min(start),\n    end = max(end),\n    transcript = str_c(transcript, collapse = \" \"),\n  ) |&gt; \n  mutate(overlapping = start &lt; lag(end))\nturn\nspeaker\nstart\nend\ntranscript\nlag\n\n\n\n1\nIVR\n192.110\n203.610\nwell uh, I have a number of uh things I'd like to ask you about. I wonder if you'd just mind uh. answering questions uh one after another if you\nNA\n\n\n2\nKY25A\n203.295\n205.225\nyeah well\n-0.315\n\n\n3\nIVR\n204.740\n205.805\nif I remind you of a\n-0.485\n\n\n4\nKY25A\n205.510\n212.120\nnow you might start that I was born in eighteen sixty seven\n-0.295\n\n\n5\nIVR\n213.350\n215.450\nmhm and that makes you how old?\n1.230\n\n\n6\nKY25A\n215.780\n216.600\nninety three\n0.330\n\n\n7\nIVR\n216.665\n217.455\nninety three\n0.065\nThis was just the first example that came to mind, but there’s probably a lot of data processing tasks that can be made a lot less annoying with dplyr::consecutive_id().",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "A handy dplyr function for linguistics"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-05/index.html#extra",
    "href": "posts/2023/02/2023-02-05/index.html#extra",
    "title": "A handy dplyr function for linguistics",
    "section": "Extra",
    "text": "Extra\nI’ll throw the duration of within-turn pauses in there.\n\nlibrary(glue)\n\n\ntranscription |&gt; \n  mutate(\n    turn = consecutive_id(speaker)\n  ) |&gt; \n  mutate(\n    .by = turn,\n    pause_dur = start - lag(end),\n    transcript = case_when(\n      .default = transcript,\n      is.finite(pause_dur) ~ glue(\n        \"&lt;{round(pause_dur, digits = 2)} second pause&gt; {transcript}\"\n      )\n    )\n  ) |&gt; \n  summarise(\n    .by = c(turn, speaker),\n    start = min(start),\n    end = max(end),\n    transcript = str_c(transcript, collapse = \" \"),\n  ) |&gt; \n  mutate(lag = start - lag(end)) |&gt; \n  relocate(lag,  .before = start)\n\n\n\n\n\n\n\nturn\nspeaker\nlag\nstart\nend\ntranscript\n\n\n\n1\nIVR\nNA\n192.110\n203.610\nwell uh, I have a number of uh &lt;0.82 second pause&gt; things I'd like to ask you about. I wonder if you'd just mind uh. &lt;0.49 second pause&gt; answering questions uh &lt;1.23 second pause&gt; one after another if you\n\n\n2\nKY25A\n-0.315\n203.295\n205.225\nyeah &lt;0.34 second pause&gt; well\n\n\n3\nIVR\n-0.485\n204.740\n205.805\nif I remind you of a\n\n\n4\nKY25A\n-0.295\n205.510\n212.120\nnow you might start that &lt;0.51 second pause&gt; I was born in &lt;0.85 second pause&gt; eighteen sixty seven\n\n\n5\nIVR\n1.230\n213.350\n215.450\nmhm and that makes you how old?\n\n\n6\nKY25A\n0.330\n215.780\n216.600\nninety three\n\n\n7\nIVR\n0.065\n216.665\n217.455\nninety three",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "A handy dplyr function for linguistics"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html",
    "title": "R Package Exploration (Jan 2023)",
    "section": "",
    "text": "As I scroll through my feeds, I often come across a really cool looking package, or a new feature of a package, that I think looks really cool, and then I forget to go back to really kick the tires to see how it works. So I’ve decided to try to set up a workflow where I send the docs or pkgdown pages for the package to a Trello board, and then come back maybe once a month and experiment with them in a blog post.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-ggdensity-and-geomtextpath",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-ggdensity-and-geomtextpath",
    "title": "R Package Exploration (Jan 2023)",
    "section": "\n{ggforce}, {ggdensity} and {geomtextpath}\n",
    "text": "{ggforce}, {ggdensity} and {geomtextpath}\n\nThe packages I want to mess around with today are all extensions to ggplot2, so I’ll load up the palmerpenguins dataset for experimentation.\n\n## setup\nlibrary(tidyverse)\nlibrary(khroma)\nlibrary(palmerpenguins)\n\n## exploration packages\nlibrary(ggforce)\nlibrary(ggdensity)\nlibrary(geomtextpath)",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-and-convex-hulls",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggforce-and-convex-hulls",
    "title": "R Package Exploration (Jan 2023)",
    "section": "\n{ggforce} and convex hulls",
    "text": "{ggforce} and convex hulls\nThe ggforce package as the option to add a convex hull over your data (ggforce::geom_mark_hull()), kind of indicating where the data clusters are. Here’s my base plot.\n\nplot1 &lt;- \n  penguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(bill_length_mm, bill_depth_mm, color = species))+\n    geom_point()+\n    scale_color_brewer(palette = \"Dark2\")+\n    scale_fill_brewer(palette = \"Dark2\")\nplot1\n\n\n\n\n\n\nFigure 1: The base penguins scatterplot\n\n\n\n\nI’ll throw on the default convex hull.\n\nplot1 +\n  geom_mark_hull()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\nFigure 2: Basic convex hull\n\n\n\n\nDefault is ok, but for this data set, the hulls are a bit jagged. That can be adjusted with the concavity argument. I’ll also throw in a fill color.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(\n      fill = species\n    )\n  )\n\n\n\n\n\n\nFigure 3: Smoothed out and filled convex hulls\n\n\n\n\nThat’s better. It also comes with a mappable label and description aesthetics. Here, it seems a bit more touchy.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(fill = species,\n        label = species,\n    ),\n    label.family = \"Fira Sans\"\n  )\n\n\n\n\n\n\nFigure 4: Attempted labelling of convex hulls\n\n\n\n\nThe labels actually appear in the RStudio IDE for me, but not in the rendered page here because it wants more headroom around the plot. I’ll add that in by setting the expand arguments to ggplot::scale_y_continuous() and ggplot::scale_x_continuous(), and I’ll drop the legend while I’m at it.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(fill = species,\n        label = species,\n    ),\n    label.family = \"Fira Sans\"\n  )+\n  scale_y_continuous(\n    expand = expansion(\n      mult = c(0.25, 0.25)\n    )\n  )+\n  scale_x_continuous(\n    expand = expansion(\n      mult = c(0.25, 0.25)\n    )\n  ) +\n  guides(\n    color = \"none\",\n    fill = \"none\"\n  )\n\n\n\n\n\n\nFigure 5: Labelled convex hulls\n\n\n\n\nThoughts\nI like the convex hulls as a presentational aide. It probably shouldn’t be taken as a statistical statement about, for example the degree of overlap between these three species, but is useful for outlining data points of interest.\nI kind of wish this was separated out into a few different, more conventional, ggplot2 layers. It’s called a geom_ but the convex hulls are definitely stat_s. The convex hull statistic layer isn’t exposed to users, so you can’t mix-and-match convex hull estimation and the geom used to draw it. On the other hand, I can see that it’s much more souped up than a typical geom. For example, you can filter the data within the aes() mapping.\n\nplot1 +\n  geom_mark_hull(\n    concavity = 5,\n    aes(\n      filter = sex == \"female\"\n    )\n  )\n\n\n\n\n\n\nFigure 6: Filtered convex hulls",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggdensity",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#ggdensity",
    "title": "R Package Exploration (Jan 2023)",
    "section": "{ggdensity}",
    "text": "{ggdensity}\nAs pointed out on the ggdensity readme, there’s already a stat+geom in ggplot2 to visualize 2d density plots.\n\nplot2 &lt;- \n  penguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(bill_length_mm, bill_depth_mm))\n\nplot2 +\n  stat_density_2d_filled()\n\n\n\n\n\n\nFigure 7: Density contour plot\n\n\n\n\nThose levels are a little hard to follow, though, which is what ggdensity::stat_hdr() is for. It will plot polygons/contours for given probability levels, of the data distribution\n\nplot2 +\n  stat_hdr()\n\n\n\n\n\n\nFigure 8: Highest density region contour plot\n\n\n\n\nThe probabilities are mapped to transparency by default, so you can map the fill color to a different dimension.\n\nplot2 +\n  stat_hdr(aes(fill = species))+\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\nFigure 9: Highest density region contour plot, filled by species\n\n\n\n\nThe package also has a ggdensity::stat_hdr_rug() to add density distribution rugs to plots.\n\nplot2 +\n  geom_point()+\n  stat_hdr_rug(fill = \"grey90\")\n\n\n\n\n\n\nFigure 10: HDR rug",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#geomtextpath",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#geomtextpath",
    "title": "R Package Exploration (Jan 2023)",
    "section": "{geomtextpath}",
    "text": "{geomtextpath}\nI’ve actually been messing around with this for a bit, but geomtextpath allows you to place text along lines. There’s standalone geom_textpath() and geom_labelpath() functions, but just to stick with the penguins data, I’m going to match the textpath geom with a different stat.\n\nplot3 &lt;-\n  penguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(bill_length_mm, bill_depth_mm, color = species))+\n    scale_color_brewer(palette = \"Dark2\")\n\nplot3 +\n  stat_smooth(\n    geom = \"textpath\", \n    # you have to map a label aesthetic\n    aes(label = species),\n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\nFigure 11: Trendlines with text written along them\n\n\n\n\nYou can move the location of the text on the path back and forth by either setting or mapping hjust to a number between 0 and 1, and you can lift the text off the line with vjust.\n\nplot3 +\n  stat_smooth(\n    geom = \"textpath\", \n    # you have to map a label aesthetic\n    aes(label = species),\n    hjust = 0.1,\n    vjust = -1\n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\nFigure 12: Trendlines with text written along them\n\n\n\n\nMixing and matching statistics and these direct labels could get pretty powerful. For example, here’s the name of each species written around data ellipses.\n\nplot3 +\n  stat_ellipse(\n    geom = \"textpath\", \n    # you have to map a label aesthetic\n    aes(label = species),\n    hjust = 0.1  \n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\nFigure 13: Data ellipses text written along them",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-27_jan-rpackages/index.html#combo-ggdensity-and-geomtextpath",
    "href": "posts/2023/01/2023-01-27_jan-rpackages/index.html#combo-ggdensity-and-geomtextpath",
    "title": "R Package Exploration (Jan 2023)",
    "section": "Combo {ggdensity} and {geomtextpath}\n",
    "text": "Combo {ggdensity} and {geomtextpath}\n\nSince the ggdensity statistics are ordinary stat_, we can also combine them with textpaths to label the probability levels directly.\n\nplot2 +\n  stat_hdr_lines(\n    aes(label = after_stat(probs)),\n    color = \"grey90\",\n    geom = \"textpath\"\n  ) +\n  guides(alpha = \"none\")\n\n\n\n\n\n\nFigure 14: Higest density region plot with direct labels",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "R Package Exploration (Jan 2023)"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html",
    "title": "Rising Wooders: Part 3",
    "section": "",
    "text": "This is part 3 of my blog posts to accompany my ADS2023 Poster.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#waughter-worder-wooder",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#waughter-worder-wooder",
    "title": "Rising Wooders: Part 3",
    "section": "Waughter? Worder? Wooder?",
    "text": "Waughter? Worder? Wooder?\nIn my first post, I talked about the timeline of when Philadelphians came to realize there was something distinctive about the way we say water, and the use of &lt;wooder&gt; in print. In my second post, I modelled the timeline of when Philadelphians started saying [wʊɾɚ], which seems to have been in variation with [wɔɾɚ] and [wɔɹɾɚ] for people born across the 20th century. Now there’s just the question of why any of this happened at all.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#story-1-wɔɾɚ-raising",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#story-1-wɔɾɚ-raising",
    "title": "Rising Wooders: Part 3",
    "section": "Story 1: [wɔɾɚ] raising",
    "text": "Story 1: [wɔɾɚ] raising\nOne possibility is that changes in the pronunciation of /ɔ/ just naturally shifted the vowel’s pronunciation in water in particular. If you look at the &lt;ɔ&gt; symbol on an IPA chart, you’d probably describe it as a mid to low back vowel. However, /ɔ/ is pretty significantly raised in Philly so that for many speakers its more like a high back vowel.\n\n\n\n\n\n\n\n\n\n(a) “canonical” ɔ placement\n\n\n\n\n\n\n\n\n\n(b) Philly ɔ placement\n\n\n\n\n\n\nFigure 1: The “canonical” placement of /ɔ/ vs a typical Philadelphia /ɔ/\n\n\nIn fact, this is essentially the story we told in 100 Years of Sound Change. (“/oh/” and “open-o” are how we referred to this vowel).\n\nThe parallel back vowel is /oh/ in long open-o words talk, lost, off, and so on. Here the social stereotype is firmly fixed on one word, water, pronounced with a high back nucleus.\nLabov, Rosenfelder, and Fruehwald (2013)\n\nIn a lot of ways this story makes sense, since not only is /ɔ/ very high in Philadelphia, but with it coming immediately after a /w/, the coarticulatory effect could pull it even higher to [ʊ].\nSome shortcomings\nA shortcoming for me about this story is that there are other words with a /wɔ/ sequence that haven’t shifted to [wʊ]. For example, wall [wɔɫ] is very distinct from wool [wʊɫ] for me. Similarly, none of walk, Waldo, walnut, Walter, waltz have [ʊ] in them.\nThis is a classic kind of problem/debate in the study of sound change. Is it exceptionless? Does it happen all at once, or does it move word by word?1 This could be an example of “lexical diffusion”, where for some reason or another, change from [ɔ] ➡️ [ʊ] happened in just one word, water, and not in any of the other words that are similar.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#story-2-passing-through-worder.",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#story-2-passing-through-worder.",
    "title": "Rising Wooders: Part 3",
    "section": "Story 2: Passing through Worder.",
    "text": "Story 2: Passing through Worder.\nAnother possibility here that recently occurred to me is that maybe the [wɔɹɾɚ] tokens I found in the PNC aren’t just some third variant in the mix, but are actually evidence of how water moved from [ɔ] to [ʊ]. But it has a few steps to it.\nr-dissimilation\nUnlike a lot of eastern seaboard cities in North America, like Boston, New York City and Charleston, Philadelphia has always been an /r/ pronouncing city. Where in Boston and NYC they might drop the /r/ in park [phaːk] and car [khaː], Philly has always pronounced that /r/: [phɒɹk] and [khɒɹ].2\nThe one exception to this rule is when there is more than one /r/ in the word, and if one of those /r/s is dropable, people are likely to drop it (Ellis, Groff, and Mead 2006). This tendency is maybe most notable in the name of a nearby suburb and college “Swarthmore”, which is usually pronounced [swɑθ.mɔɹ]. In fact, Swarthmore College recently capitalized on this variation in dueling billboards.3\n\n\n\n\n\n\n\n\n\n“Swathmore” with the first &lt;r&gt; dropped.\n\n\n\n\n\n\n\n\n“Swathmore”, with the first &lt;r&gt; present.\n\n\n\n\n\nFigure 2: Dueling enregisterments.\n\n\nBut, this /r/ dissimilation isn’t restricted to just the word “Swarthmore.” The first /r/ is usually dropped in words like “quarter” or “corner”.\nReinterpretation\nI think this /r/ dissimilation is what got “worder” in the door, which I’ll try to illustrate with an example. Let’s say you’re in Philadelphia and from Philadelphia, and some shows you this coin and says “Here’s a [kʰwɔ.ɾɚ].”\n\n\n\n\n\nFigure 3: A quarter\n\n\nThere are two possible ways you could decide to interpret the underlying form of this word. The first would be to take into account /r/ dissimulation, and stick the /r/ back in, for a /kʰwɔɹ.tɹ̩/ representation. The other would be to not take into account /r/ dissimilation, and decided on a /kʰwɔ.tɹ̩/representation.\n\n\n\n\n\nFigure 4: The possible re-interpretations of kʰwɔ.ɾɚ\n\n\nNow, let’s just take off the [kʰ] at the beginning, and we have a remarkably similar kind of interpretation pathways for water!\n\n\n\n\n\nFigure 5: Possible re-interpretations of wɔ.ɾɚ\n\n\nMerger\nSo, not only is the /wɔɹ.tɹ̩/ reinterpretation plausible, I heard a lot of [wɔɹɾɚ] when I was coding the data. The next thing to know about these vowels in Philly is that the vowels /ɔɹ/ (as in tore) and /uɹ/ tour have merged to a high back position. Perhaps the most noticeable aspect of this merger is that it’s dragged /aɹ/, as in tar, to a much higher and rounded position in Philly than many other varieties.\n\n\n\n\n\n\n\n\n\ntar, tore and tour prior to the merger\n\n\n\n\n\n\n\n\ntar, tore and tour after the merger\n\n\n\n\n\nFigure 6: Before and after the merger\n\n\nIf people were inserting an /ɹ/ into the middle of water, it would belong to this /ɔɹ/ vowel, and we would expect it to be merged into /uɹ/.\nSo, once you’ve put an /ɹ/ into the middle of water and merged it with /uɹ/, what do you get if you then do /r/ dissimilation on that?\nWooder!\n\n\n\n\n\nFigure 7: My proposed pathway of “wooder” development.\n\n\nShortcomings\nSo, a shortcoming of this story about how we got “wooder” is that it could seem a little convoluted. Like we decided to just stick an /ɹ/ in just to delete it later, and voilà! Wooder!\nBenefits\nOn the other hand, there are a bunch of [wɔɹɾɚ] tokens in the Philadelphia Neighborhood Corpus. It also would account for why non of the other /wɔ/ words had this shift to [ʊ]. It also captures a but of what’s going on with daughter as satirized by the SNL skit Murder Durder.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#which-story-is-the-right-one",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#which-story-is-the-right-one",
    "title": "Rising Wooders: Part 3",
    "section": "Which story is the right one?",
    "text": "Which story is the right one?\nI’d say I need to do a bit more work, looking at the acoustic data more closely, looking at how individuals vary, as well as looking at daughter and the other /ɔɹ/ words that both have and don’t have /r/ dissimilation to really get closer to settling the issue.\nBut I think the way you react to each account could shed a little light on the kind of linguistic analyses you prefer. I really like Story 2. All the pieces that make it work are happening and have been described already, so I didn’t need to describe any new phenomenon to get “water” to turn into “wooder”. It just got pulled up in the interplay of different sound changes. It looks like a beautiful pattern, to me.\nHowever, maybe Story 2 looks overwrought to you. “Words just do things sometimes!” you might say. “The /ɔ/ was rising, and there’s a /w/ right there!” And you know what, fair enough! That was my own analysis until about a year ago as well.\nThe good news is that Story 2 has piqued my interest, which means I’m going to be digging into this more in the near future. And maybe as I go I’ll find the “passage through worder” account unsupported, but for right now it seems really plausible to me!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-04_wooder3/index.html#footnotes",
    "href": "posts/2023/01/2023-01-04_wooder3/index.html#footnotes",
    "title": "Rising Wooders: Part 3",
    "section": "Footnotes",
    "text": "Footnotes\n\nLabov (1981), Bybee (2002), Phillips (2006), Fruehwald (2007), and many many others↩︎\nI’ll come back to that different vowel in a sec.↩︎\n\n\n\n\nHave you seen the new signs at the (SEPTA?) train station? 👀 pic.twitter.com/3DYfiGcRPs\n\n— Swarthmore College ((swarthmore?)) August 26, 2022\n\n↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 3"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html",
    "title": "Rising Wooders",
    "section": "",
    "text": "This is a blog post to accompany my American Dialect Society poster.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#wooder",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#wooder",
    "title": "Rising Wooders",
    "section": "“Wooder”",
    "text": "“Wooder”\nIf there’s one thing people know about the Philadelphia dialect, it’s that we say [wʊɾɚ], often spelled “wooder” for the word water. It was how the LA Times opened their story about Mare of Easttown.\n\nBarely 11 minutes into the first episode of “Mare of Easttown,” Kate Winslet goes where few actors have gone before.\nShe says the word “wooder.”\nAs in, what the good people of southeastern Pennsylvania call the stuff that comes out of the faucet.\n\nWhen accepting a Webby award in 2019, the NHL mascot Gritty held up a sign saying\n\nIt Pronounced Wooder, not Water.\n\n\n\nGritty (2019)\n\nThe fact that Philadelphians say “wooder” is also the number one thing anyone ever wants to talk to me about when they find out I study the Philadelphia dialect. Back in 2013, when the big paper about what we found in the Philadelphia Neighborhood Corpus came out (Labov, Rosenfelder, and Fruehwald 2013), we got interviewed by the local news, and they asked me “What about ‘wooder’”? I said “Philadelphians say wooder, and that’s that.”\nWhen I said it, I meant it almost apologetically. We hadn’t investigated anything about the word, mostly because we were focusing on larger structural shifts in the vowel system. As far as I knew, the “wooder” pronunciation was just a one off alteration to a single word, and I didn’t think there was anything too interesting to say about it. “That’s that.” The way it got edited into the final broadcast, it seemed like I was making more of a statement of finality, as if to say “Lots of other things are changing about the Philadelphia dialect, but not ‘wooder.’ And that’s that.”\nThis project is me circling back around to both of those possible messages behind “and that’s that” and asking “is it really?”",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-notice-wooder",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-notice-wooder",
    "title": "Rising Wooders",
    "section": "When did people notice “wooder”?",
    "text": "When did people notice “wooder”?\nI start off the poster looking at what people have had to say about “wooder”. In part, this is inspired by a disagreement I’ve had with how the vowel ought to be described. With my own introspection, I think the eye-dialect version &lt;wooder&gt; is just right. I think the vowel in the first syllable is /ʊ/, or Foot class. However, Labov has suggested here and there that it’s more of an extremely raised and stereotyped realization of /ɔ/ or Thought class.\nI was explaining this to a friend, and she said “Well, you would think it was /ʊ/ growing up with the ‘wooder’ spelling in media and print.” To which I replied, “I don’t think &lt;wooder&gt; was a thing people wrote out when I was growing up.” So, that launched my first systematic exploration of ‘wooder.’\nWooder in print\nMy best approach, so far, to see how long the pronunciation [wʊɾɚ] has been represented as &lt;wooder&gt; is to do a NewsBank search for it in Philadelphia area newspapers. Those results are represented in the next figure.\n\nData Loadinglibrary(tidyverse)\nwooder &lt;- read_csv(\"data/wooder.csv\")\n\n\n\n\nplotting codeall_year &lt;- tibble(\n  year = seq(\n    min(wooder$year),\n    max(wooder$year),\n    by = 1\n  )\n)\n\nwooder |&gt;\n  full_join(all_year) |&gt; \n  replace_na(list(hits = 0)) |&gt; \n  ggplot(aes(year, hits))+\n    stat_smooth(method = \"gam\", \n                method.args = list(family = \"poisson\")) +\n    geom_point()+\n    labs(\n      title = str_wrap(\n        \"'wooder' in print\",\n        width = 30\n        ),\n      subtitle = str_wrap(\n        \"The number of hits for the word 'wooder'\n        in Philadelphia area newspapers by year\",\n        width = 40\n        ),\n      caption = \"Source: Newsbank\"\n      )+\n    theme(aspect.ratio = 5/8,\n          text = element_text(size = 16),\n          plot.subtitle = element_text(size = 10, color = \"grey80\"),\n          plot.caption = element_text(size = 10, color = \"grey80\"))\n\n\n\n\n\n\nFigure 1: Search results for &lt;wooder&gt; in print.\n\n\n\n\nIt won’t be surprising to Philly area people, but every hit prior to 1994 is from a single columnist, Clark DeLeon1. DeLeon wrote a popular column, and often had a keen ear for Philly area dialect features. But other than one columnist’s keen ear, there’s essentially nothing until about the year 2000 when a gradual increase begins. There’s a bit of a phase change in 2015, and when I looked at most of those stories, they were about Philadelphia Brewing Company’s “Holy Wooder,” a Belgian Tripel they released in honor of the papal visit to the city.\nIt’s worth saying I didn’t personally realize there was anything distinctive about the way I said water until I was 18 and in college, and someone did a double take when I said [wʊɾɚ]. That was in 2003, just before the &lt;wooder&gt; boom. Needless to say, I don’t think an 18 year old Philadelphian could repeat that experience today!\nTalking about wooder\nBy this point, I’ve gone through and listened to every token of water in the Philadelphia Neighborhood Corpus, and there are a number of points of “metalinguistic commentary”, or people talking about the word’s pronunciation. The earliest example is from 1974, when a young man said\n\nThat’s what a lot of people say, [wɑtʰɚ], cause [wʊɾɚ] sounds like W-O-O-D-D-E-R or something.\n\nThis is a really interesting comment, cause he’s specifically singling out the [ɑ] vowel pronunciation as something a lot of (presumably other) people say. In fact, two of the other instances of commentary about water only ever use the [ɑ] vowel version as something marked about New York City speech. He also combines [ɑ] with an aspirated [tʰ], which would seem to align use of [ɑ] with hyper- or hypercorrect- speech. While flapping the /t/ in water is nearly a North American standard, it seems to get almost just as much notice and attention as the vowel quality.\nAnd speaking of vowel quality! This comment also identifies the vowel category with /ʊ/, because he specifically spells it out “W-O-O-D”. So, there’s one point in my column!",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-start-saying-wooder",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#when-did-people-start-saying-wooder",
    "title": "Rising Wooders",
    "section": "When did people start saying “wooder”?",
    "text": "When did people start saying “wooder”?\nNow, it’s possible for a linguistic feature to be part a community’s repertoire for a long time without it becoming noticed or commentable. Take the distinctively named Philly area dessert, “water ice,” as an example. This phrase contains two distinctively Philly area pronunciation features:\n\nThe pronunciation of “water” as [wʊɾɚ].\nThe pre-voiceless centralization of the onset of /ay/ to [ʌi] in “ice”.\n\nI’ve actually done a lot of research on this second feature, which is now a well established feature of the dialect. But I’m actually not aware of any public commentary about it, and when the whole phrase is rendered in eye dialect or printed on a tee-shirt, it comes out as just &lt;wooder ice&gt;, not &lt;wooder uhys&gt;\nThat is to say, just because the earliest time I can find anyone discussing “wooder” is 1974 doesn’t mean that’s when people started pronouncing it that way. In fact, it definitely means people starting pronouncing it that way well before! Figuring out the history here became the second part of the project, which I’ll write about in a second post.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-31_wooder1/index.html#footnotes",
    "href": "posts/2022/12/2022-12-31_wooder1/index.html#footnotes",
    "title": "Rising Wooders",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you have access to NewsBank or the Inquirer Archives, the first &lt;wooder&gt; hit is from April 26, 1983 in DeLeon’s column The Scene with the lede “LANGUAGE: YO, BILL, WATCHES TAWKIN’ ABOUT?”. The “Bill” in question was William Safire.↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Rising Wooders"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html",
    "href": "posts/2022/12/2022-12-17/index.html",
    "title": "What is R?",
    "section": "",
    "text": "In the Spring 2023 semester, I’m going to be teaching two R intensive courses: a statistics for linguists course, and an R for the Arts and Sciences course. For both, I’m going to have to do a “What is R” discussion during week 1, and given the breadth of tools I hope students come away with, I’ve been rethinking my usual answers.\nLoading Librarieslibrary(tidyverse)\nlibrary(crandep)\nlibrary(igraph)\nlibrary(ggnetwork)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(plotly)\nlibrary(khroma)\nlibrary(scales)\n\nsource(here::here(\"_defaults.R\"))",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#a-programming-language",
    "href": "posts/2022/12/2022-12-17/index.html#a-programming-language",
    "title": "What is R?",
    "section": "A programming language?",
    "text": "A programming language?\nThe Wikipedia slug for R says\n\nR is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing\n\nAnd yeah, it is definitely a programming language. Here it is doing some programming language things:\n\n2+2\n\n[1] 4\n\n\n\n2+2 &lt; 5\n\n[1] TRUE\n\n\nAnd it can do statistical computing, like a linear model\n\ncars_model &lt;- lm(dist ~ speed, data = cars)\nsummary(cars_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nAnd it can do graphics.\n\n# `PLOT_FONT &lt;- \"Fira Sans\"` in my .Rprofile\npar(family = PLOT_FONT)\nplot(cars)\n\n\n\n\n\n\nFigure 1: A plot\n\n\n\n\nObviously, none of these things are unique to R. In the other programming language I know best, Python, you can fit a linear model and make a scatter plot. What differentiates programming languages, in my experience, is what kinds of operations, data structures, and workflows it prioritizes.\nFor R, I think it’s uncontroversial to say it prioritizes rectangular data with mixed data-type rows & single data-type columns and also provides a lot of options for indexing column-wise. And a lot of the extensions to R have leaned into this prioritization hard.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#an-ecosystem",
    "href": "posts/2022/12/2022-12-17/index.html#an-ecosystem",
    "title": "What is R?",
    "section": "An ecosystem?",
    "text": "An ecosystem?\nBut “R” isn’t just a programming language, it’s also an ecosystem of community created packages. “Learning R” involves learning about these packages, and how they’re interrelated. I grabbed the list of all packages on CRAN and the packages they import with the crandep package.\n\ncran_df &lt;- crandep::get_dep_all_packages()\n\n\nImport Summariescran_df |&gt; \n  filter(type == \"imports\", !reverse) |&gt; \n  count(to) |&gt; \n  arrange(desc(n)) |&gt; \n  mutate(rank = 1:n()) -&gt; imported\n\nimported_10 &lt;- imported |&gt; slice(1:10)\n\n\nIf you count up how often each package gets imported and rank them, you get the familiar power-law plot. I’ve plotted this one out a bit non standard-ly so that frequency is on the x axis for both the main plot and the inset, and so that I could include the package names in the inset with horizontal text.\n\nPlotting codeimported |&gt; \n  ggplot(aes(n, rank))+\n    geom_point()+\n    scale_x_log10(labels = label_comma())+\n    scale_y_log10(labels = label_comma())+\n    labs(title = \"Frequency by rank of imported R packages\") -&gt; mainplot\n\nimported_10 |&gt; \n  mutate(to = as.factor(to),\n          to = fct_reorder(to, rank)) |&gt; \n  ggplot(aes(n, to))+\n    geom_col(fill = \"white\")+\n    geom_text(aes(label = to,\n                  x = 0), \n              color = \"grey10\",\n              hjust = 0,\n              nudge_x = 100, \n              family = PLOT_FONT,\n              size = 4.5)+\n    scale_x_continuous(expand = expansion(mult = 0),\n                       labels = label_comma())+\n    theme(axis.text.y = element_blank(),\n          text = element_text(size = 10),\n          panel.grid.minor  = element_blank(),\n          panel.grid.major.y = element_blank())+\n    labs(y = NULL,\n         title = \"top10\") -&gt; inset\n \nmainplot + inset_element(inset, 0.05, 0.05, 0.5, 0.6)\n\n\n\n\n\n\nFigure 2: A log(rank) by log(frequency) plot of R imports\n\n\n\n\nHere’s a network visualization of these imports and dependencies. I color coded the nodes according to common R package naming trends\n\ngg* - Packages extending ggplot2\ntidy* - Packages declaring their adherence to tidy-data principles (and the tidyverse more generally)\n*r - Packages declaring that they are… R packages\n\n\nNetwork graph setupimported |&gt;\n  filter(n &gt;= 5) |&gt; \n  pull(to) -&gt; to_network\n\ncran_df |&gt;\n  filter(!reverse, type == \"imports\",\n         to %in% to_network) |&gt;\n  df_to_graph(nodelist = cran_df |&gt; rename(name = from)) -&gt; cran_network\n\nset.seed(300)\ncran_flat &lt;- ggnetwork(cran_network, layout = with_drl())\n\nxclip &lt;- quantile(cran_flat$x, c(0.0025, 0.9975))\nyclip &lt;- quantile(cran_flat$y, c(0.0025, 0.9975))\n\n\n\nNetwork graphcran_flat |&gt; \n  mutate(name_pattern = case_when(str_detect(name, \"[rR]$\") ~ \"thingr\",\n                                  str_detect(name, \"tidy\") ~ \"tidy\",\n                                  str_detect(name, \"^[Gg]g\") ~ \"gg\",\n                                  T ~ \"else\"),\n         name_pattern = factor(name_pattern, levels = c(\"tidy\", \"gg\", \"thingr\", \"else\"))) |&gt; \n  arrange(desc(name_pattern)) |&gt; \n  filter(x &gt;= xclip[1], x &lt;= xclip[2],\n         y &gt;= yclip[1], y &lt;= yclip[2]) |&gt; \nggplot(aes(x = x, y = y, xend = xend, yend = yend, color = name_pattern))+\n  #geom_nodes()+  \n  geom_nodes(aes(alpha = name_pattern))+\n  scale_color_bright(limits = c(\"tidy\", \"gg\", \"thingr\", \"else\"),\n                     labels = c(\"tidy*\", \"gg*\", \"*r\", \"else\"))+\n  dark_theme_void()+\n  scale_alpha_manual(values = c(0.5,0.3, 0.08, 0.02), \n                     limits = c(\"tidy\", \"gg\", \"thingr\", \"else\"),\n                     guide = \"none\")+\n  labs(color = NULL,\n       title = \"CRAN imports network visualization\")+\n  theme(legend.position = c(0.2,0.8), \n        legend.text = element_text(family = PLOT_FONT),\n        text = element_text(family = PLOT_FONT),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nFigure 3: CRAN network graph",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#a-communications-platform",
    "href": "posts/2022/12/2022-12-17/index.html#a-communications-platform",
    "title": "What is R?",
    "section": "A communications platform?",
    "text": "A communications platform?\nBut beyond just the R packages that implement specific analysis or process data in a specific way, there are also all of the tools built around R (and mostly around the RStudio IDE) that also make R what I might call a “communications platform.” From Sweave to knitr to rmarkdown and now Quarto, the kind of literate programming you can do in R has moved from ugly1 Beamer slides to, well, full on blogs.\nBut, it’s not just for the novelty or nerd appeal that I think it’s important to learn about R authoring tools available. They’ve also changed my own discovery and learning process about new packages. You can always find the documentation for a package on CRAN, but you should really try to find its pkgdown site.2",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#what-does-it-mean-to-know-r",
    "href": "posts/2022/12/2022-12-17/index.html#what-does-it-mean-to-know-r",
    "title": "What is R?",
    "section": "What does it mean to “know R”?",
    "text": "What does it mean to “know R”?\nWhen I think about what it means to “know R”, and my goal for the kind of knowledge my students should start getting a handle on, it involves all of these components: the programming syntax, the social graph of the ecosystem, and the authoring tools to use and seek out.\nA lot of other programming languages have similar kinds of features, especially Python with pypi or conda keeping track of the ecosystem and Sphinx providing the authoring tools. There too I’d say that getting to “know Python” involves a lot more than learning its syntax.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-17/index.html#footnotes",
    "href": "posts/2022/12/2022-12-17/index.html#footnotes",
    "title": "What is R?",
    "section": "Footnotes",
    "text": "Footnotes\n\nsorry, but they are↩︎\nMost often, click on the URL listed on CRAN, which takes you to the package’s github, which then probably has a link to the pkgdown site in its “about” box.↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "What *is* R?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Væl Space",
    "section": "",
    "text": "Hello!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with stat_manual()\n\n\n\n\n\n\n\n\n\n\n\n2025-09-12\n\n\nJosef Fruehwald\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nggplot2 4.0 and Dark Mode\n\n\n\n\n\n\n\n\n\n\n\n2025-09-11\n\n\nJosef Fruehwald\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nAI Policy\n\n\n\n\n\nA version of my AI course policy.\n\n\n\n\n\n2025-08-21\n\n\nJosef Fruehwald\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nBlueprint Model of Production, Pythonically\n\n\n\n\n\nI try to implement the formal description of phonology and phonetics from @nelson2025 in python.\n\n\n\n\n\n2025-08-12\n\n\nJosef Fruehwald\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nSetting Up Rogue Scholar\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n\n\nJosef Fruehwald\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nGetting the xkcd color survey sqlite database\n\n\n\n\n\n\n\n\n\n\n\n2025-07-16\n\n\nJosef Fruehwald\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nLight vs Dark &lt;color&gt;\n\n\n\n\n\n\n\n\n\n\n\n2025-07-14\n\n\nJosef Fruehwald\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the Color Sphere\n\n\n\n\n\n\n\n\n\n\n\n2025-07-12\n\n\nJosef Fruehwald\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\ntidytuesday color survey\n\n\n\n\n\n\n\n\n\n\n\n2025-07-09\n\n\nJosef Fruehwald\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nDoing cool things with the Discrete Cosine Transform in tidynorm\n\n\n\n\n\nDCT coefficients are really useful!\n\n\n\n\n\n2025-06-17\n\n\nJosef Fruehwald\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nIntroducing tidynorm\n\n\n\n\n\nHere’s a brief introduction to the new {tidynorm} package.\n\n\n\n\n\n2025-06-16\n\n\nJosef Fruehwald\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nCustom Code Chunk css\n\n\n\n\n\n\n\n\n\n\n\n2025-03-11\n\n\nJosef Fruehwald\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nIn Remembrance\n\n\n\n\n\n\n\n\n\n\n\n2024-12-17\n\n\nJosef Fruehwald\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nRandom Effects and Overdispersion\n\n\n\n\n\n\n\n\n\n\n\n2024-11-20\n\n\nJosef Fruehwald\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nRandom effect priors, redo\n\n\n\n\n\n\n\n\n\n\n\n2024-11-19\n\n\nJosef Fruehwald\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nSetting default ggplot2 colors\n\n\n\n\n\n\nggplot2\n\ndataviz\n\n\n\n\n\n\n\n\n\n2024-10-01\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding up a complex {gt} table.\n\n\n\n\n\n\n\n\n\n\n\n2024-09-15\n\n\nJosef Fruehwald\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nRolling for Damage with the Central Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n2024-09-08\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting RStudio for GitHub Classrom and Codespaces\n\n\nA living document\n\n\n\n\n\n\n\n\n2024-09-06\n\n\nJosef Fruehwald\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with Object Oriented Programming in R\n\n\n\n\n\n\n\n\n\n\n\n2024-09-02\n\n\nJosef Fruehwald\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\nWorking with the Discrete Cosine Transform in R\n\n\n\n\n\n\n\n\n\n\n\n2024-07-19\n\n\nJosef Fruehwald\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nUsing FastTrackPy and aligned-textgrid\n\n\n\n\n\n\n\n\n\n\n\n2024-02-16\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nSome lessons in using Github Classroom and Codespaces\n\n\n\n\n\n\n\n\n\n\n\n2024-02-14\n\n\nJosef Fruehwald\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nThinking About Hierarchical Variance Parameters\n\n\n\n\n\n\n\n\n\n\n\n2023-06-29\n\n\nJosef Fruehwald\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nGetting a sense of priors for logistic regression\n\n\n\n\n\n\n\n\n\n\n\n2023-06-28\n\n\nJosef Fruehwald\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nChanging Project Defaults\n\n\n\n\n\n\n\n\n\n\n\n2023-06-16\n\n\nJosef Fruehwald\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nUsing Quarto Blogs for Teaching\n\n\n\n\n\n\n\n\n\n\n\n2023-05-03\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nSimulating DND Rolls\n\n\n\n\n\n\n\n\n\n\n\n2023-02-12\n\n\nJosef Fruehwald\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nA handy dplyr function for linguistics\n\n\n\n\n\n\n\n\n\n\n\n2023-02-05\n\n\nJosef Fruehwald\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nMaking a Plotly Plot\n\n\n\n\n\n\nplotly\n\n\n\n\n\n\n\n\n\n2023-01-29\n\n\nJosef Fruehwald\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nR Package Exploration (Jan 2023)\n\n\n\n\n\n\nR\n\nR package exploration\n\n`{ggforce}`\n\n`{geomtextpath}`\n\n`{ggdensity}`\n\n\n\n\n\n\n\n\n\n2023-01-27\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nMaking a spectrogram in R\n\n\n\n\n\n\nhow-to\n\nr\n\n\n\n\n\n\n\n\n\n2023-01-22\n\n\nJosef Fruehwald\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nRising Wooders: Part 3\n\n\nPart 3: The role of r-dissimilation\n\n\n\nresearch\n\nlinguistics\n\n\n\n\n\n\n\n\n\n2023-01-04\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nRising Wooders: Part 2\n\n\nPart 2: Saying Wooder\n\n\n\nresearch\n\nlinguistics\n\n\n\n\n\n\n\n\n\n2023-01-01\n\n\nJosef Fruehwald\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nRising Wooders\n\n\nPart 1: Talking about Wooder\n\n\n\nresearch\n\nlinguistics\n\n\n\n\n\n\n\n\n\n2022-12-31\n\n\nJosef Fruehwald\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nGithub Onboarding with RStudio\n\n\n\n\n\n\n\n\n\n\n\n2022-12-21\n\n\nJosef Fruehwald\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nWhat is R?\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n2022-12-17\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nA lesson in over-preparing\n\n\n\n\n\n\n\n\n\n\n\n2022-12-10\n\n\nJosef Fruehwald\n\n8 min\n\n\n\n\nNo matching items\n\n  \n\nReuseCC-BY 4.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m an Assistant Professor of Linguistics at the university of Kentucky. You can find out more about me and my research on my homepage."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I’m an Assistant Professor of Linguistics at the university of Kentucky. You can find out more about me and my research on my homepage."
  },
  {
    "objectID": "about.html#about-the-title",
    "href": "about.html#about-the-title",
    "title": "About",
    "section": "About the title",
    "text": "About the title\nIn my variety of Philadelphia English, the diphthong /aw/ merges with /æː/ before /l/. So vowel and val both sound like [væːɫ], and sometimes vow gets into the mix as well.\nThroughout grad school and a few years after, I maintained a blog called Val Systems. It’s still there, but I got out of the habit of blogging there, and eventually came to feel a bit hampered by the drafting, posting & designing process there. So now, maybe I’ll maintain Væl Space."
  },
  {
    "objectID": "about.html#site-fonts",
    "href": "about.html#site-fonts",
    "title": "About",
    "section": "Site fonts",
    "text": "Site fonts\n\nHeaders\n\nComfortaa\n\nBody\n\nPublic Sans\n\nFigures\n\nPublic Sans\n\nCode\n\nFira code"
  },
  {
    "objectID": "defaults.html",
    "href": "defaults.html",
    "title": "Site Defaults",
    "section": "",
    "text": "_defaults.R\n\nlibrary(ggplot2)\nlibrary(ggdark)\nlibrary(showtext)\nlibrary(colorspace)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(rlang)\n\n# get plot fonts\nfont_add_google(name = \"Public Sans\", family = \"Public Sans\")\nshowtext_auto()\n\n# Set global variable for setting fonts\n# that aren't set by theme(text=...)\nPLOT_FONT &lt;- \"Public Sans\"\n\n\n# from the theme _variables.scss\nbody_bg &lt;- \"#222222\"\nplot_bg &lt;- darken(\"#375a7f\", 0.50)\nmajor &lt;- lighten(\n  plot_bg,\n  amount = 0.25\n)\nminor &lt;- lighten(\n  plot_bg,\n  amount = 0.125\n)\nstrip_bg &lt;- lighten(plot_bg, 0.5)\n\nptol_red &lt;- \"#EE6677\"\nptol_blue &lt;- \"#4477AA\"\n\n\ntheme_set(\n  theme_minimal(base_size = 16) +\n    theme(\n      text = element_text(family = \"Public Sans\")\n    ) +\n    theme_sub_panel(\n      grid = element_blank(),\n    ) +\n    theme_sub_legend(\n      key = element_blank(),\n      background = element_blank()\n    ) +\n    theme_sub_axis(\n      ticks = element_blank(),\n      line = element_line(color = \"grey60\", linewidth = 0.2)\n    )\n)\n\ntheme_darkmode &lt;- function(){\n  theme_minimal(\n    base_size = 16,\n    paper = \"#222\",\n    ink = \"white\"\n  ) +\n    theme(\n      text = element_text(family = \"Public Sans\")\n    ) +\n    theme_sub_panel(\n      background = element_rect(\n        fill = \"#424952\", color = NA\n      ),\n      grid = element_blank()\n    ) +\n    theme_sub_legend(\n      key = element_blank(),\n      background = element_blank()\n    ) +\n    theme_sub_axis(\n      line = element_line(\n        color = \"grey60\", linewidth = 0.2\n      )\n    )\n}\n\ndark_render &lt;- function(plot){\n  ggdark::invert_geom_defaults()\n  withr::defer(ggdark::invert_geom_defaults())\n  print(plot)\n}\n\n\ntheme_no_y &lt;- function(){\n  theme(\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n}\n\ntheme_no_x &lt;- function(){\n  theme(\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank(),\n    panel.grid.major.x = element_blank()\n  )\n}\n\nout2fig = function(out.width, out.width.default = 0.7, fig.width.default = 6) {\n  fig.width.default * out.width / out.width.default \n}\n\noptions(\n  ggplot2.discrete.colour = lapply(1:12, ggthemes::ptol_pal()),\n  ggplot2.discrete.fill = lapply(1:12, ggthemes::ptol_pal()),\n  ggplot2.ordinal.colour = \\(...) scale_color_viridis_d(option = \"G\", direction = -1, ...),\n  ggplot2.ordinal.fill = \\(...) scale_fill_viridis_d(option = \"G\", direction = -1, ...),  \n  ggplot2.continuous.colour = \\(...) scico::scale_color_scico(palette = \"batlow\", ...),\n  ggplot2.continuous.fill = \\(...) scico::scale_fill_scico(palette = \"batlow\", ...)\n)\n\n# set a crop: true hook\nknitr::knit_hooks$set(crop = knitr::hook_pdfcrop)\n\n\n# dark gt theme\ndark_gt_theme &lt;- function(tbl){\n  \n  style_cells &lt;- list(\n    cells_body(),\n    cells_column_labels(),\n    cells_column_spanners(),\n    cells_footnotes(),\n    cells_row_groups(),\n    cells_source_notes(),\n    cells_stub(),\n    cells_stubhead(),\n    cells_title()\n  )\n  \n  summary_info &lt;- tbl$`_summary` |&gt; \n    map(\n      ~\":GRAND_SUMMARY:\" %in% .x$groups\n    )  |&gt; \n    list_simplify()\n  \n  if(any(summary_info)){\n    style_cells &lt;- c(\n      style_cells,\n      list(\n        cells_grand_summary(),\n        cells_stub_grand_summary()\n      )\n    )\n  }\n  \n  if(any(!(summary_info %||% T))){\n    style_cells &lt;- c(\n      style_cells,\n      list(\n        cells_stub_summary(),\n        cells_summary()\n      )\n    )\n  }\n  \n  tbl |&gt; \n    opt_table_font(\n      font = c(\n        google_font(name = \"Public Sans\"),\n        default_fonts()\n      )\n    )|&gt; \n    tab_style(\n      style = \"\n        background-color: var(--bs-body-bg);\n        color: var(--bs-body-color)\n      \",\n      locations = style_cells\n    ) \n}\n\nnew_post &lt;- function(title = \"\"){\n  \n  day &lt;- Sys.Date()\n  slug = snakecase::to_snake_case(title, sep_out =\"-\")\n  \n  pieces &lt;- stringr::str_split(day, \"-\")[[1]]\n  year &lt;- pieces[1]\n  month &lt;- pieces[2]\n  month_path = here::here(\"posts\", year, month)\n  post_path = fs::path(\n    year, \n    month,\n    stringr::str_glue(\"{day}_{slug}\")\n  )\n  \n  if(!fs::dir_exists(month_path)){\n    fs::dir_create(month_path, recurse = T)\n  }\n  \n  quarto::new_blog_post(\n    title = title,\n    dest = post_path\n  )\n}"
  },
  {
    "objectID": "defaults.html#r-defaults",
    "href": "defaults.html#r-defaults",
    "title": "Site Defaults",
    "section": "",
    "text": "_defaults.R\n\nlibrary(ggplot2)\nlibrary(ggdark)\nlibrary(showtext)\nlibrary(colorspace)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(rlang)\n\n# get plot fonts\nfont_add_google(name = \"Public Sans\", family = \"Public Sans\")\nshowtext_auto()\n\n# Set global variable for setting fonts\n# that aren't set by theme(text=...)\nPLOT_FONT &lt;- \"Public Sans\"\n\n\n# from the theme _variables.scss\nbody_bg &lt;- \"#222222\"\nplot_bg &lt;- darken(\"#375a7f\", 0.50)\nmajor &lt;- lighten(\n  plot_bg,\n  amount = 0.25\n)\nminor &lt;- lighten(\n  plot_bg,\n  amount = 0.125\n)\nstrip_bg &lt;- lighten(plot_bg, 0.5)\n\nptol_red &lt;- \"#EE6677\"\nptol_blue &lt;- \"#4477AA\"\n\n\ntheme_set(\n  theme_minimal(base_size = 16) +\n    theme(\n      text = element_text(family = \"Public Sans\")\n    ) +\n    theme_sub_panel(\n      grid = element_blank(),\n    ) +\n    theme_sub_legend(\n      key = element_blank(),\n      background = element_blank()\n    ) +\n    theme_sub_axis(\n      ticks = element_blank(),\n      line = element_line(color = \"grey60\", linewidth = 0.2)\n    )\n)\n\ntheme_darkmode &lt;- function(){\n  theme_minimal(\n    base_size = 16,\n    paper = \"#222\",\n    ink = \"white\"\n  ) +\n    theme(\n      text = element_text(family = \"Public Sans\")\n    ) +\n    theme_sub_panel(\n      background = element_rect(\n        fill = \"#424952\", color = NA\n      ),\n      grid = element_blank()\n    ) +\n    theme_sub_legend(\n      key = element_blank(),\n      background = element_blank()\n    ) +\n    theme_sub_axis(\n      line = element_line(\n        color = \"grey60\", linewidth = 0.2\n      )\n    )\n}\n\ndark_render &lt;- function(plot){\n  ggdark::invert_geom_defaults()\n  withr::defer(ggdark::invert_geom_defaults())\n  print(plot)\n}\n\n\ntheme_no_y &lt;- function(){\n  theme(\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n}\n\ntheme_no_x &lt;- function(){\n  theme(\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank(),\n    panel.grid.major.x = element_blank()\n  )\n}\n\nout2fig = function(out.width, out.width.default = 0.7, fig.width.default = 6) {\n  fig.width.default * out.width / out.width.default \n}\n\noptions(\n  ggplot2.discrete.colour = lapply(1:12, ggthemes::ptol_pal()),\n  ggplot2.discrete.fill = lapply(1:12, ggthemes::ptol_pal()),\n  ggplot2.ordinal.colour = \\(...) scale_color_viridis_d(option = \"G\", direction = -1, ...),\n  ggplot2.ordinal.fill = \\(...) scale_fill_viridis_d(option = \"G\", direction = -1, ...),  \n  ggplot2.continuous.colour = \\(...) scico::scale_color_scico(palette = \"batlow\", ...),\n  ggplot2.continuous.fill = \\(...) scico::scale_fill_scico(palette = \"batlow\", ...)\n)\n\n# set a crop: true hook\nknitr::knit_hooks$set(crop = knitr::hook_pdfcrop)\n\n\n# dark gt theme\ndark_gt_theme &lt;- function(tbl){\n  \n  style_cells &lt;- list(\n    cells_body(),\n    cells_column_labels(),\n    cells_column_spanners(),\n    cells_footnotes(),\n    cells_row_groups(),\n    cells_source_notes(),\n    cells_stub(),\n    cells_stubhead(),\n    cells_title()\n  )\n  \n  summary_info &lt;- tbl$`_summary` |&gt; \n    map(\n      ~\":GRAND_SUMMARY:\" %in% .x$groups\n    )  |&gt; \n    list_simplify()\n  \n  if(any(summary_info)){\n    style_cells &lt;- c(\n      style_cells,\n      list(\n        cells_grand_summary(),\n        cells_stub_grand_summary()\n      )\n    )\n  }\n  \n  if(any(!(summary_info %||% T))){\n    style_cells &lt;- c(\n      style_cells,\n      list(\n        cells_stub_summary(),\n        cells_summary()\n      )\n    )\n  }\n  \n  tbl |&gt; \n    opt_table_font(\n      font = c(\n        google_font(name = \"Public Sans\"),\n        default_fonts()\n      )\n    )|&gt; \n    tab_style(\n      style = \"\n        background-color: var(--bs-body-bg);\n        color: var(--bs-body-color)\n      \",\n      locations = style_cells\n    ) \n}\n\nnew_post &lt;- function(title = \"\"){\n  \n  day &lt;- Sys.Date()\n  slug = snakecase::to_snake_case(title, sep_out =\"-\")\n  \n  pieces &lt;- stringr::str_split(day, \"-\")[[1]]\n  year &lt;- pieces[1]\n  month &lt;- pieces[2]\n  month_path = here::here(\"posts\", year, month)\n  post_path = fs::path(\n    year, \n    month,\n    stringr::str_glue(\"{day}_{slug}\")\n  )\n  \n  if(!fs::dir_exists(month_path)){\n    fs::dir_create(month_path, recurse = T)\n  }\n  \n  quarto::new_blog_post(\n    title = title,\n    dest = post_path\n  )\n}"
  },
  {
    "objectID": "defaults.html#prerender",
    "href": "defaults.html#prerender",
    "title": "Site Defaults",
    "section": "Prerender",
    "text": "Prerender\n\n\nprerender.py\n\nfrom commonmeta import encode_doi\nfrom pathlib import Path\nimport yaml\nimport logging\n\ndef get_all_posts(post_base:str = \"posts\")-&gt;list[Path]:\n  \"\"\"Get all post directories\n\n  Args:\n      post_base (str, optional): \n        base post path. Defaults to \"posts\".\n\n  Returns:\n      list[Path]: Path to every post\n  \"\"\"\n  post_dirs = [\n    post\n    for year in Path(post_base).glob(\"*/\")\n    for month in year.glob(\"*/\")\n    for post in month.glob(\"*/\")\n  ]\n  return post_dirs\n\ndef make_metadata(path:Path) -&gt; None:\n  \"\"\"Create empty `_metadata.yml` file.\n\n  Args:\n      path (Path): Path to post\n  \"\"\"\n  metadata_path = path.joinpath(\"_metadata.yml\")\n  if metadata_path.exists():\n    return\n  else:\n    metadata_path.touch()\n\ndef make_all_metadata(paths:list[Path]) -&gt; None:\n  \"\"\"Make all `_metadata.yml`\n\n  Args:\n      paths (list[Path]): \n        List of all post paths\n  \"\"\"\n  for p in paths:\n    make_metadata(p)\n    \ndef make_date(path:Path)-&gt;str:\n  \"\"\"Return a date string based on the path name\n\n  Args:\n      path (Path): Path to post\n\n  Returns:\n      (str): Date string\n  \"\"\"\n  return path.name.split(\"_\")[0]\n\ndef make_doi() -&gt; str:\n  \"\"\"Make doi number\n\n    Returns:\n      (str): doi\n  \"\"\"\n  doi_url = encode_doi(\"10.59350\")\n  doi_number = doi_url.removeprefix(\"https://doi.org/\")\n  return doi_number\n    \ndef check_metadata(path:Path)-&gt;None:\n  \"\"\"Check and update a `_metadata.yml` file\n\n  Args:\n      path (Path): Path to post\n  \"\"\"\n  metadata_path = path.joinpath(\"_metadata.yml\")\n  dat = yaml.safe_load(metadata_path.read_text())\n\n  if dat is None:\n    dat = {\n      \"date\": make_date(path),\n      \"doi\": make_doi()\n    }\n    metadata_path.write_text(\n      yaml.dump(dat)\n    )\n    return\n  \n  # just to be safe\n  if not isinstance(dat, dict):\n    logging.warning(f\"Non-dict _metadata for {str(path)}\")\n    return\n  \n  # avoid writing if not necessary\n  if \"date\" in dat \\\n    and \"doi\" in dat \\\n    and dat[\"date\"] == make_date(path):\n\n    return\n  \n  if not \"doi\" in dat:\n    dat[\"doi\"] = make_doi()\n\n  if not \"date\" in dat:\n    dat[\"date\"] = make_date(path)\n  elif dat[\"date\"] != make_date(path):\n    dat[\"date\"] = make_date(path)\n   \n  metadata_path.write_text(\n      yaml.dump(dat)\n  )\n\ndef check_all_metadata(paths: list[Path]) -&gt; None:\n  \"\"\"Check and update all `_metadata.yml` files\n\n  Args:\n      paths (list[Path]): _description_\n  \"\"\"\n  for p in paths:\n    check_metadata(p)\n  \npost_dirs = get_all_posts(\"posts\")\nmake_all_metadata(post_dirs)\ncheck_all_metadata(post_dirs)"
  },
  {
    "objectID": "defaults.html#github-action",
    "href": "defaults.html#github-action",
    "title": "Site Defaults",
    "section": "Github Action",
    "text": "Github Action\nname: Build Site\non:\n  push:\n    branches: [\"master\"]\n    \nenv:\n  RENV_PATHS_ROOT: ~/.cache/R/renv\n  QUARTO_PYTHON: renv/python/virtualenvs/renv-python-3.12/bin/python\njobs:\n  build-docs:\n     runs-on: macos-latest\n     steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n        with:\n          python-version: \"3.12\"\n      - name: Set Reticulate\n        run: |\n          echo \"RETICULATE_PYTHON=$(which python)\" &gt;&gt; \"$GITHUB_ENV\"\n          echo \"RENV_PYTHON=$(which python)\" &gt;&gt; \"$GITHUB_ENV\"\n          echo \"QUARTO_PYTHON=$(which python)\" &gt;&gt; \"$GITHUB_ENV\"\n          pip install -r requirements.txt\n      - uses: r-lib/actions/setup-r@v2\n      - name: Cache packages\n        uses: actions/cache@v4\n        with:\n          path: ${{ env.RENV_PATHS_ROOT }}\n          key: ${{ runner.os }}-renv-${{ hashFiles('**/renv.lock') }}\n          restore-keys: |\n            ${{ runner.os }}-renv-          \n      - name: Restore packages\n        shell: Rscript {0}\n        run: |\n          if (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\n          renv::install(\"reticulate\")\n          #reticulate::install_python(\"3.12:latest\")\n          renv::use_python(type = \"virtualenv\")\n          renv::restore()\n      - name: Set up quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n      # - name: Set Quarto Python\n      #   run: |\n      #     echo \"QUARTO_PYTHON=renv/python/virtualenvs/renv-python-3.12/bin/python\" &gt;&gt; $GITHUB_ENV\n      - name: Render and publish to gh pages\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages"
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html",
    "href": "posts/2022/12/2022-12-10/index.html",
    "title": "A lesson in over-preparing",
    "section": "",
    "text": "One of the courses I taught in the Fall 2022 semester was Natural Language Processing (NLP). It was a fun course to teach, and I learned a lot since it’s not a topic in my core areas of research. At the same time, the amount of work I put into it has made me really start to rethink how I prepare for teaching.\nMy tendency, for a while, has been to prepare extensive course notes that I publish on my website (Exhibit A, Exhibit B). I’ve never really reflected on how much work that actually takes. Moreover, I don’t tend to consider it when I reflect on how much “writing” I get done (and inevitably get a bit discouraged about my productivity).\nBut, it occurred to me that I could quantify how much writing I really did this semester. I wrote all my course notes in Quarto, which generates a search index in a .json file. To get a total count of words that I wrote in course notes, I just need to parse that json file and tokenize it! I found a handy blog post about analyzing git repos that helped a lot in the process.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#setup",
    "href": "posts/2022/12/2022-12-10/index.html#setup",
    "title": "A lesson in over-preparing",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidyjson)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(padr)\nlibrary(glue)\n\nsource(here::here(\"_defaults.R\"))\n\nThis block of code is just copied from the blog post I just mentioned.\n\n# Remote repository URL\nrepo_url &lt;- \"https://github.com/JoFrhwld/2022_lin517.git\"\n\n# Directory into which git repo will be cloned\nclone_dir &lt;- file.path(tempdir(), \"git_repo\")\n\n# Create command\nclone_cmd &lt;- glue(\"git clone {repo_url} {clone_dir}\")\n\n# Invoke command\nsystem(clone_cmd)\n\nThere’s a handful of R libraries for reading json files into R, but after searching around I went with tidyjson because I like using tidyverse things.\n\nsearch_file &lt;- file.path(clone_dir, \"_site\", \"search.json\")\nsite_search &lt;- read_json(search_file)\n\nI was really glad to find that the search.json file is relatively flat, so pulling out the metadata and text was not as complicated as it could have been.\n\nsite_search |&gt;\n  gather_array() |&gt;\n  hoist(..JSON,\n    \"title\",\n    \"section\",\n    \"text\"\n  )  |&gt;\n  as_tibble() |&gt;\n  select(array.index, title, section, text) |&gt;\n  unnest_tokens(input = text, output = \"words\") -&gt; tokens_table\n\nI noticed in the json file that there were multiple entries for a single page of course notes, one for each subsection, but there were also some entries with the subsection value set to blank. I just wanted to double check that the blank section entries weren’t the entire page of lecture notes, with additional entries duplicating the text by subsection.\n\ntokens_table |&gt;\n  mutate(has_section = section != \"\") |&gt;\n  group_by(title, has_section) |&gt;\n  count()\n\n# A tibble: 45 × 3\n# Groups:   title, has_section [45]\n   title                              has_section     n\n   &lt;chr&gt;                              &lt;lgl&gt;       &lt;int&gt;\n 1 Addendum                           FALSE          43\n 2 Addendum                           TRUE          100\n 3 Additional Neural Network Concepts FALSE         473\n 4 Additional Neural Network Concepts TRUE         1109\n 5 Comprehensions and Useful Things   FALSE        1116\n 6 Data Processing                    FALSE         654\n 7 Data Processing                    TRUE         1804\n 8 Data Sparsity                      FALSE         720\n 9 Data Sparsity                      TRUE         1724\n10 Evaluating models                  FALSE          42\n# ℹ 35 more rows\n\n\nLooks like no. The blank titled sections are probably cases where I had a paragraph or two that came before the first section header.\nSo how many words?\n\n\ntokens_table |&gt;\n  nrow()\n\n[1] 43637\n\n\nBased on the default tokenization from tidytext, it looks like I wrote just north of 40k words. I don’t have the best sense of how this compares to other kinds of genre writing, but apparently the goal of NaNoWriMo (National Novel Writing Month) is to write a novel that is 50k words.\nSo, I didn’t quite write a novel. But the amount of work that went into these 40k words was still considerable in terms of background research and trying to come to my own understanding of relatively complex mathematical formulae so that I could distill them into a comprehensible lesson. Also not accounted for was all the code I wrote and had to debug within the course notes!",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#how-many-words-did-i-write-over-time",
    "href": "posts/2022/12/2022-12-10/index.html#how-many-words-did-i-write-over-time",
    "title": "A lesson in over-preparing",
    "section": "How many words did I write over time?",
    "text": "How many words did I write over time?\nSince I was publishing the course notes to github, that means I also have a preserved history of how my word count grew over time. All I have to do is apply the same procedures to the history of search.json.\n\ncode to get the git commit historylog_format_options &lt;- c(datetime = \"cd\", commit = \"h\", parents = \"p\", author = \"an\", subject = \"s\")\noption_delim &lt;- \"\\t\"\nlog_format   &lt;- glue(\"%{log_format_options}\") |&gt; glue_collapse(option_delim)\nlog_options  &lt;- glue('--pretty=format:\"{log_format}\" --date=format:\"%Y-%m-%d %H:%M:%S\"')\nlog_cmd      &lt;- glue('git -C {clone_dir} log {log_options}')\n\n\n\nsystem(log_cmd, intern = TRUE) |&gt;\n  str_split_fixed(option_delim, length(log_format_options)) |&gt;\n  as_tibble(.name_repair = \"minimal\") |&gt;\n  setNames(names(log_format_options))-&gt;commit_history\n\n\nget_length() function definitionget_length &lt;- function(commit, clone_dir){\n  search_file &lt;- file.path(clone_dir, \"_site\", \"search.json\")\n  checkout_cmd &lt;- glue(\"git -C {clone_dir} checkout {commit} {search_file}\")\n  system(checkout_cmd)\n  site_search &lt;- read_json(search_file)\n  site_search |&gt;\n    gather_array() |&gt;\n    hoist(..JSON,\n      \"title\",\n      \"section\",\n      \"text\"\n    )  |&gt;\n    as_tibble() |&gt;\n    select(array.index, title, section, text) |&gt;\n    unnest_tokens(input = text, output = \"words\") |&gt;\n    nrow() -&gt; word_count\n  return(word_count)\n}\n\n\nThe data frame commit_history contains metadata about each commit. What I’ll do next is apply the function I wrote in the collapsed code above to the list of commit hashes, and get the word count at each commit.\n\ncommit_history |&gt;\n  mutate(word_count = map(commit, ~get_length(.x, clone_dir)) |&gt;\n           simplify()) -&gt; word_history\n\nI often had many commits in a single day, so to simplify things a bit, I’ll just get the max number of words I had by the end of a given day. Fortunately, I did not stay up past midnight pushing commits this semester, so the data shouldn’t be messed up by work sessions overlapping across days.\n\nword_history |&gt;\n  mutate(datetime = ymd_hms(datetime)) |&gt;\n  thicken(\"day\") |&gt;\n  group_by(datetime_day) |&gt;\n  summarise(words = max(word_count)) |&gt;\n  arrange(datetime_day) -&gt; daily_words\n\nCumulative Wordcount\nHere’s the cumulative count of words over the course of the semester. I forget why, exactly, it starts out at 7,000 words. I think I might’ve been managing the course notes differently between the end of August and start of September, and then just copied them over. But it is a pretty continuous rise, with a few notable plateaus. Some of those plateaus occurred when I had actually prepared more than could be covered in a one or two class meetings, so we stayed with a set of lecture notes for a longer period of time.\n\nplotting codedaily_words |&gt;\n  pad(\"day\") |&gt;\n  fill(words) |&gt;\n  ggplot(aes(datetime_day, words))+\n    geom_ribbon(aes(ymin = 0, ymax = words), alpha = 0.2)+\n    geom_line(linewidth = 1,\n              lineend = \"round\")+\n    expand_limits(y = 0)+\n    scale_y_continuous(labels = label_comma(),\n                       expand = expansion(mult = c(0, 0.05)))+\n    scale_x_date(expand = expansion(mult = 0.01))+\n    labs(x = \"date\",\n         y = \"number of words\",\n         title = \"Cumulative number of words in course notes\")\n\n\n\n\n\n\nFigure 1: The number of words I wrote over the course of Fall 2022\n\n\n\n\nWhen was I writing?\nI can also double check when I was writing my course notes. I was teaching on a Monday-Wednesday-Friday schedule, and Fridays were usually given over to practical exercises (the text of which is largely absent from these counts…).\nIt was no surprise to me that I wrote most of the course notes on Sundays. The total number of Sunday words is about 17k, which comes out to about mean of about 1,300 words for Sundays this semester.\n\nplotting codedaily_words |&gt;\n  pad(\"day\") |&gt;\n  fill(words) |&gt;\n  mutate(daily_words = words - lag(words),\n         wday = wday(datetime_day, label = T)) |&gt;\n  drop_na() |&gt;\n  group_by(wday) |&gt;\n  summarise(word = sum(daily_words)) |&gt;\n  ggplot(aes(wday, word))+\n    geom_col(fill = \"grey90\")+\n    scale_y_continuous(labels = label_comma())+\n    labs(x = \"day of the week\",\n         y = \"total words\",\n         title = \"words written by day of week\")\n\n\n\n\n\n\nFigure 2: The total number of words I wrote per day of the week.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#thoughts",
    "href": "posts/2022/12/2022-12-10/index.html#thoughts",
    "title": "A lesson in over-preparing",
    "section": "Thoughts",
    "text": "Thoughts\nWhile I feel pretty good about the material I produced, I really don’t think this way of doing things is tenable. The writing that I did do for these course notes was writing that I didn’t do for any other project. Moreover, you can almost see how much energy Sundays took out of me! And even though I wasn’t writing as many course notes Tuesday through Friday, those were full work days that I was doing all of my other work during.\nSo the upshot is: I’m already brainstorming on how to not prepare like this for a class again!",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-10/index.html#sources",
    "href": "posts/2022/12/2022-12-10/index.html#sources",
    "title": "A lesson in over-preparing",
    "section": "Sources",
    "text": "Sources\n\nhttps://drsimonj.svbtle.com/embarking-on-a-tidy-git-analysis\nhttps://tidyr.tidyverse.org/reference/hoist.html",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "A lesson in over-preparing"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html",
    "title": "Github Onboarding with RStudio",
    "section": "",
    "text": "I’m writing this primarily for students I’ll be teaching in Spring 2023 who I want to use Git/Github with Posit Workbench.\n\nThis tutorial is appropriate for:\n\nAnyone using RStudio/RStudio Server/Posit Workbench/Posit Cloud\n\nI will assume:\n\n\nGit is already installed and available.\nYou have not already configured Git locally.\nYou cannot access the terminal.\n\n\n\nIf item number 1 is not correct, or you want more detail on using Git and Github with RStudio, you should check out Jennifer Bryan’s much more extensive Happy Git and GitHub for the useR.\nItem number 2 might be a strange assumption, but the Posit Workbench configuration I have access to actually does not allow opening a terminal.\nInstruction boxes with  should be done on github, and instruction boxes with  should be done in RStudio.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#the-target-audience",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#the-target-audience",
    "title": "Github Onboarding with RStudio",
    "section": "",
    "text": "I’m writing this primarily for students I’ll be teaching in Spring 2023 who I want to use Git/Github with Posit Workbench.\n\nThis tutorial is appropriate for:\n\nAnyone using RStudio/RStudio Server/Posit Workbench/Posit Cloud\n\nI will assume:\n\n\nGit is already installed and available.\nYou have not already configured Git locally.\nYou cannot access the terminal.\n\n\n\nIf item number 1 is not correct, or you want more detail on using Git and Github with RStudio, you should check out Jennifer Bryan’s much more extensive Happy Git and GitHub for the useR.\nItem number 2 might be a strange assumption, but the Posit Workbench configuration I have access to actually does not allow opening a terminal.\nInstruction boxes with  should be done on github, and instruction boxes with  should be done in RStudio.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-1-create-a-github-account",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-1-create-a-github-account",
    "title": "Github Onboarding with RStudio",
    "section": "Step 1: Create a Github Account",
    "text": "Step 1: Create a Github Account\nGo over to Github and create a free account.\n\n\n\n\n\n\nTip\n\n\n\n\nGo To\n\nhttps://github.com/\n\n\n\n\nAs suggested in Happy Git Chapter 4, it would make sense to register an account username that aligns with your professional identity.\nAfter you’ve created your free account, if you are affiliated with a university, I would also suggest applying for the education benefits here: https://education.github.com/. There are a few nice, but not mandatory, perks.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-2-configure-git-in-rstudio",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-2-configure-git-in-rstudio",
    "title": "Github Onboarding with RStudio",
    "section": "Step 2: Configure Git in RStudio",
    "text": "Step 2: Configure Git in RStudio\nNow, you need to tell Git a little bit about yourself on the computer/server you’re using RStudio on.\n\n\n\n\n\n\nTip\n\n\n\n\nGo To\n\nWherever you are using RStudio ( could be Posit Workbench, Posit Cloud, RStudio Server, or RStudio Desktop)\n\nThen Go To\n\nThe Console (a.k.a the R Prompt)\n\n\n\n\n\n\n\n\n\nThe Console in RStudio is here.\n\nNext, we need to tell the local version of Git who you are, specifically your username (which should match your Github username) and your email address (which should match the email address you registered for Github with).\n\n\n\n\n\n\nTip, \n\n\n\nIn the code below, USERNAME should be replaced with your Github username and EMAIL should be replaced with the email you registered your github account with.\n\n\n\nRun this in the R Console:\n\nsystem('git config --global user.name \"USERNAME\"')\nsystem('git config --global user.email \"EMAIL\"')",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-3-configure-rstudio-to-communicate-with-github",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#step-3-configure-rstudio-to-communicate-with-github",
    "title": "Github Onboarding with RStudio",
    "section": "Step 3: Configure RStudio to Communicate with Github",
    "text": "Step 3: Configure RStudio to Communicate with Github\nIn order to be able to push commits from RStudio to Github, you’ll need to set up secure communication between wherever you are using RStudio and Github. I’ll walk you through how to do this with SSH credentials. (See also Happy Git with R for personal access tokens via HTTPS).\nRStudio Configuration\n\n\n\n\n\n\nTip\n\n\n\n\nGo To:\n\nThe Tools menu, then Global Options\n\n\n\n\n\n\n\nThen Go To:\n\nGit/SVN from the left hand side option selector. Its icon is a cardboard box\n\n\n\n\n\n\n\nThen Go To\n\nCreate SSH Key\n\n\n\n\n\n\n\nThen\n\nThe default options should be fine to use. The passphrase here is for the ssh key. It should not be your Github password, or the password for logging into Posit Workbench or Posit Cloud. Once you’re ready, click Create.\n\nThen\n\nAfter creating the SSH key, you should see the option “View Public Key”. Click on it, and copy the text that appears.\n\n\n\n\nThis concludes everything necessary on the RStudio side of things. You should probably keep the session open so that you can come back to re-copy your public key.\nGithub Configuration\nNow, you’ll need to go over to github to add the public key to your profile.\n\n\n\n\n\n\nTip\n\n\n\n\nGo To\n\nYour Github Profile Settings\n\n\n\n\n\n\n\nThen Go To\n\nSSH and GPG keys from the left side menu\n\n\n\n\n\n\n\nThen\n\nClick on the New SSH key button\n\n\n\n\n\n\n\nThen\n\nGive this key an informative name so you can remember which computer it’s coming from.\n\nThen\n\nPaste the text you copied from RStudio into the Key box and click Add SSH Key.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2022/12/2022-12-21_github-onboarding/index.html#configured",
    "href": "posts/2022/12/2022-12-21_github-onboarding/index.html#configured",
    "title": "Github Onboarding with RStudio",
    "section": "Configured",
    "text": "Configured\nNow, wherever you are using RStudio from should be able to push commits to your Github account.",
    "crumbs": [
      "About",
      "Posts",
      "2022",
      "12",
      "Github Onboarding with RStudio"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html",
    "title": "Rising Wooders: Part 2",
    "section": "",
    "text": "This is part 2 of my blog posts to accompany my ADS2023 Poster.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#saying-wooder",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#saying-wooder",
    "title": "Rising Wooders: Part 2",
    "section": "Saying Wooder",
    "text": "Saying Wooder\nAs I said in the previous post, people seem to be very aware now that pronouncing “water” as [wʊɾɚ] is a feature of Philadelphia English, but the surge in written recognition from 2000 onwards isn’t an accurate indicator of when people started pronouncing it that way.\nThe order in which I actually approached this is a little different from how it’s laid out in the poster, but for the sake of blogginess, I’ll describe it in more chronological order. My bias was that this was a very old feature of Philadelphia English, so I started with the oldest records of Philadelphia speech I know of in the Linguistic Linguistic Atlas of the Middle and South Atlantic States (LAMSAS) (Kretzschmar et al. 1993).",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#written-records",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#written-records",
    "title": "Rising Wooders: Part 2",
    "section": "Written Records",
    "text": "Written Records\nLAMSAS was a linguistic atlas project carried out between 1933 and 1974. The interviews had a heavy focus on regional and dialect words, and for the earliest interviews, the only records available are fieldworker transcriptions. In 1939, Guy Lowman interviewed 8 people in Philadelphia, and for one target item, transcribed how they pronounced “glass/tumbler of water.”\n\nLAMSAS Informant Demographics\n\nLAMSAS ID\nGender\nRace\nAge\nYear of Birth\n\n\n\nPA1G\nman\nwhite\n70\n1869\n\n\nPA1A\nman\nwhite\n68\n1871\n\n\nPA1C\nwoman\nwhite\n68\n1871\n\n\nPA1E\nman\nwhite\n68\n1871\n\n\nPA1D\nman\nwhite\n66\n1873\n\n\nPA1F\nman\nwhite\n62\n1877\n\n\nPA1H\nwoman\nwhite\n59\n1880\n\n\nPA1B\nman\nwhite\n43\n1896\n\n\n\nI’m not going to have much more to say about these LAMSAS speakers, though, because they were all transcribed as saying [wɔɾɚ], which would be the same vowel as in wall or walk, not as in wood. In fact, I’ve done some more combing through the LAMSAS records from counties surrounding Philadelphia in both Pennsylvania and New Jersey, and there are no records of [wʊɾɚ].\nWooderless?\nOn the one hand, I was really surprised by this, but I don’t know why I should have been. There’s really no reason why [wʊɾɚ] should have been a long-standing feature of the dialect as long as a century ago. Moreover, Tucker (1944) is often cited as the earliest description of the Philadelphia dialect,1 and he includes a few word-by-word descriptions that ring true based on my own Philly upbringing like:\n\n“spigot, pronounced spicket and commonly used for ‘faucet.’”\n“taffy ‘lollipop,’ known in some places as a ‘sucker.’”\n“yo […] used especially by children in calling one another”\n\nOthers must’ve fallen out of use before my time, and I’m completely unfamiliar with them, such as\n\n“yakers (or yakes or yacks), or yakers on it! ‘I claim it’; also Goods (on it)I ‘I claim a share.’”\n“this after, short for ‘this afternoon.’”\n\nThe point, though, is that he does not mention anything about the word “water” at all. So sometime around the early 1940s, the pronunciation [wʊɾɚ] was either non-existent or used too infrequently for either Tucker or Lowman to note it.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#acoustics",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#acoustics",
    "title": "Rising Wooders: Part 2",
    "section": "Acoustics",
    "text": "Acoustics\nThis is where I would normally turn to acoustic data in the Philadelphia Neighborhood Corpus, but there are a few complications there. First and foremost, I really don’t trust any automated system to be able to place a firm boundary between where the /w/ ends and the vowel begins in a word like “water”, especially because I wouldn’t really trust myself to be able to do so. So, while I do have access to acoustic data for all tokens of “water”, I don’t have a high degree of trust in them as they are.\nThe exciting news is that the longest ago born person in the PNC (1890) overlaps with the latest born person in LAMSAS (1896), meaning we have some degree of data continuity in year of births ranging from 1869 through to 1990. But, the fact that the LAMSAS data only exists in transcription form means I need to do things the old fashioned way and listen to all of these tokens and impressionistically code them.\nSo that’s what I did, starting with the speaker born in 1890.\nWorder!\nI’ve really lucked out that the speaker born in 1890 (interviewed in 1973) talked about water a lot. And, to my surprise, l coded most of his pronunciations as [wɔɾɚ], just like Guy Lowman’s informants from 1939. However, while he never said [wʊɾɚ], there were a bunch of tokens where I could hear an [ɹ] before the flap, and could see a falling F3, characteristic of [ɹ]s as well. Here’s a representative spectrogram.\n\nlibrarieslibrary(tidyverse)\nlibrary(khroma)\nlibrary(scales)\nlibrary(geomtextpath)\nlibrary(ggnewscale)\n\nlibrary(tuneR)\nlibrary(seewave)\n\nlibrary(readtextgrid)\n\nsource(here::here(\"_defaults.R\"))\n\n\n\nloading datawater_wav &lt;- readWave(\"data/water.wav\")\nwater_formant &lt;- read_csv(\"data/water.csv\")\nwater_tg &lt;- read_textgrid(\"data/water.TextGrid\")\n\n\n\nspectrogram processingwater_wav |&gt; \n  spectro(\n    # window length, in terms of samples\n    wl = 0.005 * water_wav@samp.rate, \n    # window overlap\n    ovlp = 90, \n    plot = F\n    ) -&gt; spect\n\n# \"dynamic range\"\ndyn &lt;- -50\n\ncolnames(spect$amp) &lt;- spect$time\nrownames(spect$amp) &lt;- spect$freq\n\nspect_df &lt;- \n  spect$amp |&gt; \n  as_tibble(rownames = \"freq\") |&gt; \n  pivot_longer(-freq, names_to = \"time\", values_to = \"amp\") |&gt; \n  mutate(freq = as.numeric(freq),\n           time = as.numeric(time)) |&gt; \n  # floor at the dynamic range\n  mutate(amp = case_when(amp &lt;= dyn ~ dyn,\n                         TRUE ~ amp)) \n\n\n\ntextgrid processingwater_labels &lt;- \n  water_tg |&gt; \n  filter(tier_num == 1) |&gt; \n  mutate(ipa_label = case_when(text == \"W\" ~ \"wɔ(ɹ)\",\n                               text == \"AO1\" ~ \"wɔ(ɹ)\",\n                               text == \"T\" ~ \"ɾ\",\n                               text == \"ER0\" ~ \"ɚ\")) |&gt; \n  group_by(ipa_label) |&gt; \n  summarise(\n    xmin = min(xmin),\n    xmax = max(xmax)\n    ) |&gt; \n  arrange(xmin) |&gt; \n  rowwise() |&gt; \n  mutate(midpoint = median(c(xmin, xmax)))\n\nwater_boundaries &lt;- \n  water_labels |&gt; \n  select(-midpoint) |&gt; \n  pivot_longer(-ipa_label) |&gt; \n  select(value) |&gt; \n  distinct()\n\n\n\nformant track processingformant_df &lt;- \n  water_formant |&gt; \n  select(time, f1, f2, f3) |&gt; \n  pivot_longer(\n    -time, \n    names_to = \"formant\",\n    values_to = \"hz\"\n    ) |&gt; \n  mutate(\n    formant = toupper(formant),\n    formant_num = str_extract(formant, \"\\\\d\"),\n    formant_num = as.numeric(formant_num)\n    )\n\n\n\nplotting codespect_df |&gt; \n  ggplot(aes(time, freq*1000))+\n    stat_contour(\n      aes(z = amp, fill = after_stat(level)),\n      geom = \"polygon\",\n      bins = 500\n      )+\n    scale_fill_grayC(reverse = T, guide = \"none\")+\n    geom_vline(\n      data = water_boundaries, \n      aes(xintercept = value),\n      linetype = 2, \n      color = \"grey70\"\n      )+\n    geom_text(\n      data = water_labels,\n      aes(\n        x = midpoint, \n        label = ipa_label\n        ),\n      family = \"Fira Sans\",\n      y = 5000,\n      size = 6\n    )+\n    new_scale_fill()+\n    geom_labelline(\n      data = formant_df,\n      aes(\n        x = time,\n        y = hz,\n        fill = formant,\n        color = formant,\n        label = formant,\n        hjust = formant_num/10\n        ),\n      textcolor = \"white\",\n      linewidth = 1.5\n      )+\n    scale_fill_manual(\n      values = c(\"#33BBEE\", \"#EE3377\", \"#009988\"),\n      guide = \"none\"\n    )+\n    scale_color_manual(\n      values = c(\"#33BBEE\", \"#EE3377\", \"#009988\"),\n      guide = \"none\"\n    )+\n    scale_y_continuous(expand = expansion(mult = 0),\n                       labels = label_comma())+\n    coord_cartesian(ylim = c(0, 5500))+\n    labs(\n      x = \"time (s)\", \n      y = \"freq (hz)\",\n      title = '\"water\" spectrogram',\n      subtitle = str_wrap(\n        \"sample token from a speaker born in 1890\n        with a dropping F3 prior to the flap\",\n        width = 40\n        )\n      )+\n    theme(aspect.ratio = 5/8,\n          text = element_text(size = 16),\n          plot.subtitle = element_text(size = 10, color = \"grey80\"),\n          plot.caption = element_text(size = 10, color = \"grey80\"))\n\n\n\n\n\n\nFigure 1: Spectrogram of a token of ‘water’ from the PNC, illustrating a dropping F3 prior to the flap.\n\n\n\n\nThe coding scheme\nThis pronunciation with an [ɹ] hasn’t really been discussed by anyone, but it is reminiscent of how daughter is sometimes pronounced in Philly, as they recently (kinda2) portrayed in the SNL satirization of Mare of Easttown, Murder Durder.\nI had to take this variant into account in my coding. In fact, I think it’s actually key to understanding where wooder came from. So my coding scheme had the following values:\n\nʊ - any high, rounded realization without a [ɹ]\nɔɹ - any mid to high-ish rounded realization with a [ɹ]\nɔ - any lowish to mid rounded realization without an [ɹ]\nɑ - any low, unrounded realization\n\nThis unrounded variant mostly shows up when people are talking about the pronunciation of water, but not exclusively.\nThe model\nSince this is already getting to be a pretty long post, I’ll just skip to the model results! I fit a multilevel multinomial logistic regression model3 with these 4 variants as outcomes.\n\nlibrarieslibrary(brms)\nlibrary(tidybayes)\n\n\n\nload brms modelmodel &lt;- read_rds(\"data/multinomial_linear.rds\")\n\n\n\nget model estimatesnewdat &lt;- tibble(\n  dob = 1869:1990,\n  decade = (dob - 1950)/10\n  )\n\nests &lt;- \n  newdat |&gt; \n  add_epred_draws(model, re_formula = NA) |&gt; \n  group_by(dob, .category) |&gt; \n  median_hdci(.epred, .width = seq(0.2, 0.8, by = 0.1)) |&gt; \n  mutate(code = case_when(.category == \"0_a\" ~ \"ɑ\",\n                          .category == \"1_oh\" ~ \"ɔ\",\n                          .category == \"2_ohr\" ~ \"ɔr\",\n                          .category == \"3_u\" ~ \"ʊ\"), \n         code = as.factor(code) |&gt; fct_rev()) \n\n\n\nplot codeests |&gt; \n ggplot(aes(dob))+\n    geom_ribbon(\n      aes(\n        ymin = .lower, \n        ymax = .upper, \n        group = paste(.width, .category), \n        fill = .category\n        ),\n      alpha = 0.2)+\n    scale_fill_bright(guide = \"none\")+\n    facet_wrap(~code)+\n    scale_y_continuous(\n      expand = expansion(mult = 0), \n      limits = c(0,1), \n      breaks = c(0, 0.5, 1)\n      )+\n    labs(x = \"year of birth\",\n         y = \"vowel probability\",\n         title = \"variant probability\",\n         subtitle = str_wrap(\n           \"the probability of one of four vowel variants in water'\n           being used for different year of birth cohorts, as modeled by a\n           multinomial logistic regression\",\n           width = 60\n           )\n         )+\n    theme(aspect.ratio = 5/8,\n          text = element_text(size = 16),\n          plot.subtitle = element_text(size = 8, color = \"#ebebeb\"),\n         ) \n\n\n\n\n\n\nFigure 2: Modeled estimates of the probability of one of the 4 vowel variants being used in water.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#wooder-on-the-rise",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#wooder-on-the-rise",
    "title": "Rising Wooders: Part 2",
    "section": "Wooder on the rise",
    "text": "Wooder on the rise\nSo it looks like the shift from [wɔɾɚ] to [wʊɾɚ] was shift that occurred in Philadelphia during the 20th century alongside many others. But it also seems like it was not as simple as a categorical pronunciation jump nor a gradual shift from [ɔ] to [ʊ] because we also have this rhotic variant [wɔɹɾɚ] in the mix.\nI actually think the [wɔɹɾɚ] variant is key to figuring out how we got from [wɔɾɚ] to [wʊɾɚ], specifically because of another pattern in Philadelphia English, r-dissimilation, which I’ll hopefully write up for a post tomorrow!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-01_wooder2/index.html#footnotes",
    "href": "posts/2023/01/2023-01-01_wooder2/index.html#footnotes",
    "title": "Rising Wooders: Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\nAlthough, he offers no information for the basis of how he came to describe the dialect. As in, this is more like his collected observations, but he doesn’t even really say that either.↩︎\nWhat’s off about the SNL sketch is they pronounce the first syllable of daughter with the Nurse vowel, [ɚ], when it should be something ranging between [ɔɹ] to [ʊɹ].↩︎\n\nMore specifically, I fit the model using brms::brm like so\nbrm(\n    data = data,\n    family = categorical(\n      link = logit,\n      refcat = \"1_oh\"\n      ),\n    code_param ~ decade + (1|idstring),\n  )\n↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Rising Wooders: Part 2"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html",
    "title": "Making a spectrogram in R",
    "section": "",
    "text": "I might flesh this out as a more detailed tutorial for LingMethodsHub, but for now this is going to be a rough-around-the-edges post about making spectrograms in R. My goal will be to get as close as possible to recreating a spectrogram like you might get from Praat.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#pre-processing.",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#pre-processing.",
    "title": "Making a spectrogram in R",
    "section": "Pre-processing.",
    "text": "Pre-processing.\nTo keep things simple, I grabbed vowel audio clip from the Wikipedia IPA vowel chat with audio (audio info and license).\nYour browser does not support the audio element. \nExplanation of a hacky thing I had to do.\nI know tuneR package has a readWave() function, and I couldn’t figure out how to read in an Oog file, so step 1 was converting the Oog to a wav file. Since I’m writing this in a quarto notebook, I thought I should be able to drop in a ```{sh} code chunk, but it seems like doesn’t have access to my PATH. Long story short, that’s why I’ve got this kind of goofy R code chunk with system() and the full path to sox.\n\nlibrary(glue)\n\nsource(here::here(\"_defaults.R\"))\n\n\nogg_file &lt;- \"assets/Close_front_unrounded_vowel.ogg\"\nwav_file &lt;- \"assets/Close_front_unrounded_vowel.wav\"\nsystem(glue(\"/opt/homebrew/bin/sox {ogg_file} {wav_file}\"))",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#loading-the-audio-file",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#loading-the-audio-file",
    "title": "Making a spectrogram in R",
    "section": "Loading the audio file",
    "text": "Loading the audio file\nThe seewave package, which I’m using to make the spectrogram, takes the sound objects created by tuneR, so that’s what I’ll use for reading in the audio file.\n\nlibrary(tuneR)\nlibrary(seewave)\n\n\ni_wav &lt;- readWave(\"assets/Close_front_unrounded_vowel.wav\")\n\nTo get a sense of what information is in the wav file, you can use str()\n\nstr(i_wav)\n\nFormal class 'Wave' [package \"tuneR\"] with 6 slots\n  ..@ left     : int [1:26524] -2 16 42 24 33 53 56 68 51 55 ...\n  ..@ right    : num(0) \n  ..@ stereo   : logi FALSE\n  ..@ samp.rate: int 44100\n  ..@ bit      : int 16\n  ..@ pcm      : logi TRUE\n\n\nSince I’m going to be zooming in to 0 to 5,000 Hz on the spectrogram, I’ll downsample the audio to 10000.\n\ni_wav_d &lt;- downsample(i_wav, 10000)",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#computing-the-spectrogram",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#computing-the-spectrogram",
    "title": "Making a spectrogram in R",
    "section": "Computing the spectrogram",
    "text": "Computing the spectrogram\nThe function to compute the spectrogram is seewave::spectro(). Its argument names are formatted in a way I find a bit grating. A lot of them are compressed down to single characters or other abbreviations that require having the docs constantly open.\nAnyway, the arguments that seem most important are:\n\nwl\n\nwindow length in samples\n\nwn\n\nwindow function, defaulting to Hanning\n\novlp\n\nWindow overlap, in percentage. That is, a 25% overlap between analysis windows should be passed to ovlp as 25.\n\n\nPraat defaults\nLet’s have a look at the Praat spectrogram defaults\n\n\n\n\n\nFigure 2: Praat defaults for spectrograms\n\n\nHere’s a quick illustration of what these defaults correspond to. It takes an analysis window that’s 0.005 seconds long, and moves it over time by 0.002 second increments. Also, the data coming into the analysis window is weighted by a Gaussian distribution.\n\nlibrary(tidyverse)\n\n\nPlot Codewin_len &lt;- 0.005\ntime_step &lt;- 0.002\ntibble(\n  center = seq(win_len/2, 0.02, by = time_step),\n  left_edge = center - (win_len/2),\n  right_edge = center + (win_len/2),\n  win_num = seq_along(center)\n) -&gt; window_fig\nwindow_fig |&gt; \n  ggplot(aes(x = center, y = win_num))+\n    geom_pointrange(\n      aes(\n        xmin = left_edge,\n        xmax = right_edge\n      ),\n      size = 2,\n      linewidth = 2\n    )+\n  labs(x = \"time\",\n       y = NULL,\n       title = \"Overlapping spectrogram window illustration\")+\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n\n\nFigure 3: An illustration of overlapping analysis windows that produce a spectrogram\n\n\n\n\nThe seewave::spectro() function defines this same relationship, except instead of the time step or window hop length, we need to define by what % the windows overlap. We also need to express how wide the windows are in terms of audio sample, rather than in terms of time, but that just requires multiplying the desired time width by the sampling rate.\n\nwin_len &lt;- 0.005 * i_wav_d@samp.rate\nhop_len &lt;- 0.002 * i_wav_d@samp.rate\noverlap &lt;- ((win_len - hop_len) / win_len) * 100\n\nThe one thing that I can’t recreate for now is the Gaussian window function. seewave doesn’t have it implemented, so I’ll just stick with its default (Hamming)\nComputing the spectrogram\nNow, it’s a pretty straightforward call to spectro().\n\nspect &lt;-\n  i_wav_d |&gt;\n  spectro(\n    # window length, in terms of samples\n    wl = win_len,\n    # window overlap\n    ovlp = overlap,\n    # don't plot the result\n    plot = F\n    )\n\nThe spect object is a list with three named items\n\n$time\n\na vector corresponding to the time domain\n\n$freq\n\na vector corresponding to the frequency domain\n\n$amp\n\na matrix of amplitudes across the time and frequency domains\n\n\n\nglimpse(spect)\n\nList of 3\n $ time: num [1:299] 0 0.00202 0.00404 0.00606 0.00807 ...\n $ freq: num [1:25] 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 ...\n $ amp : num [1:25, 1:299] -35.1 -39.5 -51.3 -49.9 -51.7 ...\n\n\nTidying up\nIn order to make a plot of the spectrogram in ggplot2, we need to do some tidying up. I’ll go about this by setting the row and column names of the spect$amp matrix to the frequency and time domain values, converting it to a data frame, then doing some pivoting.\n\n# set the colnames and rownames\ncolnames(spect$amp) &lt;- spect$time\nrownames(spect$amp) &lt;- spect$freq\n\n\nspect_df &lt;-\n  spect$amp |&gt;\n  # coerce the row names to a column\n  as_tibble(rownames = \"freq\") |&gt;\n  # pivot to long format\n  pivot_longer(\n    # all columns except freq\n    -freq, \n    names_to = \"time\", \n    values_to = \"amp\"\n  ) |&gt;\n  # since they were names before,\n  # freq and time need conversion to numeric\n  mutate(\n    freq = as.numeric(freq),\n    time = as.numeric(time)\n  )\n\n\nsummary(spect_df)\n\n      freq          time             amp        \n Min.   :0.0   Min.   :0.0000   Min.   :-89.94  \n 1st Qu.:1.2   1st Qu.:0.1494   1st Qu.:-45.17  \n Median :2.4   Median :0.3008   Median :-35.85  \n Mean   :2.4   Mean   :0.3008   Mean   :-35.43  \n 3rd Qu.:3.6   3rd Qu.:0.4521   3rd Qu.:-26.69  \n Max.   :4.8   Max.   :0.6015   Max.   :  0.00  \n\n\n“Dynamic Range”\nFrequency data is represented in terms of kHz, which I’ll leave alone for now. One last thing we need to re-recreate from Praat is the “dynamic range”. All values below some cut off (by default, 50 below the maximum) are plotted with the same color. We can do that with some data frame operations here.\n\ndyn = -50\nspect_df_floor &lt;- \n  spect_df |&gt; \n  mutate(\n    amp_floor = case_when(\n      amp &lt; dyn ~ dyn,\n      TRUE ~ amp  \n    )\n  )",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#plotting-the-spectrogram",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#plotting-the-spectrogram",
    "title": "Making a spectrogram in R",
    "section": "Plotting the spectrogram",
    "text": "Plotting the spectrogram\nNow what’s left is to plot the thing. I’ll load the khroma package in order to get some nice color scales.\n\nlibrary(khroma)\n\nBasic raster plot\nAs a first step, we can plot the time by frequency data as a raster plot, with little rectangles for each position filled in with their amplitude.\n\nspect_df_floor |&gt; \n  ggplot(aes(time, freq))+\n    geom_raster(aes(fill = amp_floor))+\n    guides(fill = \"none\")+\n    labs(\n      x = \"time (s)\",\n      y = \"frequency (kHz)\",\n      title = \"spectrogram raster plot\"\n    )\n\n\n\n\n\n\nFigure 4: A raster spectrogram plot\n\n\n\n\nSpectrogram contour plot\nTo get closer to the Praat output, though, we need to make a contour plot instead. Here’s where I’m getting a bit stymied. I wind up with these weird diagonal “shadows” on the right and left hand side of the spectrogram, which I think are a result of how the stat_contour() is being computed and plotted, rather than anything to do with the actual spectrogram.\n\nspect_df_floor |&gt; \n  ggplot(aes(time, freq))+\n    stat_contour(\n      aes(\n        z = amp_floor,\n        fill = after_stat(level)\n      ),\n      geom = \"polygon\",\n      bins = 300\n    )+\n    scale_fill_batlow()+\n    guides(fill = \"none\")+\n    labs(\n      x = \"time (s)\",\n      y = \"frequency (kHz)\",\n      title = \"spectrogram contour plot\"\n    )\n\n\n\n\n\n\nFigure 5: A contour spectrogram plot\n\n\n\n\nOne way around this I’ve found is to compute the spectrogram on the higher sampling rate audio, and then zoom into the frequency range you want.\n\nre-running the spectrogramwin_len &lt;- 0.005 * i_wav@samp.rate\nhop_len &lt;- 0.002 * i_wav@samp.rate\noverlap &lt;- ((win_len - hop_len) / win_len) * 100\n\nspect2 &lt;-\n  i_wav |&gt;\n  spectro(\n    # window length, in terms of samples\n    wl = win_len,\n    # window overlap\n    ovlp = overlap,\n    # don't plot the result\n    plot = F\n    )\n\n# set the colnames and rownames\ncolnames(spect2$amp) &lt;- spect2$time\nrownames(spect2$amp) &lt;- spect2$freq\n\nspect2_df &lt;-\n  spect2$amp |&gt;\n  # coerce the row names to a column\n  as_tibble(rownames = \"freq\") |&gt;\n  # pivot to long format\n  pivot_longer(\n    # all columns except freq\n    -freq, \n    names_to = \"time\", \n    values_to = \"amp\"\n  ) |&gt;\n  # since they were names before,\n  # freq and time need conversion to numeric\n  mutate(\n    freq = as.numeric(freq),\n    time = as.numeric(time)\n  )\n\ndyn = -50\nspect2_df_floor &lt;- \n  spect2_df |&gt; \n  mutate(\n    amp_floor = case_when(\n      amp &lt; dyn ~ dyn,\n      TRUE ~ amp  \n    )\n  )\n\n\n\nspect2_df_floor |&gt; \n  ggplot(aes(time, freq))+\n    stat_contour(\n      aes(\n        z = amp_floor,\n        fill = after_stat(level)\n      ),\n      geom = \"polygon\",\n      bins = 300\n    )+\n    scale_fill_batlow()+\n    guides(fill = \"none\")+\n    labs(\n      x = \"time (s)\",\n      y = \"frequency (kHz)\",\n      title = \"spectrogram contour plot\"\n    )+\n    coord_cartesian(ylim = c(0, 5))\n\n\n\n\n\n\nFigure 6: A contour spectrogram plot\n\n\n\n\nI think this might not be necessary if I had a better handle on stat_contour() but for now, it does the trick!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-22_r-spect/index.html#update",
    "href": "posts/2023/01/2023-01-22_r-spect/index.html#update",
    "title": "Making a spectrogram in R",
    "section": "Update",
    "text": "Update\nDue to popular request, another attempt at showing the degree of window overlap.\n\n\n\nThis is fantastic, jo, thx! One quick q, I’m wondering if there might be a better way to visualize overlapping windows. The image maybe implies some y dimension (windows increasing in some vertical space). I’m just putting myself in a 1st yrs shoes in my speech class\n\n— Chandan (@GutStrings) January 22, 2023\n\nI’ve folded the code here, just because it’s medium gnarly. The short version is you can get the weights for the window functions from seewave::ftwindow(), and then I used geom_area() to plot it.\n\nthe data and plotting codewindow_fig |&gt; \n  group_by(win_num) |&gt; \n  nest() |&gt; \n  mutate(\n    dists = map(\n      data, \n      \\(df){\n        tibble(\n          time = seq(df$left_edge, df$right_edge, len = 512),\n          weight = ftwindow(512)\n        )\n      }\n    )\n  ) |&gt; \n  unnest(dists) -&gt; window_weights\n\nwindow_weights |&gt; \n  ggplot(aes(time, weight, group = win_num)) +\n    geom_area(\n      aes(\n        group = win_num, \n        fill = win_num,\n        color = win_num\n      ), \n      position = \"identity\", \n      alpha = 0.5)+\n    scale_fill_hawaii(guide = \"none\")+\n    scale_color_hawaii(guide = \"none\")+\n  labs(title = \"Hamming window functions\",\n       subtitle = \"window length = 0.005, time step =0.002\")\n\n\n\n\n\n\nFigure 7: Hamming window functions",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a spectrogram in R"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html",
    "title": "Making a Plotly Plot",
    "section": "",
    "text": "I’m a bit nervous about investing time into an interactive plotting framework after getting burned by Google Motion Charts.1 But, plotly seems to work even offline, which I think means once I’ve installed it, it doesn’t depend on a service or code hosted by the plotly company. That makes me feel a little more confident. I’d like to build some animations in it, but that means learning how it works, so here I go!\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(palmerpenguins)",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#basic-scatter.",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#basic-scatter.",
    "title": "Making a Plotly Plot",
    "section": "Basic scatter.",
    "text": "Basic scatter.\nFollowing the book and the docs, it looks like if I were to take the “layers” analogy to building a plot, the most basic layer function is going to be plotly::add_trace(). Data gets mapped to plot aesthetics with function notation.\n\nplot_ly(\n  data = penguins,\n  x = ~bill_length_mm,\n  y = ~bill_depth_mm,\n  color = ~species\n) |&gt; \n  add_trace(\n    type = \"scatter\",\n    mode = \"markers\",\n    size = 4\n  )\n\n\n\n\n\nSome thoughts:\n\nI think the type argument defines the kind of “space” the plot is placed in? Putting in an unsupported type returns a pretty diverse set of options that’s leaving me a bit confused about the exact work this argument does.\nI think mode is how you go about defining the plotted geometry, with \"markers\" being points.\nIt’s nice how the points default to ColorBrewer Dark2 with a slight transparency for overplotting.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#theming",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#theming",
    "title": "Making a Plotly Plot",
    "section": "Theming",
    "text": "Theming\nIt looks like the approach to theming is to just set everything by hand in plotly::layout(). This took a little bit of messing around with to find what all of the various parameters are called in plotly. My ggplot2::theme() translations are:\n\n\nggplot2\nplotly\n\n\n\nplot.background\npaper_bgcolor\n\n\npanel.background\nplot_bgcolor\n\n\npanel.grid.major.[x,y]\n[xy]axis\n\n\n\nAnother thing to note is that to get a transparent layer, you need to give it a hex code with 00 transparency at the end, rather than an NA or NULL value.\nI have a few of these colors defined in my blog .Rprofile.\n\nplot_bg\n\n[1] \"#122F4A\"\n\nmajor\n\n[1] \"#4C5D75\"\n\nminor\n\n[1] \"#31455E\"\n\n\nAlso, to get the right font family, you have to reference fonts you’ve imported in the html page, rather than fonts imported into R with showtext.\nHere it is in plotly.\n\nplot_ly() |&gt; \n  add_trace(\n    data = penguins,\n    x = ~bill_length_mm,\n    y = ~bill_depth_mm,\n    color = ~species,\n    type = \"scatter\",\n    mode = \"markers\"\n  ) |&gt; \n  layout(\n    plot_bgcolor = \"#ffffff00\",\n    paper_bgcolor = plot_bg,\n    font = list(\n      family = \"Fira Sans\",\n      color = \"#fff\"\n    ), \n    xaxis = list(\n      gridcolor = minor\n    ),\n    yaxis = list(\n      gridcolor = minor\n    )    \n  )",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#ggplotly",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#ggplotly",
    "title": "Making a Plotly Plot",
    "section": "ggplotly",
    "text": "ggplotly\nplotly also has a the ability to convert ggplot2 plots into plotly plots, at least somewhat. Here’s how it does by default.\n\npenguin_plot &lt;- \n  ggplot(\n    data = penguins, \n    aes(x = bill_length_mm, \n        y = bill_depth_mm, \n        color = species\n      )\n  ) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n\nggplotly(penguin_plot)\n\n\n\n\n\nSo, it looks like the panel.background = element_blank() I set in my blog theme doesn’t translate over in the conversion. Which is honestly a good illustration of why its probably worth learning a little bit about how the actual plotly system works, even if you’re going to mostly be interacting with it through plotly::ggplotly() like I am\n\nggplotly(penguin_plot) |&gt; \n  layout(\n    plot_bgcolor = \"#ffffff00\"\n  )",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/01/2023-01-29_starting-plotly/index.html#footnotes",
    "href": "posts/2023/01/2023-01-29_starting-plotly/index.html#footnotes",
    "title": "Making a Plotly Plot",
    "section": "Footnotes",
    "text": "Footnotes\n\nReally, it’s the deprecation of Flash, but Google never updated how the motion charts work.↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "01",
      "Making a Plotly Plot"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html",
    "title": "Simulating DND Rolls",
    "section": "",
    "text": "I’ve recently started playing Dungeons and Dragons, and have been really enjoying the campaign my sibling runs. I’m still getting a handle on the mechanics, especially in combat, where the sequence of events that are allowed, and keeping track of your what you roll when is still a little confusing to me. Even though it’s not playing out in real time, it feels urgent, and I don’t always keep track of things like “Am I rolling with advantage?”, “Do I have bardic inspiration” etc.\nBut in the time in between sessions, in addition to thinking through the mechanics to remember, I’ve also been thinking about the probabilities of it all. And what do you know! There’s an R package for that: {droll} !",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#simulating-rolls",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#simulating-rolls",
    "title": "Simulating DND Rolls",
    "section": "Simulating rolls",
    "text": "Simulating rolls\n\nlibrary(tidyverse)\nlibrary(geomtextpath)\nlibrary(khroma)\nlibrary(droll)\n\nsource(here::here(\"_defaults.R\"))\n\nThe {droll} package works seems to be explicitly built to compatible with the DnD directions work. For example, you might roll a 20 sided die, or a “d20”, and add an ability “modifier” to the result. In droll commands, we’ll create a d20, set a dexterity modifier, then roll a random value then add that modifier:\n\nset.seed(12)\n\n\n# make a d20\nd20 &lt;- d(20)\n# low dex\ndex &lt;- 1\nd20 + dex\n\n[1] 3\n\n\nAnother thing you might do is roll multiple dice, then add the result together. For example “roll 3d8” means you roll three 8-sided dice, then add the result together for something to happen.\n\nd8 &lt;- d(8)\n3 * d8\n\n[1] 17",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#distributions",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#distributions",
    "title": "Simulating DND Rolls",
    "section": "Distributions",
    "text": "Distributions\nIt also comes with a few probability distributions built to get the density, cumulative probability, and quantiles of die, which might already be familiar to some R users. Here’s the density distributions of rolling 1, 2, 3, and 4 d8s.\n\n## A function to make a tibble\n## of n rolls of a die.\nmake_roll_tibble &lt;- function(n, die){\n  nfaces &lt;- max(die@faces)\n  tibble(\n    rolls = n:(n*nfaces),\n    faces = nfaces,\n    density = droll(rolls, n*die)\n  )\n}\n\n\n## Constructing the roll densities\ntibble(n = 1:4) |&gt; \n  mutate(\n    roll_df = map(\n      n,\n      # new R anonymous function\n      \\(n) make_roll_tibble(n, d8)\n    )\n  ) |&gt; \n  unnest(roll_df) -&gt; \n  roll_densities\n\n\n## plotting the roll densities\nroll_densities |&gt; \n  mutate(\n    nd = str_c(n, \"d\", faces)\n  ) |&gt; \n  ggplot(aes(rolls, density))+\n    geom_area(fill = \"grey90\")+\n    expand_limits(x = 1)+\n    facet_wrap(\n      ~nd, \n      scales = \"free_x\"\n      )+\n  labs(\n    title = \"Density distributions of 1 through 4 d8 rolls\"\n  )\n\n\n\n\n\n\nFigure 1: Density distributions of nd8 rolls",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#advantage-vs-disadvantage",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#advantage-vs-disadvantage",
    "title": "Simulating DND Rolls",
    "section": "Advantage vs Disadvantage",
    "text": "Advantage vs Disadvantage\nOne mechanic in DnD is rolling with “Advantage” vs rolling with “Disadvantage”. If you have advantage (say, because an enemy is restrained), you roll two d20s and take the highest value. If you roll with disadvantage (say, because you are restrained), you roll two d20s and take the lowest value.\nThere’s not a straightforward way to get the advantage vs disadvantage rolls, but I figured out a way to do with with some tidyverse tricks.\n\n## Set up the number of rolls\nnsims = 10000\nnrolls = nsims * 2\n\n## Initial tibble with \n## random rolls\ntibble(\n  roll_id = 0:(nrolls-1),\n  roll_value = rroll(nrolls, d20)\n) |&gt; \n  ## convert to roll groups\n  mutate(\n    roll_group = floor(roll_id/2)\n  ) |&gt; \n  ## group\n  group_by(roll_group) |&gt; \n  ## number the rolls\n  mutate(\n    roll_num = row_number()\n  ) |&gt; \n  ## Get advantage, \n  ## disadvantage\n  ## and first roll\n  summarise(\n    advantage = max(roll_value),\n    disadvantage = min(roll_value),\n    normal = roll_value[1]\n  ) -&gt; \n  simulated_rolls\n\nhead(simulated_rolls)\n\n# A tibble: 6 × 4\n  roll_group advantage disadvantage normal\n       &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1          0        12            3     12\n2          1        16            2     16\n3          2        17            5     17\n4          3        16           12     16\n5          4         8            2      2\n6          5        11            1      1\n\n\nNext step is to count up how many of each value we got, which requires pivoting.\n\nsimulated_rolls |&gt; \n  pivot_longer(\n    cols = advantage:normal,\n    names_to = \"roll_type\",\n    values_to = \"roll_value\"\n  ) -&gt;\n  rolls_long\n\nhead(rolls_long)\n\n# A tibble: 6 × 3\n  roll_group roll_type    roll_value\n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1          0 advantage            12\n2          0 disadvantage          3\n3          0 normal               12\n4          1 advantage            16\n5          1 disadvantage          2\n6          1 normal               16\n\n\nAfter pivoting long, I’ll calculate the cumulative probability that a player will pass the skill check.\n\nrolls_long |&gt; \n  count(roll_type, roll_value) |&gt; \n  arrange(desc(roll_value)) |&gt; \n  mutate(\n    .by = roll_type,\n    prob = cumsum(n)/sum(n)\n  )  -&gt; \n  check_prob\nhead(check_prob)\n\n# A tibble: 6 × 4\n  roll_type    roll_value     n   prob\n  &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1 advantage            20   979 0.0979\n2 disadvantage         20    31 0.0031\n3 normal               20   515 0.0515\n4 advantage            19   884 0.186 \n5 disadvantage         19    86 0.0117\n6 normal               19   503 0.102 \n\n\nLast thing to do is make a plot!\n\ncheck_prob |&gt; \n  ggplot(aes(roll_value, prob, color = roll_type))+\n    geom_textpath(\n      aes(label = roll_type),\n      linewidth = 2\n    )+\n    scale_x_continuous(\n      breaks = c(5, 10, 15, 20),\n      minor_breaks = c(\n        1:4,\n        6:9,\n        11:14,\n        16:19\n      )\n    )+\n    scale_color_manual(\n      values = c(\"#b59e54\", \"#AB6dac\",\"#c73032\" )\n    )+\n    guides(\n      color = \"none\"\n    )+\n    labs(\n      title = \"Probability of passing a skill check, no modifier\",\n      x = \"Difficulty class\"\n    )\n\n\n\n\n\n\nFigure 2: Cumulative probability density functions",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/02/2023-02-12_dndroll/index.html#closing-thoughts",
    "href": "posts/2023/02/2023-02-12_dndroll/index.html#closing-thoughts",
    "title": "Simulating DND Rolls",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nOne thought I had, while writing this post, was how the different reasons for doing these dice rolls in the game affected the kind probability plot I made. Most often you’ll be rolling 3d8 in order to calculate how much damage you’re going to do, so for that plot what you want to know what the point probabilities of each outcome is, hence the density functions.\nFor rolling d20s with advantage or disadvantage, you’re wanting to see what the probability is that you’ll pass the skill check, that is, that you’ll roll at least some value, hence the inverse cumulative probability distributions!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "02",
      "Simulating DND Rolls"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html",
    "title": "Changing Project Defaults",
    "section": "",
    "text": "I’ve moved a bunch of R defaults that I want for each post from .Rprofile into _defaults.R, and now run source(here::here(\"_defaults.R\")) in each post where I want them. That looks like more work, but it actually makes things run a bit faster with the way Quarto runs R and freezes outputs.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#upshot",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#upshot",
    "title": "Changing Project Defaults",
    "section": "",
    "text": "I’ve moved a bunch of R defaults that I want for each post from .Rprofile into _defaults.R, and now run source(here::here(\"_defaults.R\")) in each post where I want them. That looks like more work, but it actually makes things run a bit faster with the way Quarto runs R and freezes outputs.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#initial-defaults",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#initial-defaults",
    "title": "Changing Project Defaults",
    "section": "Initial Defaults",
    "text": "Initial Defaults\nAround when I was setting up this blog project, I decided that I wanted some consistent theming for the figures so that they would fit in nicely into the rest of the blog, but I didn’t want to have to include a megablock of code in every post that looked like this:\n\nlibrary(ggplot2)\nlibrary(khroma)\nlibrary(ggdark)\nlibrary(showtext)\nlibrary(colorspace)\n\n# get Fira Sans from google\nfont_add_google(name = \"Fira Sans\", family = \"Fira Sans\")\nshowtext_auto()\nbody_bg &lt;- \"#222222\"\nplot_bg &lt;- darken(\"#375a7f\", 0.50)\n\nmajor &lt;- lighten(\n  plot_bg,\n  amount = 0.25\n)\n\nminor &lt;- lighten(\n  plot_bg,\n  amount = 0.125\n)\n\nstrip_bg &lt;- lighten(plot_bg, 0.5)\n\ntheme_set(dark_theme_gray(base_size = 12) + \n            theme(text = element_text(family = \"Fira Sans\"),\n                  plot.background = element_rect(fill = plot_bg),\n                  panel.background = element_blank(),\n                  panel.grid.major = element_line(color = major, linewidth = 0.2),\n                  panel.grid.minor = element_line(color = minor, linewidth = 0.2),\n                  legend.key = element_blank(),\n                  strip.background = element_rect(fill = strip_bg),\n                  strip.text = element_text(color = body_bg),\n                  axis.ticks = element_blank(),\n                  legend.background = element_blank()))\n\noptions(\n  ggplot2.discrete.colour = khroma::scale_color_bright,\n  ggplot2.discrete.fill = khroma::scale_fill_bright,\n  ggplot2.continuous.colour = khroma::scale_color_batlow,\n  ggplot2.continuous.fill = khroma::scale_fill_batlow\n)\n\nAll that means I can just do some minimal ggplot2 code in each post and it’ll look something like this:\n\ndata(penguins, package = \"palmerpenguins\")\n\nggplot(\n  penguins, \n  aes(\n    x = bill_length_mm,\n    y = bill_depth_mm,\n    color = species\n  )\n)+\n  geom_point()\n\n\n\n\n\n\n\nSo, I stuck that big block of code into the .Rprofile for the blog project so that every time I opened the project, R would automatically source it. Nice, right?",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#the-heaviness-of-.rprofile",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#the-heaviness-of-.rprofile",
    "title": "Changing Project Defaults",
    "section": "The heaviness of .Rprofile\n",
    "text": "The heaviness of .Rprofile\n\nI started realizing this wasn’t optimal every time I re-rendered the blog for a new post. I have my quarto set to “freeze” each post after it’s rendered, meaning it won’t re-run all of the R code in a post unless I make a change to it, instead using the output of the previous time it ran. That’s a time saver, cause even with many very simple posts with code, it just takes a while to run everything.\nThe issue was, even with freeze: true, Quarto would still source .Rprofile on every post. Which means that big block of code, including the call to showtext::font_add_google() would run for every post when I re-rendered the blog. And that was starting to get tedious!",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#moving-to-_defaults.r",
    "href": "posts/2023/06/2023-06-16_changing_blog_defaults/index.html#moving-to-_defaults.r",
    "title": "Changing Project Defaults",
    "section": "Moving to _defaults.R\n",
    "text": "Moving to _defaults.R\n\nSo, I moved all of the customization code from .Rprofile into _defaults.R file. I forget where I saw a _defaults.R first, but I think it was in some repository maintained by Hadley Wickham. The downside is that it’s not as automatic as .Rprofile, in that I need to source it at the start of every post. That would be annoying if I was going to write the path out by hand, but it’s a little easier with here::here().\n\nsource(\n  here::here(\"_defaults.R\")\n)\n\nThe major upside, though, is that sourcing code gets frozen along with all of the other code chunks in a post! So when I re-render the whole blog, Quarto won’t re-run all of the code in _defaults.R unless the code has changed in a post. Overall, it feels worth it!.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Changing Project Defaults"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "",
    "text": "I’m still thinking about priors, distributions, and logistic regressions. The fact that a fairly broad normal distribution in logit space turns into a bimodal distribution in probability space has got me thinking about the standard deviation of random effects in logistic regression. Specifically, what happens in cases where the population of individuals may be bimodal\nsetuplibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(marginaleffects)\n\nlibrary(gt)\n\nsource(here::here(\"_defaults.R\"))\n\nseed &lt;- 2023-6-29\n\nset.seed(seed)",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#simulating-some-data",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#simulating-some-data",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Simulating some data",
    "text": "Simulating some data\nI’ll kick things off with simulating some data. Our predictor variable will be just randomly sampled from ~normal(0,1), so it’ll handily already be z-scored. I’ll also go for a slope in logit space of 1, so logit(y) = x.\nI’ll treat each point I’ve sampled for X as belonging to an individual, or subject, grouping variable, and each individual’s personal probability will be sampled from a beta distribution. I’ll simulate two possibilities here, one where individuals are kind of closely clustered near each other, and another where they’re pretty strongly bifurcated close to 0 and 1.1\n\nsimulation of individualstibble(\n  x = rnorm(100),\n  logit = x,\n  prob = plogis(logit)\n) |&gt; \n  mutate(\n    individual = row_number(),\n    subj_prob_low = rbeta(\n      n = n(), \n      prob * 0.25, \n      (1-prob) * 0.25\n    ),\n    subj_prob_high = rbeta(\n      n = n(), \n      prob * 7, \n      (1-prob) * 7\n      )\n  )-&gt;\n  sim_params\n\n\n\nplotting codesim_params |&gt; \n  pivot_longer(\n    starts_with(\"subj_prob\")\n  ) |&gt; \n  mutate(\n    name = case_when(\n      name == \"subj_prob_high\" ~ \"unimodal\",\n      name == \"subj_prob_low\" ~ \"bifurcated\"\n    )\n  ) |&gt; \n  ggplot(aes(x, value))+\n    geom_point()+\n    labs(title = \"subject-level probabilities\",\n         y = \"prob\")+\n    scale_x_continuous(\n      breaks = seq(-2,2, by = 2)\n    ) +  \n    facet_wrap(~name)\n\n\n\n\n\n\nFigure 1: Probabilities for individuals\n\n\n\n\nFor each of these probabilities, I’ll simulate 50 binomial observations.\n\nsimulating utterancessim_params |&gt; \n  rowwise() |&gt; \n  mutate(\n    obs = list(tibble(\n      y_prob_low = rbinom(n = 50, size = 1, prob = subj_prob_low),\n      y_prob_high = rbinom(n = 50, size = 1, prob = subj_prob_high)\n    ))\n  ) |&gt; \n  unnest(obs) -&gt;\n  sim_obs\n\n\n\nplotting codesim_obs |&gt; \n  pivot_longer(\n    starts_with(\"y_\"),\n    names_to = \"simulation\",\n    values_to = \"observation\"\n  ) |&gt; \n  mutate(\n    simulation = case_when(\n      simulation == \"y_prob_high\" ~ \"unimodal\",\n      simulation == \"y_prob_low\" ~ \"bifurcated\"\n    )\n  ) |&gt; \n  ggplot(aes(x, factor(observation))) +\n    stat_sum(alpha = 0.3)+\n    labs(title = \"simulated observations\")+\n    scale_x_continuous(\n      breaks = seq(-2,2, by = 2)\n    ) +\n    facet_wrap(~simulation)\n\n\n\n\n\n\nFigure 2: simulated observations",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#looking-at-the-default-priors",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#looking-at-the-default-priors",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Looking at the default priors",
    "text": "Looking at the default priors\nIf we take a look at the default priors a logistic model would get in brms, we can see that both the Intercept and the slope get pretty broad priors (the blank prior for the slope means it’s a flat prior).\n\nget_prior(\n  bf(y_prob_low ~ x + (1|individual)),\n  data = sim_obs,\n  family = bernoulli(link = \"logit\")\n) |&gt; \n  as_tibble() |&gt; \n  select(prior, class, coef, group) |&gt; \n  gt()\n\n\n\n\n\nprior\nclass\ncoef\ngroup\n\n\n\n\nb\n\n\n\n\n\nb\nx\n\n\n\nstudent_t(3, 0, 2.5)\nIntercept\n\n\n\n\nstudent_t(3, 0, 2.5)\nsd\n\n\n\n\n\nsd\n\nindividual\n\n\n\nsd\nIntercept\nindividual\n\n\n\n\n\n\nIf we real quick look at how the prior on the intercept plays out in the probability space, we get one of these bimodal distributions.\n\nplotting codetibble(\n  x = rstudent_t(1e6, df = 3, sigma = 2.5)\n) |&gt; \n  ggplot(aes(plogis(x))) +\n    stat_slab(fill = ptol_red)+\n    theme_no_y()+\n    scale_y_continuous(\n      expand = expansion(mult = 0)\n    )+\n    labs(title = \"invlogit(student_t(3, 0, 2.5))\",\n         x = NULL)\n\n\n\n\n\n\nFigure 3: Student-t prior in the probability space.\n\n\n\n\nSo, for the intercept and slope priors, I’ll adjust them to be ~normal(0, 1.5) and ~normal(0,1), respectively.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#actually-fitting-the-models.",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#actually-fitting-the-models.",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Actually fitting the models.",
    "text": "Actually fitting the models.\nBimodal population\nFirst, here’s the model for the population where individuals’ probabilities were squished out towards 0 and 1.\n\nlow_mod &lt;- brm(\n  y_prob_low ~ x + (1|individual),\n  data = sim_obs,\n  family = bernoulli(link = \"logit\"),\n  prior = c(\n    prior(normal(0,1.5), class = \"Intercept\"),\n    prior(normal(0,1), class = \"b\", coef = \"x\")\n  ),\n  cores = 4, \n  seed = seed,  \n  backend = \"cmdstanr\",\n  file = \"low_mod.RDS\"\n) \n\n\nsummary tablelow_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mean_hdci() |&gt; \n  select(.variable, .value, .lower, .upper) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n.variable\n.value\n.lower\n.upper\n\n\n\nb_Intercept\n0.60\n−1.04\n2.13\n\n\nb_x\n3.01\n1.62\n4.36\n\n\nsd_individual__Intercept\n7.85\n5.85\n10.21\n\n\n\n\n\n\nThings have kind of clearly gone off the rails here. The intercepts and slopes are all over the place, but that’s maybe not surprising given the trade offs the model is making between the population level slopes and the individual level probabilities. It’s worth noting that this model had no diagnostic warnings and was well converged.\n\nplotting codelow_mod |&gt; \n  predictions(\n    newdata = datagrid(x = seq(-3, 3, length = 100)),\n    re_formula = NA\n  ) |&gt;\n  posterior_draws() |&gt; \n  ggplot(aes(x, draw)) +\n    stat_lineribbon(linewidth = 0.5)+\n    scale_fill_brewer() +\n    labs(\n      title = \"Bifurcated population\",\n      y = \"prob\"\n    )\n\n\n\n\n\n\nFigure 4: The posterior fitted values.\n\n\n\n\nIn fact, that posterior distribution for the between-speaker sd is very extreme at about 8. If we plot the kind of distribution of individuals it suggests when the population level probability = 0.5, we get those steep walls near 0 and 1 again.\n\nplotting codelow_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    regex = T\n  ) |&gt; \n  slice_sample(\n    n = 10\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(\n    individuals = list(tibble(\n      individual = rnorm(1e5, mean = 0, sd=.value)\n    ))\n  ) |&gt; \n  unnest(individuals) |&gt; \n  ggplot(aes(plogis(individual)))+\n    stat_slab(\n      aes(group = factor(.value)),\n      linewidth = 0.5,\n      fill = NA,\n      color = ptol_red\n    ) +\n    scale_y_continuous(expand = expansion(mult = 0))+\n    labs(\n      title = \"Random intercepts distribution around 0.5\"\n    )+\n    theme_no_y()\n\n\n\n\n\n\nFigure 5: Implied distribution of individuals in the bifircated population.\n\n\n\n\nThe Unimodal Population\nLet’s do it all again, but now for the population where individuals’ probabilities were clustered around the population probability.\n\nhigh_mod &lt;- brm(\n  y_prob_high ~ x + (1|individual),\n  data = sim_obs,\n  family = bernoulli(link = \"logit\"),\n  prior = c(\n    prior(normal(0,1.5), class = \"Intercept\"),\n    prior(normal(0,1), class = \"b\", coef = \"x\")\n  ),\n  cores = 4,\n  adapt_delta = 0.9,\n  seed = seed,\n  backend = \"cmdstanr\",\n  file = \"high_mod.RDS\"\n) \n\n\nsummary tablehigh_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    `b_.*`,\n    regex = T\n  ) |&gt; \n  mean_hdci() |&gt; \n  select(\n    .variable, .value, .lower, .upper\n  ) |&gt; \n  gt() |&gt; \n   fmt_number(decimals = 2)\n\n\n\n\n\n.variable\n.value\n.lower\n.upper\n\n\n\nb_Intercept\n−0.06\n−0.23\n0.10\n\n\nb_x\n1.05\n0.87\n1.22\n\n\nsd_individual__Intercept\n0.77\n0.63\n0.91\n\n\n\n\n\n\nThe intercepts and slope posteriors are much more tight, and the inter-speaker sd posterior is &lt;1.\n\nplottng codehigh_mod |&gt; \n  predictions(\n    newdata = datagrid(x = seq(-3, 3, length = 100)),\n    re_formula = NA\n  ) |&gt; \n  posterior_draws() |&gt; \n  ggplot(aes(x, draw)) +\n    stat_lineribbon(linewidth = 0.5)+\n    scale_fill_brewer()+\n    labs(\n      title = \"Unimodal population\",\n      y = \"prob\"\n    )\n\n\n\n\n\n\nFigure 6: Posterior fitted values for the unimodal population.\n\n\n\n\nLet’s look at the implied individual level distribution around 0.5.\n\nplotting codehigh_mod |&gt; \n  gather_draws(\n    `sd_.*`,\n    regex = T\n  ) |&gt; \n  slice_sample(\n    n = 20\n  ) |&gt; \n  rowwise() |&gt; \n  mutate(\n    individuals = list(tibble(\n      individual = rnorm(1e5, mean = 0, sd=.value)\n    ))\n  ) |&gt; \n  unnest(individuals) |&gt; \n  ggplot(aes(plogis(individual)))+\n    stat_slab(\n      aes(group = factor(.value)),\n      linewidth = 0.5,\n      fill = NA,\n      color = ptol_red\n    ) +\n    scale_y_continuous(expand = expansion(mult = 0))+\n    labs(\n      title = \"Random intercepts distribution around 0.5\"\n    )+\n    theme_no_y()\n\n\n\n\n\n\nFigure 7: Implied distribution of individuals in the unimodal population.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#the-upshot",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#the-upshot",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "The Upshot",
    "text": "The Upshot\nEven though I’ve fit and looked at hierarchical logistic regressions before, I hadn’t stopped to think about how to interpret the standard deviation of the random intercepts before. If you’d asked me before what a large sd implied about the distribution of individuals in the probability space, I think I would have said they’d be more uniformly distributed, but actually it means they’re more bifurcated!\nAlso, if you’ve got a fairly bifurcated population, the population level estimates are going to get pretty wonky.\nAll food for thought moving forward.",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#footnotes",
    "href": "posts/2023/06/2023-06-29_hierarchical-variance/index.html#footnotes",
    "title": "Thinking About Hierarchical Variance Parameters",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially also tried simulating a case where individuals were strictly categorical based on the probability associated with their x, and things did not go well for the models.↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2023",
      "06",
      "Thinking About Hierarchical Variance Parameters"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "",
    "text": "Last semester, I spent time co-developing some python packages:\nSo, I thought I’d share a little walkthrough of a cool way to use them. They can both be installed with pip.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#fasttrackpy",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#fasttrackpy",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "FastTrackPy",
    "text": "FastTrackPy\nfasttrackpy (Fruehwald and Barreda 2023) is a python implementation of Santiago Barreda’s Praat plugin (Barreda 2021). Right now, its design is really geared towards command line usage, and has three different subcommands\n\n\nfasttrack audio\n\nThis will run fasttrack on a single audio file or a directory of audio files\n\n\n\nfasttrack audio-textgrid\n\nThis will run fasttrack on an (audio, textgrid) tuple\n\n\n\nfasttrack corpus\n\nThis will run fasttrack on a corpus of paired audio + textgrid files\n\n\n\nYou can check out the docs for all of the processing options. I’ll be using a config file that looks like this:\n\nyaml\n\n# config.yml\ncorpus: data/corpus/\noutput: data/results/formants.csv\nentry-classes: \"Word|Phone\"\ntarget-tier: Phone\ntarget-labels: \"[AEIOU]\"\nmin-duration: 0.05\nmin-max-formant: 4000\nmax-max-formant: 7000\nnstep: 20\nwhich-output: winner\ndata-output: formants\nSome of these settings are just the defaults, but I’m just illustrating the kind of things you could do. To run it:\n\nbash\n\nfasttrack corpus --config config.yml\nOn my laptop, it got formant estimates for 339 vowels in about 18 seconds.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#looking-at-the-data",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#looking-at-the-data",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Looking at the data",
    "text": "Looking at the data\nLet’s get R up and running\n\n\nr\n\nSetup librariessource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(marginaleffects)\nlibrary(gt)\nlibrary(reticulate)\nlibrary(khroma)\nlibrary(geomtextpath)\nlibrary(downlit)\nlibrary(xml2)\n\n\n\n\nr\n\nvowel_data &lt;- read_csv(\"data/results/formants.csv\")\n\nJust to skim over some data columns of interest\n\n\nr\n\nvowel_data |&gt; \n colnames()\n\n [1] \"F1\"            \"F2\"            \"F3\"            \"F1_s\"         \n [5] \"F2_s\"          \"F3_s\"          \"error\"         \"time\"         \n [9] \"max_formant\"   \"n_formant\"     \"smooth_method\" \"file_name\"    \n[13] \"id\"            \"group\"         \"label\"         \"F4\"           \n[17] \"F4_s\"         \n\n\n\n\n\n\n\n\nNoteUseful Columns\n\n\n\n\nF1, F2, F3, F4\n\nThe formant tracks as estimated by the LPC analysis\n\nF1_s, F2_s, F3_s, F4_s\n\nSmoothed formant tracks, using discrete cosine transform\n\nfile_name\n\nThe basename for each file in the corpus\n\ngroup\n\nIf there were multiple talkers annotated in a file, which talker\n\nid\n\nA unique ID for each phone\n\n\n\n\nI’m going to zoom in on my favorite vowel, “AY”, and fit a quick model.\n\n\nr\n\n# Getting the ay data\nvowel_data |&gt; \n  filter(\n    group %in% c(\"KY25A\", \"group_0\"),\n    str_detect(label, \"AY\")\n  ) |&gt; \n  select(\n    file_name,\n    id,\n    group,\n    label,\n    F1_s, F2_s,\n    time\n  ) |&gt; \n  mutate(\n    rel_time = time - min(time),\n    prop_time = rel_time / max(rel_time),\n    .by = c(file_name, id)\n  )-&gt;\n  ay_data\n\n\n\nr\n\nModel fitting (not the main point)ay_data |&gt; \n  group_by(\n    file_name\n  ) |&gt; \n  nest() |&gt; \n  mutate(\n    model = map(\n      data, \n      ~gam(\n        list(F1_s ~ s(prop_time),\n             F2_s ~ s(prop_time)),\n        data = .x,\n        family = mvn(d = 2)\n      )\n    ),\n    pred = map(\n      model,\n      ~predictions(\n        .x, \n        newdata = datagrid(\n          prop_time = seq(0,1,length = 100)\n        )\n      )\n    )\n  ) |&gt; \n  select(file_name, pred) |&gt; \n  unnest(pred) |&gt; \n  select(file_name, rowid, group, estimate, prop_time) |&gt; \n  mutate(\n    group = str_glue(\"F{group}\")\n  ) |&gt; \n  pivot_wider(\n    names_from = group,\n    values_from = estimate\n  )-&gt;\n  ay_predictions\n\n\n\n\nr\n\nPlotting codelibrary(scales)\nlog_rev_trans = trans_new(\n  name = \"log_rev\",\n  transform = \\(x) -log(x),\n  inverse = \\(x) exp(-x)\n)\n\nay_predictions |&gt; \n  ggplot(\n    aes(\n      F2, \n      F1\n    )\n  )+\n    geom_path(\n      arrow = arrow(type = \"closed\"),\n      linewidth = 1\n    ) +\n    scale_x_continuous(trans = log_rev_trans)+\n    scale_y_continuous(trans = log_rev_trans)+\n    coord_fixed()+\n    facet_wrap(~file_name)\n\n\n\n\n\n\nFigure 1: /ay/ trajectories\n\n\n\n\nCool! Except… One of the most important factors for /ay/ is missing: whether or not the following segment is voiced or voiceless! Since fasttrackpy is designed to be very general purpose, (and not too feature laden) this kind of info isn’t added to the output. But. we can easily get it with aligned-textgrid.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#working-with-aligned-textgrid",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#working-with-aligned-textgrid",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Working with aligned-textgrid",
    "text": "Working with aligned-textgrid\nRight now, aligned-textgrid (Fruehwald and Brickhouse 2023) mostly designed to be worked with either in scripts, or interactively, so we’re going to switch over to python code. I’ll work over just one TextGrid for clarity.\n\n\npython\n\nfrom aligned_textgrid import AlignedTextGrid, Word, Phone\nfrom pathlib import Path\nimport pandas as pd\n\n\n\npython\n\ntg1_path = Path(\n  \"data\", \n  \"corpus\",\n  \"josef-fruehwald_speaker.TextGrid\"\n  )\n  \ntg1 = AlignedTextGrid(\n  textgrid_path = tg1_path,\n  entry_classes = [Word, Phone]\n)\n\ntg1\n\nAlignedTextGrid with 1 groups named ['group_0'] each with [2] tiers. [['Word', 'Phone']]\n\n\nI want to grab out enriched data for each phone for the group_0 speaker, which we can do with the dynamically created accessors for each speaker group and tier class like so.\n\n\npython\n\nphone_tier = tg1.group_0.Phone\nphone_tier\n\nSequence tier of Phone; .superset_class: Word; .subset_class: Bottom_wp\n\n\nWe can grab individual phones via indexing.\n\n\npython\n\nphone_tier[30]\n\nClass Phone, label: IY0, .superset_class: Word, .super_instance: the, .subset_class: Bottom_wp\n\n\nBut I want to focus in on just the phones with an AY label, which I’ll do with a list comprehension.\n\n\npython\n\nays = [p for p in phone_tier if \"AY\" in p.label]\n\nTo grab the following segment for each /ay/, we can use the .fol accessor.\n\n\npython\n\n# a single example\nays[0].fol.label\n\n'T'\n\n\n\n\npython\n\n# for all /ays/\n[p.fol.label for p in ays]\n\n['T', 'K', 'K', 'T', 'T', '#', 'Z', 'N', 'N', '#', 'D', '#', '#', '#', 'D', 'Z', 'Z', 'M', 'D', 'T', 'P']\n\n\nYou can see that some /ay/ tokens have a # following segment, meaning a word boundary. If we wanted to get the following segment tier-wise, we can do so to.\n\n\npython\n\n[p.get_tierwise(1).label for p in ays]\n\n['T', 'K', 'K', 'T', 'T', 'AH0', 'Z', 'N', 'N', '', 'D', 'R', 'DH', 'DH', 'D', 'Z', 'Z', 'M', 'D', 'T', 'P']\n\n\nLet’s pop this all into a pandas dataframe\n\n\npython\n\nays_context = pd.DataFrame({\n  \"id\":       [p.id for p in ays],\n  \"fol\":      [p.fol.label for p in ays],\n  \"fol_abs\":  [p.get_tierwise(1).label for p in ays],\n  \"word\":     [p.within.label for p in ays],\n  \"fol_word\": [p.within.fol.label for p in ays ]\n})\n\nays_context\n\n           id fol fol_abs      word    fol_word\n0     0-0-3-4   T       T  sunlight     strikes\n1     0-0-4-3   K       K   strikes   raindrops\n2    0-0-12-1   K       K      like           a\n3    0-0-25-1   T       T     white       light\n4    0-0-26-1   T       T     light            \n5    0-0-49-1   #     AH0      high       above\n6    0-0-60-2   Z       Z   horizon            \n7    0-0-85-1   N       N     finds          it\n8   0-0-153-1   N       N      sign        from\n9   0-0-188-2   #               sky            \n10  0-0-193-2   D       D     tried          to\n11  0-0-209-1   #       R        by  reflection\n12  0-0-216-1   #      DH        by         the\n13  0-0-235-1   #      DH        by         the\n14  0-0-246-0   D       D     ideas       about\n15  0-0-263-1   Z       Z      size          of\n16  0-0-279-1   Z       Z      size          of\n17  0-0-287-2   M       M   primary     rainbow\n18  0-0-333-1   D       D      wide      yellow\n19  0-0-343-1   T       T    lights        when\n20  0-0-354-1   P       P      type          of\n\n\nWith the way aligned-textgrid links intervals and relates their hierarchical structure, I’m able to quickly able to navigate up, down, and over between intervals using straightforwardly named accessors.\nWe can get pretty silly, like: what is the second to last phoneme in the word following the word this vowel is in?\n\n\npython\n\n[\n  ays[0].label,\n  ays[0].within.label,\n  ays[0].within.fol.label,\n  ays[0].within.fol.last.label,\n  ays[0].within.fol.last.prev.label\n]\n\n['AY2', 'sunlight', 'strikes', 'S', 'K']",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#joining-together",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#joining-together",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Joining together",
    "text": "Joining together\nBack to the /ays/ data, we can quickly join this enriched data onto the formant data, because the id column is the same between the two.\n\n\nr\n\nay_data |&gt; \n  left_join(\n    py$ays_context |&gt; \n      mutate(file_name = \"josef-fruehwald_speaker\")\n  ) |&gt; \n  filter(\n    !is.na(fol)\n  ) |&gt; \n  mutate(\n    voicing = case_when(\n      fol %in% c(\"P\", \"T\", \"K\") ~ \"vless\",\n      fol == \"#\" ~ \"final\",\n      .default = \"vced\"\n    )\n  )-&gt;\n  ays_enriched\n\nays_enriched |&gt; \n  head() |&gt; \n  rmarkdown::paged_table()\n\n\n  \n\n\n\nAnd now I can refit the model and plot.\n\n\nr\n\nModelling code# gam is annoying and needs\n# voicing to explicitly be a factor\nays_enriched |&gt; \n  mutate(voicing = factor(voicing)) -&gt;\n  ays_enriched\n\nays_enriched_model &lt;- gam(\n  list(\n    F1_s ~ voicing + s(prop_time, by = voicing),\n    F2_s ~ voicing + s(prop_time, by = voicing)\n  ),\n  data = ays_enriched,\n  family = mvn(d = 2) \n)\n\n\nays_enriched_model |&gt; \n  predictions(\n    newdata = datagrid(\n      prop_time = seq(0, 1, length = 100),\n      voicing = unique\n    )\n  ) |&gt; \n  as_tibble() |&gt; \n  select(\n    rowid, group,\n    estimate, prop_time, voicing\n  ) |&gt; \n  mutate(\n    group = str_glue(\"F{group}\")\n  ) |&gt; \n  pivot_wider(\n    names_from = group,\n    values_from = estimate\n  ) -&gt;\n  ays_enriched_pred\n\n\n\n\nr\n\nplotting codeays_enriched_pred |&gt; \n  ggplot(\n    aes(\n      F2,\n      F1,\n      color = voicing\n    )\n  )+\n    geom_textpath(\n      aes(label = voicing),\n      linewidth = 1,\n      arrow = arrow(type = \"closed\")\n    )+\n    scale_x_continuous(\n      trans = log_rev_trans\n    )+\n    scale_y_continuous(\n      trans = log_rev_trans\n    )+\n    scale_color_bright(\n      guide = \"none\"\n    )+\n    coord_fixed()\n\n\n\n\n\n\nFigure 2: enriched /ays/ data",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/02/2024-02-16_fs-atg/index.html#let-me-know-how-it-goes",
    "href": "posts/2024/02/2024-02-16_fs-atg/index.html#let-me-know-how-it-goes",
    "title": "Using FastTrackPy and aligned-textgrid",
    "section": "Let me know how it goes!",
    "text": "Let me know how it goes!\nIf you start using either fasttrackpy or aligned-textgrid for any purpose, I’d love to know how it’s going! For any feature requests, or bug reports, checkout their respective github repositories.\n\naligned-textgrid Github\nfasttrackpy Github",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "02",
      "Using FastTrackPy and aligned-textgrid"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "",
    "text": "I’ve been doing a lot of python development recently, and really leaning into object oriented programming for my projects. For example, in the aligned-textgrid package, I started off by defining a class to represent intervals, which at their core have start and end times, and a label. Each interval also points to the interval objects preceding and following, as well as any intervals it contains or is contained in.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html#doing-it-in-r",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html#doing-it-in-r",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "Doing it in R?",
    "text": "Doing it in R?\nI don’t exactly want to replicate the entire package inside R, and for a while I wasn’t even sure how I could, because of R’s copy-on-modify behavior.\nLet’s say we had the following sequence of intervals:\n\n\n\n\n\nWe could represent the basic information (start, end, and label) in lists. I’ll also set up a reference to interval_b as following interval_a.\n\n\nr\n\ninterval_a &lt;- list(\n  start = 1,\n  end   = 2,\n  label = \"a\"\n)\n\ninterval_b &lt;- list(\n  start = 3,\n  end   = 4,\n  label = \"b\"\n)\n\ninterval_a$fol &lt;- interval_b\n\nWe can double check that the fol reference is working with identical() .\n\n\nr\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNoteidentical()\n\n\n\n\n\nThis took me a while to find. Python has a commonly used is operator for checking whether two variables refer to the same object, rather than just being equal.\n\n\npython\n\na = 1.0\nb = 2/2\n\na is b\n\nFalse\n\n\n\n\npython\n\na == b\n\nTrue\n\n\nBut I’ve never used R’s identical() before, and it doesn’t usually show up in intros to the language like python’s is.\n\n\n\n\nCopy-on-modify\nThings become a problem if we want to make changes to interval_b, though. One of the core tasks I wanted aligned-textgrid to make easy is the modification of interval labels. But in R, if we change the value of interval_b$label, that change won’t be reflected in the values in interval_a$fol, and the reference between the two objects will be broken.\n\n\nr\n\ninterval_b$label &lt;- \"new B\"\n\n# this is now false\nidentical(interval_a$fol, interval_b)\n\n[1] FALSE\n\n\n\n\nr\n\n# this is the original label\ninterval_a$fol$label\n\n[1] \"b\"\n\n\nTo understand what’s going on here, I’d recommend checking out Hadley Wickham’s Advanced R chapter on Names and Values, especially the section on Copy-on-modify.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html#oop-in-r",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html#oop-in-r",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "OOP in R",
    "text": "OOP in R\nOne of these days, there’s going to be a new R package called yaoop for Yet Another Object Oriented Paradigm. The class systems that ship with R are called S3 and S4, and there are two new-ish class packages R6 and S7. These two new packages both have pretty interesting properties, but as pointed out in Advanced R\n\nR6 objects are mutable, which means that they are modified in place, and hence have reference semantics.\n\nS7 is the newer package, but I’ve double checked, and it also follows copy-on-modify, which means R6 is the way to go for this kind of use case.\n\nTrying out \n\n\nr\n\nlibrary(R6)\n\nFollowing the intro in the R6 documentation, the most basic SequenceInterval class would be something like this.\n\n\nr\n\n\n\n1\n\nThe name of the class you want to be returned when you use class() .\n\n2\n\nAny generally available class properties and methods are declared in a list passed to the public argument.\n\n3\n\nI believe a any properties you want to use or set through the rest of the class definition need to be declared here first.\n\n4\n\ninitialize is a special method that’s called when you use SequenceInterval$new()\n\n\n\nSequenceInterval &lt;- R6Class(\n1  classname = \"SequenceInterval\",\n2  public = list(\n3    start = numeric(0),\n    end   = numeric(0),\n    label = character(0),\n    prev  = NULL,\n    fol   = NULL,\n                                    \n4    initialize = function(\n      start = numeric(0),\n      end   = numeric(0),\n      label = character(0)\n    ){\n      self$start = start\n      self$end   = end\n      self$label = label\n    }\n    \n  )                               \n)\n\nMy two sequence objects would then be:\n\n\nr\n\ninterval_a &lt;- SequenceInterval$new(\n  start = 1,\n  end   = 2,\n  label = \"a\"\n)\n\ninterval_b &lt;- SequenceInterval$new(\n  start = 2,\n  end   = 3,\n  label = \"b\"\n)\n\ninterval_a$fol &lt;- interval_b\n\nLet’s double check that sequence_b is appropriately following sequence_a.\n\n\nr\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\nNow, for the moment of truth, we’ll change the label on interval_b and see if it breaks things.\n\n\nr\n\ninterval_b$label &lt;- \"new B\"\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\n\n\nr\n\ninterval_a$fol$label\n\n[1] \"new B\"\n\n\nSuccess!",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-02_OOP-r/index.html#getting-more-complicated",
    "href": "posts/2024/09/2024-09-02_OOP-r/index.html#getting-more-complicated",
    "title": "Experimenting with Object Oriented Programming in R",
    "section": "Getting more complicated",
    "text": "Getting more complicated\nI’m going to try to make things a little more complicated with respect to thefol and prev properties. I want\n\nWhen fol is set, prev is automatically set.\nWhen prev is set, fol is automatically set.\n\nThese aren’t just nice quality of life features, but also capture the necessary logical properties of following and preceding.\nI think the way to go about this will be\n\nto lock off fol and prev from being directly settable. I think the best way to do this is to move them to active bindings, which seems a lot like using the python @property decorator on a method.\nAdd private .fol and .prev properties.\nDefine setter functions that will update the _fol and _prev properties, being careful to avoid infinite recursion!\n\nI’ve described each new piece in the code annotations.\n\n\nr\n\n\n\n1\n\nDefining private properties. Convention in python is for the name of these properties to start with and underscore, but that’s not allowed in R, so I’m going with dots.\n\n2\n\nThese are the getter functions to return the actual .fol and .prev objects.\n\n3\n\nSame as the original class definition above.\n\n4\n\nThis is the setter function for the following interval.\n\n5\n\nSUPER IMPORTANT. The set_fol() method is calling set_prev(), and the set_prev() method is calling set_fol(). To avoid infinite recursion, the function should stop here if it’s already the preceding interval to its following interval.\n\n6\n\nA kind of interesting thing is I have to use the public set_prev() method here, because the method won’t be able to dig into the private .prev property of interval.\n\n7\n\nSame logic as set_fol().\n\n\n\nSequenceInterval &lt;- R6Class(\n  classname = \"SequenceInterval\",\n  \n1  private = list(\n    .fol = NULL,\n    .prev = NULL\n  ),\n\n2  active = list(\n    fol = function(){\n      return(private$.fol)\n    },\n    prev = function(){\n      return(private$.prev)\n    }\n  ),\n\n3  public = list(\n    start = numeric(0),\n    end   = numeric(0),\n    label = character(0),\n\n    initialize = function(\n      start = numeric(0),\n      end   = numeric(0),\n      label = character(0)\n    ){\n      self$start = start\n      self$end   = end\n      self$label = label\n    },\n\n4    set_fol = function(interval){\n      private$.fol = interval\n\n5      if(identical(self$fol$prev, self)){\n        return(invisible(self))\n      }\n\n6      self$fol$set_prev(self)\n\n    },\n\n7    set_prev = function(interval){\n      private$.prev = interval\n\n      if(identical(self$prev$fol, self)){\n        return(invisible(self))\n      }\n\n      self$prev$set_fol(self)\n    }\n\n  )\n)\n\nWe can still create intervals the same way as above.\n\n\nr\n\ninterval_a &lt;- SequenceInterval$new(1, 2, \"a\")\ninterval_b &lt;- SequenceInterval$new(1, 2, \"b\")\n\nBut to set up interval_b as the interval following interval_a, you’ve got to use the set_fol() setter function.\n\n\nr\n\ninterval_a$set_fol(interval_b)\n\nidentical(interval_a$fol, interval_b)\n\n[1] TRUE\n\n\nBut now, interval_a has been automatically set as preceding interval_b!\n\n\nr\n\nidentical(interval_b$prev, interval_a)\n\n[1] TRUE\n\n\nThe major defect here is that if I, or a user, didn’t know about the setter functions, we’ll get a very inscrutable error message when trying to directly set fol or prev.\n\n\nr\n\ninterval_a$fol &lt;- interval_b\n\nError in (function () : unused argument (base::quote(&lt;environment&gt;))\n\n\nI might want to figure out if it’s possible to get a better error here, or even better, some way to short circuit this assignment attempt to set_fol(), but I think that’s a bit beyond my patience and time for right now.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Experimenting with Object Oriented Programming in R"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "",
    "text": "I’ve been playing a lot of Baldur’s Gate 3 lately. My party has gotten to max level, and we’re running around, casting spells and taking names.\nOne of the spells my wizard has is called Disintegrate, which gets summarized like this\nThe line of information under “Damage” means that to calculate how much damage you do when you cast the spell, you need to roll 10 6-sided dice (“d6”), then add 40. The “(50~100)” part summarizes the worst vs best outcomes.\nIf you just glance at the description saying that the spell will do between 50 and 100 damage, you might think that you’re equally likely to roll any amount of damage between these two values.\nBut, as you play, you’ll find that hitting something right in the middle, like 75, is very likely, and that hitting either 50 or 100 is vanishingly rare. The distribution of rolls wind up looking like this:\nIn fact, 95% of the rolls are going to be between 64 and 86. The rest of this post is about these two questions:",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#d6",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#d6",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "1d6",
    "text": "1d6\nWe should start things off easy, and look at what happens when we roll 1d6. As long as the die is fair, any value between 1 and 6 is possible. And for each value between 1 and 6, there’s only 1 possible way to get that value.\n\n\n\n\n\n\ntotal\nrolls\nn\n\n\n\n1\n1\n1\n\n\n2\n2\n1\n\n\n3\n3\n1\n\n\n4\n4\n1\n\n\n5\n5\n1\n\n\n6\n6\n1",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-1",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-1",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "2d6",
    "text": "2d6\nThings are more complicated when we start rolling 2d6 and summing them together. Let’s name each of our die “Alpha” and “Beta” to keep them straight.\nThere’s only one way for us to get a total roll of 2: both Alpha and Beta need to come up 1. Same thing for a total roll of 12: both Alpha and Beta need to come up 6.\nBut to roll a value of 3, there are two ways.\n\nAlpha rolls 1, and Beta rolls 2.\nAlpha rolls 2, and Beta rolls 1.\n\nIf we expand this out to look at all possible values between 2 and 12, we’ll find that there’s six unique ways to roll 7, with each value above and below having fewer unique ways.\n\n\n\n\n\n\ntotal\nrolls\nn\n\n\n\n2\n1 1\n1\n\n\n3\n1 2; 2 1\n2\n\n\n4\n1 3; 2 2; 3 1\n3\n\n\n5\n1 4; 2 3; 3 2; 4 1\n4\n\n\n6\n1 5; 2 4; 3 3; 4 2; 5 1\n5\n\n\n7\n1 6; 2 5; 3 4; 4 3; 5 2; 6 1\n6\n\n\n8\n2 6; 3 5; 4 4; 5 3; 6 2\n5\n\n\n9\n3 6; 4 5; 5 4; 6 3\n4\n\n\n10\n4 6; 5 5; 6 4\n3\n\n\n11\n5 6; 6 5\n2\n\n\n12\n6 6\n1\n\n\n\n\n\n\nThere’s just more different ways for a very large roll on Alpha to get balanced out by a smaller roll on Beta.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-2",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#d6-2",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "10d6",
    "text": "10d6\nIf we expand this out to the 10d6 situation, where there’s only one way to roll 10 (all six die roll 1), there are 4,395,456 different ways to roll 35. The massively larger number of ways to get 35 results in it showing up that much more often than 10 (or 60).",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/09/2024-09-08_roll-clt/index.html#unfair-dice",
    "href": "posts/2024/09/2024-09-08_roll-clt/index.html#unfair-dice",
    "title": "Rolling for Damage with the Central Limit Theorem",
    "section": "Unfair dice",
    "text": "Unfair dice\nThe nifty thing about the Central Limit Theorem is that it works no matter what the shape of the original distribution was. Let’s say we had a 10 sided die, but 3 of the sides have “1” painted, 3 of the sides have “6” painted on them, and the rest have 2 through 5. With this die, we’re a lot more likely to roll a 1 or a 6.\n\n\n\n\n\n\n\n\nIf we used this unfair die, to roll damage for the Disintegrate spell, how will that affect the outcome?\n\n\n\n\n\n\n\n\nIt might be hard to see the difference, but this unfair die is slightly more spread out than the fair d6\n\n\n\n\n\n\n\n\nBut, even though it’s more spread out, it’s still approximating a normal distribution!",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "09",
      "Rolling for Damage with the Central Limit Theorem"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html",
    "title": "Setting default ggplot2 colors",
    "section": "",
    "text": "This might be a “everyone else already knew about this” thing, but I’ve finally gotten to a place of understanding about setting default colors scales for ggplot2, so I thought I’d share.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#discrete-colors",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#discrete-colors",
    "title": "Setting default ggplot2 colors",
    "section": "Discrete colors",
    "text": "Discrete colors\nWithout doing anything\nHere’s how things look by default:\n\nggplot(\n  sample_data\n)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = level\n    )\n  ) -&gt;\n  discrete3_p\n\nggplot(\n  sample_data\n)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = a &lt; 0\n    )\n  ) -&gt;\n  discrete2_p\n\ndiscrete2_p\ndiscrete3_p\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting the default with a list\nIf we set the default colors by setting options(ggplot2.discrete.colour = ) a list of color values, ggplot will use those colors if there’s enough, and if there’s not enough, it’ll fall back to the default scale_color_hue().\nwithr::with_options(\n  list(\n    ggplot2.discrete.colour = list(\n      c(\"#AA4499\", \"#117733\")\n    )\n  ),\n  \n  {\n    print(discrete2_p)\n    print(discrete3_p)\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou could even set a completely different vector of values for 3 colors.\nwithr::with_options(\n  list(\n    ggplot2.discrete.colour = list(\n      c(\"#AA4499\", \"#117733\"),\n      c(\"#4477AA\", \"#88CCEE\", \"#DDCC77\")\n    )\n  ),\n  \n  {\n    print(discrete2_p)\n    print(discrete3_p)\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I set up in my actual _defaults.R files is to generate all of the possible palettes from ggthemes::ptol_pal(), because I like it.\n\nmy_discrete_list &lt;- lapply(\n  1:12, \n  ggthemes::ptol_pal()\n)\n\nscales::show_col(\n  my_discrete_list[[10]],\n  ncol = 5,\n  cex_label = 0.8\n)\n\n\n\n\n\n\n\nI could set ggplot2.discrete.colour to ggthemes::scale_color_ptol(). But by setting it to the progessively larger list of colors, if if make the decision1 to map a factor with 13 labels to color, instead of erroring or just not plotting some points, it will fall back to the built in scale_color_hue().\n\noptions(\n  ggplot2.discrete.colour = lapply(\n    1:12,\n    ggthemes::ptol_pal()\n  ),\n  ggplot2.discrete.fill = lapply(\n    1:12,\n    ggthemes::ptol_pal()\n  )\n)\n\nHere’s an example showing using that 13+ levels example\n\nSetting up base plotsggplot(sample_data)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = cut(a, 10)\n    )\n  )+\n  guides(\n    color = \"none\"\n  )-&gt;\n  base_10_p\n\nggplot(sample_data)+\n  geom_point(\n    aes(\n      a,\n      b,\n      color = cut(a, 15)\n    )\n  )+\n  guides(\n    color = \"none\"\n  )-&gt;\n  base_15_p\n\n\nPlotting codebase_10_p +\n  labs(\n    title = \"10 cuts, list\"\n  )\nbase_15_p +\n  labs(\n    title = \"15 cuts, list\"\n  )\nbase_10_p +\n  ggthemes::scale_color_ptol()+\n  labs(\n    title = \"10 cuts, scale\"\n  )\nbase_15_p +\n  ggthemes::scale_color_ptol()+\n  labs(\n    title = \"15 cuts, scale\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError in colors[[n]]: subscript out of bounds",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#continuous-colors",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#continuous-colors",
    "title": "Setting default ggplot2 colors",
    "section": "Continuous colors",
    "text": "Continuous colors\nThe continuous color scales need to be set more straightforwardly with a function that returns a scale_color_*(). I’m kind of bouncing around continuous color scales I like, but for now, I’m defaulting to the batlow palette from scico.\nSince getting the specific batlow palette requires passing arguments to scico::scale_color_scico(), I need to pass ggplot2.continuous.colour an anonymous function.\n\nggplot(sample_data)+\n  geom_point(\n    aes(\n      a, b,\n      color = a\n    )\n  )-&gt;\n  continuous_base\n\n\noptions(\n  ggplot2.continuous.colour = \\(...){\n    scico::scale_color_scico(\n      palette = \"batlow\", \n      ...\n    )\n  },\n  ggplot2.continuous.fill = \\(...){\n    scico::scale_fill_scico(\n      palette = \"batlow\", \n      ...\n    )\n  }\n)\n\n\ncontinuous_base",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#ordinal-colors",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#ordinal-colors",
    "title": "Setting default ggplot2 colors",
    "section": "Ordinal colors",
    "text": "Ordinal colors\nFor the longest time, I only had settings for continuous and discrete color scales, and I kept getting frustrated when an occasional plot would show up with neither of my options showing up.\n\n(\n  ggplot(sample_data) +\n    geom_hdr_points(\n      aes(a, b),\n      probs = rev(ppoints(10))\n    )+\n    guides(color = \"none\") -&gt;\n    density_points\n)\n\n\n\n\n\n\n\nI could not, for the life of me, figure out what option I had to set to change the default here! I eventually just searched the ggplot2 github for getOption and found ggplot2.ordinal.colour! This is definitely not documented anywhere in the actual ggplot2 docs!\nAnyway, l kind of like the mako viridis palette for this, so that’s what I’m using:\n\noptions(\n  ggplot2.ordinal.colour = \\(...){\n    scale_color_viridis_d(\n      option = \"G\", \n      direction = -1, \n      ...\n    )\n  },\n  ggplot2.ordinal.fill = \\(...){\n    scale_fill_viridis_d(\n      option = \"G\", \n      direction = -1, \n      ...\n    )\n  }\n)\n\n\ndensity_points",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#footnotes",
    "href": "posts/2024/10/2024-10-01_ggplot2-default-colors/index.html#footnotes",
    "title": "Setting default ggplot2 colors",
    "section": "Footnotes",
    "text": "Footnotes\n\nbad↩︎",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "10",
      "Setting default ggplot2 colors"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-20_poisson-ranef/index.html",
    "href": "posts/2024/11/2024-11-20_poisson-ranef/index.html",
    "title": "Random Effects and Overdispersion",
    "section": "",
    "text": "Today in my stats class, my students saw me realize, in real-time, that you can include random intercepts in poisson models that you couldn’t in ordinary gaussian models, and this might be a nicer way to deal with overdispersion than moving to a negative binomial model.\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(distributional)\nlibrary(ggdist)\nlibrary(palmerpenguins)\nlibrary(gt)\nlibrary(marginaleffects)",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "11",
      "Random Effects and Overdispersion"
    ]
  },
  {
    "objectID": "posts/2024/11/2024-11-20_poisson-ranef/index.html#negative-binomial",
    "href": "posts/2024/11/2024-11-20_poisson-ranef/index.html#negative-binomial",
    "title": "Random Effects and Overdispersion",
    "section": "Negative Binomial?",
    "text": "Negative Binomial?\nThe stats notes I linked to above turned to a negative binomial model to use in a case of overdispersion like this. I’m not quite in a place to evaluate the pros and cons of the negative binomial vs this random effects approach in general. But for this case I like the random effects better because\n\nIt lines up with how I think about this data as having a population level trend, with individual divergences off of it.\nIt’s easier for me to explain and understand than whatever the shape parameter is for the negative binomial.",
    "crumbs": [
      "About",
      "Posts",
      "2024",
      "11",
      "Random Effects and Overdispersion"
    ]
  },
  {
    "objectID": "posts/2025/03/2025-03-11_code-chunk-css/index.html",
    "href": "posts/2025/03/2025-03-11_code-chunk-css/index.html",
    "title": "Custom Code Chunk css",
    "section": "",
    "text": "By default in a quarto document, the code and output look something like this:\nMaybe this is just me not wanting my peas to touch my mashed potatos, but I don’t like how close the output is to the text of the document. I also feel like it gets a little visually confusing if I have a long paragraph,\nand then a short one\nand then a third.\nWhat I really want is my code output to be bundled up with the entire code chunk. I’ve worked out how to make it happen with some css, so now my code chunks and outputs look like this:\nrnorm(10)\n\n [1] -1.0257576  1.8198706 -1.2704175  0.5888158 -0.6342534 -1.2658154\n [7]  0.2837490 -1.4817612  0.9118797  0.9714688\nThe code input and output are wrapped up together visually, and remain distinct from the surrounding prose.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "03",
      "Custom Code Chunk css"
    ]
  },
  {
    "objectID": "posts/2025/03/2025-03-11_code-chunk-css/index.html#the-scss",
    "href": "posts/2025/03/2025-03-11_code-chunk-css/index.html#the-scss",
    "title": "Custom Code Chunk css",
    "section": "The scss",
    "text": "The scss\nI’ve set this up across three scss files, even though for the most part it’s not making use of the features of scss.\n\n\ncustom.scss\n\n/*-- scss:rules --*/\n.cell:not(.page-full):has(.cell-output){\n    padding: 2%;\n    border-radius: 10px;\n    margin-bottom: 1em;\n }\n\n\n\nlight.scss\n\n/*-- scss:rules --*/\n.cell:not(.page-full):has(.cell-output){\n    background-color: $gray-100;\n }\n\n\n\ndark.scss\n\n/*-- scss:rules --*/\n.cell:not(.page-full):has(.cell-output){\n   background-color: $gray-800;\n }\n\nThe separate light and dark files are there so the background color of the full wrapper is appropriate for light/darkmode. These all get included in my quarto configuration like so:\n\n\n_quarto.yml\n\n#...\nformat:\n  html:\n    theme:\n      light: [flatly, custom.scss, light.scss]\n      dark: [darkly, custom.scss, dark.scss]\n#...",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "03",
      "Custom Code Chunk css"
    ]
  },
  {
    "objectID": "posts/2025/03/2025-03-11_code-chunk-css/index.html#why-those-selectors",
    "href": "posts/2025/03/2025-03-11_code-chunk-css/index.html#why-those-selectors",
    "title": "Custom Code Chunk css",
    "section": "Why those selectors?",
    "text": "Why those selectors?\nSo, the .cell class targets the entire code cell (input and output). To be honest, I can’t reconstruct why I have a :not(.page-full) modifier in there. I must’ve done it to capture some edge case though, so I don’t question it.\nI do remember why I have the :has(.cell-output) part. The lighter enclosing div only appears when there’s code output. If I have code that doesn’t print anything, it’s just the normal default quarto formatting.\n\na &lt;- rnorm(10)\n\nIt’s also the case that figures don’t get rendered into a .cell-output div, so plotting code also gets the default quarto formatting.\n\nlibrary(ggplot2)\n\np &lt;- ggplot(\n  mtcars,\n  aes(mpg, disp)\n) +\n  geom_point()\n\nprint(p)\n\n\n\n\n\n\n\nSame for gt tables.\n\nlibrary(gt)\n\ncar_tab &lt;- gt_preview(mtcars) |&gt;\n  fmt_number(\n    decimals = 1,\n    drop_trailing_zeros = T\n  ) |&gt;\n  cols_align_decimal() |&gt; \n  opt_table_font(font = \"Public Sans\")\n\ncar_tab\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n1\n21  \n6 \n160 \n110 \n3.9\n2.6\n16.5\n0 \n1 \n4 \n4 \n\n\n2\n21  \n6 \n160 \n110 \n3.9\n2.9\n17  \n0 \n1 \n4 \n4 \n\n\n3\n22.8\n4 \n108 \n 93 \n3.9\n2.3\n18.6\n1 \n1 \n4 \n1 \n\n\n4\n21.4\n6 \n258 \n110 \n3.1\n3.2\n19.4\n1 \n0 \n3 \n1 \n\n\n5\n18.7\n8 \n360 \n175 \n3.1\n3.4\n17  \n0 \n0 \n3 \n2 \n\n\n6..31\n\n\n\n\n\n\n\n\n\n\n\n\n\n32\n21.4\n4 \n121 \n109 \n4.1\n2.8\n18.6\n1 \n1 \n4 \n2 \n\n\n\n\n\n\nThis is super handy, because if I don’t echo the code chunks, the figures and tables show up in the document just like you’d expect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ndisp\n\n\n\n1\n21  \n160 \n\n\n2\n21  \n160 \n\n\n3\n22.8\n108 \n\n\n4\n21.4\n258 \n\n\n5\n18.7\n360 \n\n\n6..31\n\n\n\n\n32\n21.4\n121 \n\n\n\n\n\n\n\n\nBut if I have unechoed code that prints code output, it will still show up with the shaded outline.\n\n\n [1]  1.2731270  1.4622685 -0.6110828 -0.3605532 -1.5291518  1.1332638\n [7] -0.4407391  0.1296048  0.8712326  0.5055147",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "03",
      "Custom Code Chunk css"
    ]
  },
  {
    "objectID": "posts/2025/03/2025-03-11_code-chunk-css/index.html#the-one-thing-to-watch-for",
    "href": "posts/2025/03/2025-03-11_code-chunk-css/index.html#the-one-thing-to-watch-for",
    "title": "Custom Code Chunk css",
    "section": "The one thing to watch for",
    "text": "The one thing to watch for\nIf ggplot returns some warnings, and you don’t have warnings in the output suppressed, the warning and the plot will all wind up in the .cell-output, even if you turn off code echoing.\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\nggplot(\n  penguins,\n  aes(\n    bill_length_mm, \n    bill_depth_mm\n  )\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "03",
      "Custom Code Chunk css"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "",
    "text": "Yesterday I posted about the normalization functions in the tidynorm R package. In order to implement formant track normalization, I had to also put together code for working with the Discrete Cosine Transform (DCT), which in and of itself can be handy to work with.\nlibrary(tidynorm)\nlibrary(tidyverse)\nlibrary(geomtextpath)\nlibrary(gt)\n\nsource(here::here(\"_defaults.R\"))",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#the-dct",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#the-dct",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "The DCT",
    "text": "The DCT\nI’ve posted about the DCT before, but to put it briefly, the DCT tries to re-describe an input signal in terms of weighted and summed cosine functions. The DCT basis looks like this:\n\nplotting codedct_mat &lt;- dct_basis(100, k = 5)\n\nas_tibble(\n  dct_mat, \n  .name_repair = \"unique\"\n) |&gt; \n  mutate(\n    x = row_number()\n  ) |&gt; \n  pivot_longer(\n    starts_with(\"...\")\n  ) |&gt; \n  ggplot(\n    aes(x, value, color = name)\n  ) +\n    geom_line(\n      linewidth = 1\n    ) +\n  guides(color = \"none\") +\n  labs(y = NULL) +\n  theme_no_x()-&gt;p\n\np\np+theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we grab one vowel’s formant track and fit a linear model using these functions as predictors, the coefficients will equal the DCT coefficients.\n\nspeaker_tracks |&gt; \n  filter(\n    speaker == \"s01\",\n    plt_vclass == \"ay\"\n  ) |&gt; \n  filter(id == first(id)) -&gt;\n  one_ay\n\n\nplotting codeone_ay |&gt; \n  ggplot(\n    aes(t, F1)\n  )+\n  geom_point(color = ptol_red, size = 2)-&gt;\n  p\n\np\np + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 5 dct coefficients\n# for a formant track with\n# 20 measurement points\ndct_mat &lt;- dct_basis(20, k = 5)\n\ndct_mod &lt;- lm(one_ay$F1 ~ dct_mat - 1)\n\ndct_direct &lt;- tidynorm::dct(one_ay$F1)[1:5]\n\ncbind(\n  coef(dct_mod),\n  dct_direct\n)\n\n                      dct_direct\ndct_mat1 602.3486557 602.3486557\ndct_mat2  97.7676452  97.7676452\ndct_mat3  -0.4687751  -0.4687751\ndct_mat4  -7.1061819  -7.1061819\ndct_mat5 -19.4956181 -19.4956181",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#using-the-dct-to-smooth",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#using-the-dct-to-smooth",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "Using the DCT to Smooth",
    "text": "Using the DCT to Smooth\nA cool thing about the DCT is that it can be used to smooth formant tracks. We can see that smoothing effect if we plot the inverse DCT of the coefficients we just got.\n\nplotting codeone_ay |&gt; \n  mutate(\n    F1_s = idct(\n      dct_direct, n = n()\n      )\n  ) |&gt; \n  ggplot(\n    aes(\n      t\n    )\n  ) +\n  geom_point(\n    aes(y = F1, color = \"original\")\n  ) +\n  geom_line(\n    aes(y = F1_s, color = \"dct smooth\"),\n    linewidth = 1\n  )+\n  labs(\n    color = NULL\n  ) -&gt; p\n\np\np + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn tidynorm() we can get these smoothed formant tracks with reframe_with_dct_smooth().\n\n# grabbing a sample of\n# a few vowel tracks\nset.seed(2025-06)\nspeaker_tracks |&gt; \n  filter(\n    speaker == \"s01\",\n    plt_vclass == \"ay0\"\n  ) |&gt; \n  filter(\n    id %in% sample(unique(id), 5)\n  ) -&gt;\n  ay_tracks\n\n# smoothing happens here\nay_tracks |&gt; \n  reframe_with_dct_smooth(\n    F1:F3,\n    .token_id_col = id,\n    .time_col = t\n  ) -&gt;\n  ay_smooths\n\n\nplotting codeay_tracks |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_path(\n    aes(color = factor(id)),\n    arrow = arrow(\n      type = \"closed\",\n      angle = 25,\n      length = unit(0.5, \"cm\")\n    ),\n    linewidth = 1\n  ) +\n  guides(\n    color = \"none\"\n  ) +\n  scale_y_reverse() +\n  scale_x_reverse() -&gt;\n  track_p\n\ntrack_p %+% ay_smooths -&gt;\n  smooth_p",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#averaging-formant-tracks",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#averaging-formant-tracks",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "Averaging formant tracks",
    "text": "Averaging formant tracks\nSomething that’s really handy about DCT coefficients is they let you average over formant tracks of vowel tokens that are all different lengths. The process goes:\n\nGet the DCT coefficients for each token with reframe_with_dct().\n\nAverage over each vowel class and dct parameter with dplyr::summarise().\nThen, convert everything back into formant-tracks with reframe_with_idct().\n\n\n# Grabbing a subset of\n# vowel classes\nspeaker_tracks |&gt; \n  filter(\n    speaker == \"s03\",\n    str_detect(plt_vclass, \"y\")\n  ) -&gt;\n  y_vowels\n\n# Step 1: Reframe with DCT\ny_vowels |&gt; \n  reframe_with_dct(\n    F1:F3,\n    .by = speaker,\n    .token_id_col = id,\n    .time_col = t\n  ) -&gt; \n  y_vowel_dct\n  \n# Step 2: average over vowel class\n#   and DCT parameter\n  y_vowel_dct |&gt; \n  summarise(\n    .by = c(speaker, plt_vclass, .param),\n    across(\n      F1:F3,\n      mean\n    )\n  ) -&gt;\n  y_vowel_dct_mean\n  \n# Step 3: Convert back to tracks\n# with the inverse DCT\ny_vowel_dct_mean |&gt; \n  reframe_with_idct(\n    F1:F3,\n    .by = speaker,\n    .token_id_col = plt_vclass,\n    .param_col = .param\n  ) -&gt;\n  y_vowel_averages\n\n\nplotting codey_vowel_averages |&gt; \n  ggplot(\n    aes(F2, F1)\n  )+\n  geom_textpath(\n    aes(\n      color = plt_vclass,\n      label = plt_vclass\n    ),\n    arrow = arrow(\n      type = \"closed\",\n      angle = 25,\n      length = unit(0.25, \"cm\")\n    ),\n    linewidth = 1\n  )+\n  guides(color = \"none\")+\n  scale_x_reverse()+\n  scale_y_reverse()-&gt;\n  y_p\n\ny_p\ny_p + theme_dark()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#regressions-with-dcts",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#regressions-with-dcts",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "Regressions with DCTs",
    "text": "Regressions with DCTs\nA really cool thing about DCTs is that you can use them as outcome measures in a regression, and get some nuanced results with some very simple models. For example, let’s go to fit a model looking at the effect of voicing on the F1 of /ay/ (“ay”, vs “ay0”).\nStep 1: get the data subset\n\nspeaker_tracks |&gt; \n  filter(\n    speaker == \"s03\",\n    str_detect(plt_vclass, \"ay\")\n  ) -&gt;\n  ays\n\nStep 2: get the DCTs\n\nays |&gt; \n  reframe_with_dct(\n    F1:F3,\n    .token_id_col = id,\n    .time_col = t\n  )-&gt;\n  ay_dcts\n\nStep 3: pivot wider\nWe need each DCT coefficient in its own column for this, so we’ll pivot wider.\n\nay_dcts |&gt; \n  pivot_wider(\n    names_from = .param,\n    values_from = F1:F3\n  ) -&gt;\n  ay_dct_wide\n\n\n\n\n\n\n\nNoteThe wide data\n\n\n\n\n\n\n\n\n\n\n\n\nspeaker\nid\nvowel\nplt_vclass\nword\n.n\nF1_0\nF1_1\nF1_2\nF1_3\nF1_4\nF2_0\nF2_1\nF2_2\nF2_3\nF2_4\nF3_0\nF3_1\nF3_2\nF3_3\nF3_4\n\n\n\n1\ns03\n0\nAY\nay\nI\n20\n508.0\n2.6\n−27.2\n−7.5\n−15.3\n908.8\n−60.2\n3.8\n−1.4\n−5.9\n1,556.1\n60.0\n−58.8\n78.7\n−52.5\n\n\n2\ns03\n15\nAY\nay\nKIND\n20\n442.6\n118.2\n14.6\n36.3\n11.8\n849.9\n134.7\n−82.7\n124.9\n6.3\n1,535.4\n87.3\n−97.2\n−37.3\n41.2\n\n\n3\ns03\n28\nAY\nay\nMY\n20\n430.2\n−47.1\n−67.3\n−22.7\n−21.5\n867.3\n−141.6\n14.8\n11.6\n23.9\n1,615.5\n−5.0\n−7.0\n−14.6\n2.7\n\n\n4\ns03\n43\nAY\nay\nI\n20\n404.3\n−23.0\n−66.9\n0.7\n−17.2\n906.7\n−76.2\n48.9\n34.0\n−0.7\n1,544.2\n−34.8\n35.9\n53.4\n87.7\n\n\n5\ns03\n55\nAY\nay\nI\n20\n391.2\n34.5\n−26.8\n−6.6\n−16.5\n1,032.7\n−132.0\n29.2\n−5.0\n1.3\n1,587.5\n−11.5\n29.8\n26.3\n−23.8\n\n\n6..45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n46\ns03\n674\nAY\nay0\nLIFE\n20\n414.8\n19.9\n−40.2\n22.9\n−24.4\n850.8\n−219.5\n107.1\n−32.5\n−4.3\n1,573.9\n−88.6\n145.4\n−83.3\n49.9\n\n\n\n\n\n\n\n\n\n\n\nspeaker\nid\nvowel\nplt_vclass\nword\n.n\nF1_0\nF1_1\nF1_2\nF1_3\nF1_4\nF2_0\nF2_1\nF2_2\nF2_3\nF2_4\nF3_0\nF3_1\nF3_2\nF3_3\nF3_4\n\n\n\n1\ns03\n0\nAY\nay\nI\n20\n508.0\n2.6\n−27.2\n−7.5\n−15.3\n908.8\n−60.2\n3.8\n−1.4\n−5.9\n1,556.1\n60.0\n−58.8\n78.7\n−52.5\n\n\n2\ns03\n15\nAY\nay\nKIND\n20\n442.6\n118.2\n14.6\n36.3\n11.8\n849.9\n134.7\n−82.7\n124.9\n6.3\n1,535.4\n87.3\n−97.2\n−37.3\n41.2\n\n\n3\ns03\n28\nAY\nay\nMY\n20\n430.2\n−47.1\n−67.3\n−22.7\n−21.5\n867.3\n−141.6\n14.8\n11.6\n23.9\n1,615.5\n−5.0\n−7.0\n−14.6\n2.7\n\n\n4\ns03\n43\nAY\nay\nI\n20\n404.3\n−23.0\n−66.9\n0.7\n−17.2\n906.7\n−76.2\n48.9\n34.0\n−0.7\n1,544.2\n−34.8\n35.9\n53.4\n87.7\n\n\n5\ns03\n55\nAY\nay\nI\n20\n391.2\n34.5\n−26.8\n−6.6\n−16.5\n1,032.7\n−132.0\n29.2\n−5.0\n1.3\n1,587.5\n−11.5\n29.8\n26.3\n−23.8\n\n\n6..45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n46\ns03\n674\nAY\nay0\nLIFE\n20\n414.8\n19.9\n−40.2\n22.9\n−24.4\n850.8\n−219.5\n107.1\n−32.5\n−4.3\n1,573.9\n−88.6\n145.4\n−83.3\n49.9\n\n\n\n\n\n\n\n\n\nStep 4: Fit the model\nI’ll fit this with a “simple” lm(). This isn’t one of the fancy GAMs you’ve heard about.\n\nay_model &lt;- lm(\n  cbind(F1_0, F1_1, F1_2, F1_3, F1_4) ~ plt_vclass,\n  data = ay_dct_wide\n)\n\nStep 5: Interpreting the model\nThings get a little weird here, but we can apply the inverse DCT to the model parameters to visualize them. Getting confidence intervals takes a few more steps.\n\nlibrary(broom)\n\n# get a dataframe \n# of the model coefficients\ntidy(ay_model) |&gt; \n  \n# apply idct to each model term\n  reframe_with_idct(\n    estimate,\n    .token_id_col = term,\n    .param_col = response,\n    .n = 50\n  ) |&gt; \n\n# plotting\n  ggplot(\n    aes(\n      .time/50, estimate\n    )\n  ) +\n  geom_line(\n    color = ptol_red,\n    linewidth = 2\n  ) +\n  facet_wrap(~term, scales = \"free_y\") -&gt;\n  model_plot\n\n\nplot renderingmodel_plot\nmodel_plot + theme_dark()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can interpret the curve in the Intercept facet like we normally do: It’s the predicted F1 formant track for the reference level. The curve in the “plt_vclassay0” facet is the difference curve, or how much different pre-voiceless /ay/ is predicted to be.\n\n\n\n\n\n\nNoteGetting CIs\n\n\n\n\n\nTo get a visualization of the uncertainty we’ll have to sample from a multivariate normal.\n\nlibrary(mvtnorm)\nlibrary(ggdist)\nSigma &lt;- vcov(ay_model)\nmu_df &lt;- tidy(ay_model)\n\nrmvnorm(\n  1000, \n  mean = mu_df$estimate, \n  sigma = Sigma\n) |&gt; \n  t() |&gt; \n  as_tibble(\n    .name_repair = \"unique_quiet\",\n  ) |&gt; \n  mutate(\n    response = mu_df$response,\n    term = mu_df$term\n  ) |&gt; \n  pivot_longer(\n    starts_with(\"...\"),\n    names_to = \"sample\",\n    values_to = \"estimate\"\n  ) |&gt; \n  reframe_with_idct(\n    estimate,\n    .by = sample,\n    .token_id_col = term,\n    .param_col = response,\n    .n = 50\n  ) |&gt; \n  ggplot(\n    aes(\n      .time, estimate\n    )\n  )+\n  geom_hline(\n    data =  tibble(term = \"plt_vclassay0\", estimate = 0),\n    aes(\n      yintercept = estimate\n    )\n  )+\n  stat_lineribbon()+\n  facet_wrap(~term, scales = \"free_y\")-&gt; ci_plot\n\nci_plot\nci_plot + theme_dark()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#getting-the-rate-and-acceleration",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#getting-the-rate-and-acceleration",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "Getting the rate and acceleration",
    "text": "Getting the rate and acceleration\nIf you’re still here, you might also be interested to know that you can also get the first and second derivatives of the inverse DCT as well. tidynorm has two functions for this (idct_rate() and idct_accel()), but there are also optional arguments .rate and .accel in reframe_with_idct()\n\ny_vowel_dct_mean |&gt; \n  # let's look at just one\n  # vowel\n  filter(\n    plt_vclass == \"ay\"\n  ) |&gt; \n  # reframe with rate and accel\n  reframe_with_idct(\n    F1,\n    .token_id_col = plt_vclass,\n    .param_col = .param,\n    .rate = T,\n    .accel = T,\n    .n = 100\n  ) -&gt;\n  formant_derivatives\n\n\nplot renderingformant_derivatives |&gt; \n  pivot_longer(\n    starts_with(\"F1\")\n  ) |&gt; \n  ggplot(\n    aes(\n      .time, value\n    )\n  )+\n  geom_line(color = ptol_red, linewidth = 1)+\n  facet_wrap(~name, scales = \"free_y\")-&gt;\n  deriv_plot\n\nderiv_plot\nderiv_plot + theme_dark()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#summing-up",
    "href": "posts/2025/06/2025-06-17_dct-in-tidynorm/index.html#summing-up",
    "title": "Doing cool things with the Discrete Cosine Transform in tidynorm",
    "section": "Summing up",
    "text": "Summing up\nThere’s a lot of cool and interesting use cases for DCT coefficients! Expect to see more about them from me!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "06",
      "Doing cool things with the Discrete Cosine Transform in tidynorm"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html",
    "title": "Visualizing the Color Sphere",
    "section": "",
    "text": "This tidytuesday dataset of colors labels is like the perfect confluence of interests for me! I’ve started learning how to do digital art to illustrate characters for a D&D campaign:\nWhich means I’ve been looking a lot at a color picker that uses Hue, Saturation and Lightness sliders (even though they’re not labelled that way).\nBut I’ve had an interest in colors and color theory for a while. From a cognitive science perspective, color constancy is an interesting phenomenon where we perceive colors in a scene to be the “same” when they’re definitely different, something I think shares a lot in common with vowel perception.\nAs was working on the last post, I actually used a physical representation of the Hue-Lightness-Saturation sphere that I already had in the house, cause I guess I’m just into colors.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#the-linguistic-angle",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#the-linguistic-angle",
    "title": "Visualizing the Color Sphere",
    "section": "The linguistic angle",
    "text": "The linguistic angle\nThere’s also a linguistic component to all of this. I could, for example, define “blue” as being a color at 220° with 50% lightness and 75% saturation.\n\nlibrary(colorspace)\n\n\nHLS(220, 0.5, 0.75) |&gt; \n  plot(cex = 10)\n\n\n\n\n\n\n\nAnd then, I could lighten it by increasing the lightness.\n\nHLS(\n  c(220, 200),\n  c(0.5, 0.75),\n  c(0.75, 0.75)\n) |&gt; \n  plot(cex = 10)\n\n\n\n\n\n\n\nBut when people see a color and say “that’s light blue”, are they describing just a shift along the lightness scale, or something else? The best way to visualize shifts through the colorspace would be to plot their vector through the color sphere, but the problem is that the HLS values in and of themselves don’t correspond to x, y and z coordinates.\n\nlibrary(tidyverse)\nlibrary(plotly)\n\nsource(here::here(\"_defaults.R\"))\nset.seed(2025-07-12)\n\n\nplotly dark themedark_plotly &lt;- function(p){\n  p |&gt; \n    layout(\n      paper_bgcolor = \"#222\",\n      scene = list(\n        xaxis = list(color = \"white\"),\n        yaxis = list(color = \"white\"),\n        zaxis = list(color = \"white\")\n      )\n    )\n}",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#the-color-cube",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#the-color-cube",
    "title": "Visualizing the Color Sphere",
    "section": "The color cube",
    "text": "The color cube\n\nHLS(\n  runif(1000, min = 0, max = 360),\n  runif(1000, min = 0, max = 1 ),\n  runif(1000, min = 0, max = 1)\n) -&gt; hls_obj\n\nhls_obj |&gt; \n  slot(\"coords\") |&gt; \n  as_tibble() |&gt; \n  mutate(hex = hex(hls_obj)) -&gt;\n  hls_df\n\n\nplot_ly(\n  x = ~H,\n  y = ~S,\n  z = ~L,\n  data = hls_df,\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(color = hls_df$hex)\n) -&gt; \n  color_cube\n\ncolor_cube\n\n\n\n\n\n\n\n\n\nWe’ve successfully made a color cube, but I really want to get to that color sphere. So this post is about the math I’ve had to work out to get to that point.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#color-unit-circle",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#color-unit-circle",
    "title": "Visualizing the Color Sphere",
    "section": "Color unit circle",
    "text": "Color unit circle\nFirst thing, we need to turn that H angle from degrees to radians, and then a sin() and cos() should get us the points along a unit circle.\n\nhls_df |&gt; \n  mutate(\n    H_radian = H * (pi/180), \n    x = sin(H_radian),\n    y = cos(H_radian)\n  ) -&gt;\n  hls_df\n\n\nhls_df |&gt; \n  ggplot(\n    aes(\n      sin(H_radian), \n      cos(H_radian),  \n      color = hex)\n  ) +\n    geom_point(size = 3) + \n    scale_color_identity() +\n    coord_fixed() -&gt;\n    color_circle\n\ncolor_circle\n(color_circle + theme_dark()) |&gt; dark_render()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ve got each color point arranged along a unit circle. I want to organize the points so that light & dark correspond to the poles of the sphere (like a “north” and “south”), and saturation corresponds to deepness within the sphere.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#the-color-puck",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#the-color-puck",
    "title": "Visualizing the Color Sphere",
    "section": "The color “puck”",
    "text": "The color “puck”\nThe first thing I tried here was just to scale the color values along the unit circle by the saturation (which ranges between 0 and 1) to place each color’s depth within the sphere, which winds up looking like this:\n\nhls_df |&gt; \n  mutate(\n    x = sin(H_radian) * S,\n    y = cos(H_radian) * S\n  ) -&gt;\n  hls_df\n\nIn 2 dimensions, this looks pretty ok!\n\nhls_df |&gt; \n  ggplot(\n    aes(x, y, color = hex)\n  ) + \n  geom_point(size = 3)+\n  scale_x_continuous(\n    limit = c(-1, 1)\n  )+\n  scale_y_continuous(\n    limits = c(-1, 1)\n  )+\n  scale_color_identity()+\n  coord_fixed() -&gt;\n  flat_puck\n\nflat_puck\n(flat_puck + theme_dark()) |&gt; dark_render()\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut when I add the third dimension, I don’t end up with my sphere.\n\nplot_ly(\n  x = ~x,\n  y = ~y,\n  z = ~L,\n  data = hls_df,\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(color = hls_df$hex)\n) -&gt; \n  color_puck\n\ncolor_puck\n\n\n\n\n\n\n\n\n\nIt looks a bit like a color puck, or a Pride layer cake that’s burnt on the bottom and under cooked on the top.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#why-the-puck",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#why-the-puck",
    "title": "Visualizing the Color Sphere",
    "section": "Why the puck?",
    "text": "Why the puck?\nTo illustrate why I wound up with a puck, I took apart my physical color sphere.\n\n\n\n\n\n\nI’d scaled the x and y values above to try to capture the saturation “depth”. The saturation value itself ranges between 0 and 1.\n\nhls_df |&gt; \n  pull(S) |&gt; \n  summary()\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0005024 0.2649756 0.4962482 0.5050065 0.7627292 0.9997049 \n\n\nBut that S value actually describes the relative distance of a point between the north-south axis and the sphere’s surface. So where a value of 1 lands in absolute x, y space depends on the total width of the slice at that point.\n\n\n\n\nSo, I need to scale the relative saturation value based on its lightness to get the sphere.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#the-color-sphere",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#the-color-sphere",
    "title": "Visualizing the Color Sphere",
    "section": "The color sphere!",
    "text": "The color sphere!\nI had to write out some notes for this one. If we rescale Lightness to run between -1 and 1, then the sphere from the side is a unit circle, and the radius at the equator is 1.\n\n\n\n\n\n\n\n\nAt some different value of L, like 0.75, the distance from the center axis to the surface will be the relative max-saturation I’m looking for.\n\n\n\n\n\n\n\n\nThe distance from the center to the same location on the surface will still be 1, though, turning this into solving the Pythagorean theorem.\n\n\n\n\n\n\n\n\nSo if\n\\[\na = L~\\text{(lightness)}\n\\]\n\\[\nb = ?\n\\]\n\\[\nc = 1\n\\]\nand\n\\[\na^2 + b^2 = c^2\n\\]\nthen\n\\[\nb = \\sqrt{|L^2 - 1|}\n\\]\nLet’s do it!\n\nhls_df |&gt; \n  mutate(\n    # new L scale between\n    # -1 and 1\n    L_scale = (L*2) - 1,\n    # maximum S\n    max_S = sqrt(abs((L_scale^2) - 1)),\n    # weighted S\n    weighted_S = S * max_S,\n    # scaled X and Y\n    x = sin(H_radian) * weighted_S,\n    y = cos(H_radian) * weighted_S,\n  ) -&gt;\n  hls_df\n\n\nplot_ly(\n  x = ~x,\n  y = ~y,\n  z = ~L_scale,\n  data = hls_df,\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(color = hls_df$hex)\n) -&gt; \n  color_sphere\n\ncolor_sphere\n\n\n\n\n\n\n\n\n\nFinally, the color sphere!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-12_color-sphere/index.html#from-the-sphere-back-to-hls",
    "href": "posts/2025/07/2025-07-12_color-sphere/index.html#from-the-sphere-back-to-hls",
    "title": "Visualizing the Color Sphere",
    "section": "From the sphere back to HLS",
    "text": "From the sphere back to HLS\nOne kind of downside of the current visualization is that the sphere looks kind of porous, especially at the equator. That comes down to all of the rescaling we did. To get a nicer continuously dense sphere, I want to uniformly sample from these 3d coordinates, and then translate them back into HLS values for coloring in.\nSampling a sphere\n\ntibble(\n  x = runif(3000, min = -1, max = 1),\n  y = runif(3000, min = -1, max = 1),\n  z = runif(3000, min = -1, max = 1)\n) |&gt; \n  filter(\n    sqrt(\n      (x^2) + (y^2) + (z^2)\n    ) &lt;= 1\n  ) -&gt;\n  sphere_df\n\n\nplot_ly(\n  x = ~x,\n  y = ~y,\n  z = ~z,\n  data = sphere_df,\n  type = \"scatter3d\",\n  mode = \"markers\"\n) -&gt;\n  sphere_plot\n\nsphere_plot\n\n\n\n\n\n\n\n\n\n\n\nThat’s a dense meatball!\nFrom x and y to degrees\nI had to double check wikipedia for this, but we can get from x and y back to angles with atan2.\n\nsphere_df |&gt; \n  mutate(\n    H_radian = atan2(y, x),\n    # to handle wrapping around 180\n    H_radian = case_when(\n      H_radian &lt; 0 ~ H_radian + (2*pi),\n      .default = H_radian\n    ),\n    H = H_radian / (pi/180)\n  ) -&gt;\n  sphere_df\n\nFrom z to Lightness\nTranslating z back into Lightness is probably the simplest bit of math\n\nsphere_df |&gt; \n  mutate(\n    L = (z + 1)/2\n  ) -&gt; \n  sphere_df\n\nFrom depth to saturation\nWe can get the absolute depth of a point on the x/y axis with the Pythagorean theorem. Then we need to divide it by the maximum possible depth like we did before.\n\nsphere_df |&gt; \n  mutate(\n    depth = sqrt((x^2) + (y^2)),\n    max_S = sqrt(abs((z^2) - 1)),\n    S = depth/max_S\n  ) -&gt;\n  sphere_df\n\nLet’s double check the numbers look right.\n\nsphere_df |&gt; \n  select(H, L, S) |&gt; \n  summary()\n\n       H                   L                 S          \n Min.   :  0.07694   Min.   :0.02029   Min.   :0.05407  \n 1st Qu.: 86.71262   1st Qu.:0.32880   1st Qu.:0.52054  \n Median :173.03568   Median :0.50556   Median :0.72744  \n Mean   :177.47325   Mean   :0.49831   Mean   :0.68226  \n 3rd Qu.:266.16528   3rd Qu.:0.67266   3rd Qu.:0.88465  \n Max.   :359.72754   Max.   :0.99171   Max.   :0.99998  \n\n\nAdding in the hex codes\n\nsphere_df |&gt; \n  mutate(\n    hex = HLS(H, L, S) |&gt; \n      hex()\n  ) -&gt;\n  sphere_df\n\nThe moment of truth\n\nplot_ly(\n  x = ~x,\n  y = ~y,\n  z = ~z,\n  data = sphere_df,\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(color = sphere_df$hex)\n) -&gt;\n  sphere_plot2\n\nsphere_plot2\n\n\n\n\n\n\n\n\n\nThere it is! The color sphere!\n\nNow I can actually visualize the difference between “blue” and “light blue”.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Visualizing the Color Sphere"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-16_color-sql/index.html",
    "href": "posts/2025/07/2025-07-16_color-sql/index.html",
    "title": "Getting the xkcd color survey sqlite database",
    "section": "",
    "text": "I had a lot of fun working with the XKCD color survey data, and I think I’ll keep messing around with it here and there in the future. But I also think I’ll want access to the full data. The tidytuesday data set was necessarily boiled down. The answers data frame contained just one hex code associated with one color label, not every color label given to every hex code in the survey. The full data set is available as a sqlite dump linked to from the xkcd blog, so this post is just about how I set up access to it within my blog RStudio project.\n# basic setup\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(colorspace)",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Getting the xkcd color survey sqlite database"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-16_color-sql/index.html#necessary-libraries",
    "href": "posts/2025/07/2025-07-16_color-sql/index.html#necessary-libraries",
    "title": "Getting the xkcd color survey sqlite database",
    "section": "Necessary Libraries",
    "text": "Necessary Libraries\nFor this workflow, I used\n\nfs: for working with file systems\nDBI and RSQLite: for working with the sqlite database\ndbplyr: for using dplyr commands on the database.\n\n\nlibrary(fs)\nlibrary(DBI)\nlibrary(RSQLite)\nlibrary(dbplyr)",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Getting the xkcd color survey sqlite database"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-16_color-sql/index.html#downloading-the-database",
    "href": "posts/2025/07/2025-07-16_color-sql/index.html#downloading-the-database",
    "title": "Getting the xkcd color survey sqlite database",
    "section": "Downloading the database",
    "text": "Downloading the database\nIf you want to hang onto the tar file locally (in case of linkrot), you could download it to a real location, but I just want to send it to a temp file. You can get the temp directory for your R session with fs::path_temp(), and I created the destination file using fs::path().\ndownload.file(\n  \"http://xkcd.com/color/colorsurvey.tar.gz\",\n  fs::path(\n    fs::path_temp(),\n    \"colorsurvey.tar.gz\"\n  )\n)\nThen, it’s a matter of untaring it.\nuntar(\n  fs::path(fs::path_temp(), \"colorsurvey.tar.gz\"),\n  exdir = fs::path(fs::path_temp(), \"colorsurvey\")\n)\nYou can get a list of the available files like so:\npath(\n  path_temp(),\n  \"colorsurvey\"\n) |&gt;\n  dir_ls() -&gt;\n  dump_files\nI’m not actually including this code in executable chunks to avoid downloading the tar file on every re-render, but dump_files will look something like\n[1] /var/folders/xyz/mainsurvey_sqldump.txt\n[2] /var/folders/xyz/satfaces_sqldump.txt\nIt’s the mainsurvey_sqldump.txt file that we want to process.\ndump_files |&gt;\n  keep(\n    ~str_detect(.x, \"mainsurvey\")\n  ) -&gt;\n  survey_dump",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Getting the xkcd color survey sqlite database"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-16_color-sql/index.html#creating-the-local-database",
    "href": "posts/2025/07/2025-07-16_color-sql/index.html#creating-the-local-database",
    "title": "Getting the xkcd color survey sqlite database",
    "section": "Creating the local database",
    "text": "Creating the local database\nTo set up the database, we need to read the sqldump file. Apparently there’s a sqlite command .read that will do this, but I couldn’t figure out how to run it within a DBI or RSQLite function, so I have to use a system() command.\nFirst, I have decide where this database is going, and since I don’t want it to wind up being duplicated in every post directory, I’ll create a top level project directory called data.\nfs::dir_create(\n  here::here(\"data\")\n)\nThen, I need to decide on the name of the database file.\nlocal_sql_path &lt;- here::here(\"data\", \"colors.sqlite\")\nI’ll glue my variables into the system command I want:\ncreation_command &lt;- str_glue(\n  \"sqlite3 {local_sql_path} &lt; {survey_dump}\"\n)\nAnd then finally run it.\nsystem(creation_command)\nUpdating .gitignore\n\nYou’ll probably want to add *.sqlite file to your gitignore, which you can either do do by hand or with usethis::use_git_ignore()\nusethis::use_git_ignore(\"*.sqlite\")",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Getting the xkcd color survey sqlite database"
    ]
  },
  {
    "objectID": "posts/2025/07/2025-07-16_color-sql/index.html#looking-at-the-database",
    "href": "posts/2025/07/2025-07-16_color-sql/index.html#looking-at-the-database",
    "title": "Getting the xkcd color survey sqlite database",
    "section": "Looking at the database",
    "text": "Looking at the database\nNow we can connect to the database and look at it.\n\ncolordb &lt;- dbConnect(\n  RSQLite::SQLite(), \n  here::here(\"data\", \"colors.sqlite\")\n)\n\n\ndbListTables(colordb)\n\n[1] \"answers\" \"names\"   \"users\"  \n\n\nWe can access tables from the database like it’s a dataframe with tbl(). It doesn’t actually read the whole thing into memory.\n\ncolors &lt;- tbl(colordb, \"answers\")\ncolors\n\n# Source:   table&lt;`answers`&gt; [?? x 7]\n# Database: sqlite 3.50.1 [/Users/joseffruehwald/Documents/blog/data/colors.sqlite]\n      id user_id  datestamp     r     g     b colorname   \n   &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       \n 1     1       1 1267418734    72   100   175 pastel blue \n 2     2       1 1267418739   204   177   246 faint violet\n 3     3       1 1267418769   182   226   245 baby blue   \n 4     4       1 1267418773   130    64   234 purple      \n 5     5       2 1267419006    75    49   234 blue        \n 6     6       2 1267419010    76   215   249 light blue  \n 7     7       2 1267419015   111   145   122 olive green \n 8     8       2 1267419019    88    70     1 brown       \n 9     9       2 1267419021   218    35   156 pink        \n10    10       4 1267419023   154    42   159 purple      \n# ℹ more rows\n\n\nNow, we can run the dplyr-like commands on this table thanks to dbplyr and only load the rows we’re interested in.\n\ncolors |&gt; \n  # get just blue and light blue\n  filter(\n    colorname %in% c(\"blue\", \"light blue\")\n  ) |&gt; \n  # manually get some hex values\n  mutate(\n    rh = sql(\"printf('%02X', r)\"),\n    gh = sql(\"printf('%02X', g)\"),\n    bh = sql(\"printf('%02X', b)\")\n  ) |&gt; \n  mutate(\n    hex = sql(\"rh||gh||bh\")\n  )\n\n# Source:   SQL [?? x 11]\n# Database: sqlite 3.50.1 [/Users/joseffruehwald/Documents/blog/data/colors.sqlite]\n      id user_id  datestamp     r     g     b colorname rh    gh    bh    hex   \n   &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; \n 1     5       2 1267419006    75    49   234 blue      4B    31    EA    4B31EA\n 2    17       2 1267419032    41   201   234 blue      29    C9    EA    29C9EA\n 3    22       2 1267419040    73    97   236 blue      49    61    EC    4961EC\n 4    27       4 1267419062    33   115   229 blue      21    73    E5    2173E5\n 5    45       6 1267419091    14    47   164 blue      0E    2F    A4    0E2FA4\n 6    49       4 1267419096    64   128   225 blue      40    80    E1    4080E1\n 7    57       7 1267419102    74    89   253 blue      4A    59    FD    4A59FD\n 8    87       6 1267419139   120   158   209 blue      78    9E    D1    789ED1\n 9   100       7 1267419170    74   107   231 blue      4A    6B    E7    4A6BE7\n10   115      10 1267419193    95   196   210 blue      5F    C4    D2    5FC4D2\n# ℹ more rows\n\n\nI’m kind of glad I messed around with manually converting the rgb values to hex values, cause it turns out I don’t know what the right way is to convert these rgb values to hex codes. My manual approach gives me #4B31EA for the first row, but using the colorspace package, I get two different hex codes depending on whether I assume r, g, b are RGB or sRGB values.\n\nRGB(\n  75/255,\n  49/255,\n  234/255\n) |&gt; \n  hex() \n\n[1] \"#9479F6\"\n\n\n\nsRGB(\n  75/255,\n  49/255,\n  234/255\n) |&gt; \n  hex()\n\n[1] \"#4B31EA\"\n\n\n\nswatchplot(\n  \"RGB\" = \"#9479F6\",\n  \"sRGB\" = \"#4B31EA\"\n)\n\n\n\n\n\n\n\n🤷‍♂️\n\ndbDisconnect(colordb)",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "07",
      "Getting the xkcd color survey sqlite database"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-21_ai-policy/index.html",
    "href": "posts/2025/08/2025-08-21_ai-policy/index.html",
    "title": "AI Policy",
    "section": "",
    "text": "Note\n\n\n\nThis is a version of the AI course policy I’m introducing this semester.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "AI Policy"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-21_ai-policy/index.html#consequences",
    "href": "posts/2025/08/2025-08-21_ai-policy/index.html#consequences",
    "title": "AI Policy",
    "section": "Consequences",
    "text": "Consequences\nYou won’t receive credit for any submitted coursework that is obviously AI generated. I will leave feedback on coursework that was AI generated that this is the reason why it isn’t receiving a grade. If you actually didn’t generate your coursework with AI, you should ask me to revisit the grade. We should be able to resolve the issue with a straightforward conversation about the work you submitted.\nI will be judicious about whether or not I flag an assignment as AI generated. If you are not using AI, you don’t need to worry about taking extra steps to make sure your coursework doesn’t “look” like AI in some way.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "AI Policy"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-21_ai-policy/index.html#assignments-arent-obstacles-theyre-the-course.",
    "href": "posts/2025/08/2025-08-21_ai-policy/index.html#assignments-arent-obstacles-theyre-the-course.",
    "title": "AI Policy",
    "section": "Assignments aren’t obstacles, they’re the course.",
    "text": "Assignments aren’t obstacles, they’re the course.\nA common way to think about course assignments is like they’re obstacles that I, the professor, put in the way between you and completion of the course. But they’re not. They’re exercises that, by virtue of you doing them, you learn. If you don’t actually do them, then you’re not learning. To quote Ted Chiang:\n\nAs the linguist Emily M. Bender has noted, teachers don’t ask students to write essays because the world needs more student essays. The point of writing essays is to strengthen students’ critical-thinking skills; in the same way that lifting weights is useful no matter what sport an athlete plays, writing essays develops skills necessary for whatever job a college student will eventually get. Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.\n\nThe same can be said for things like summarizing the contents of a textbook chapter yourself. By creating your own study guide, or condensing the contents of a chapter into a few bullet points yourself, you have already studied. The valuable thing about course notes isn’t the actual notes, it’s the act of making them. By offloading the act of making them to an AI, they lose their value.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "AI Policy"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-21_ai-policy/index.html#as-a-learner-you-dont-have-enough-context",
    "href": "posts/2025/08/2025-08-21_ai-policy/index.html#as-a-learner-you-dont-have-enough-context",
    "title": "AI Policy",
    "section": "As a learner, you don’t have enough context",
    "text": "As a learner, you don’t have enough context\nWhen you start a new chat with ChatGPT, it says at the bottom of the screen “ChatGPT can make mistakes. Check important info.” Even if you take that seriously, how would you know if an AI summarized readings or course notes incorrectly? You’re a learner in this course because you don’t already know its content. AI systems may be useful to some users who have enough context to be able to tell when they’ve gone astray, but you are definitionally not in that position.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "AI Policy"
    ]
  },
  {
    "objectID": "posts/2025/08/2025-08-21_ai-policy/index.html#learning-is-still-a-fundamental-purpose-of-higher-education",
    "href": "posts/2025/08/2025-08-21_ai-policy/index.html#learning-is-still-a-fundamental-purpose-of-higher-education",
    "title": "AI Policy",
    "section": "Learning is still a fundamental purpose of higher education",
    "text": "Learning is still a fundamental purpose of higher education\nThere are lots of different reasons to go to college, and I won’t judge any as being more or less valid than another. But in my role as an educator in this course, the purpose I am meant to tend to is your learning. I want to facilitate your learning, and in my judgment, for this course, that means prohibiting the use of AI tools.",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "08",
      "AI Policy"
    ]
  },
  {
    "objectID": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html",
    "href": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html",
    "title": "Working with stat_manual()",
    "section": "",
    "text": "Following on from my post about darkmode in ggplot2 4.0, I wanted to also mess around with the new stat_manual() that’s available. And folks, it’s good!\nsource(here::here(\"_defaults.R\"))\nlibrary(tidyverse)\nlibrary(tidynorm)\nlibrary(scales)\nThe announcement blog post says\nLet’s put it to the test!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "09",
      "Working with stat_manual()"
    ]
  },
  {
    "objectID": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html#plotting-sine-waves",
    "href": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html#plotting-sine-waves",
    "title": "Working with stat_manual()",
    "section": "Plotting sine waves",
    "text": "Plotting sine waves\nI’m teaching Phonetics this semester, so I’ve got sine waves on the mind. Over a single cycle, the amplitude of sine wave of \\(h\\) Hz can be given as\n\\[\ny = \\sin(2hx\\pi)\n\\]\nLet’s assume I’m going to pass a make_sine() function with an x and freq column.\n\nmake_sine &lt;- function(df){\n  df |&gt; \n    mutate(\n      y = sin((x * freq) * (2*pi))\n    )\n}\n\nfreq isn’t a normal aesthetic, but if I map a data column to freq in the ggplot aesthetic mapping, it’ll get processed by the make_sine() function. I’ll set up a grid of values of time by frequency with expand_grid() and pass it to ggplot. I map time to the x-axis and frequency to the freq “aesthetic” that make_sine() will make use of.\nThe important part is in geom_line(), where I tell it to use the \"manual\" aesthetic with the make_sine() function.\n\nexpand_grid(\n  time = seq(0,1, length = 500),\n  frequency = c(1, 2, 3)\n) |&gt; \n  ggplot(\n    aes(\n      x = time, \n      group = frequency,\n      freq = frequency\n    )\n  ) + \n    geom_line(\n      stat = \"manual\",\n      fun = make_sine\n    ) -&gt;\n  sine_plot\n\nsine_plot\nsine_plot + theme_darkmode()\n\n\n\n\n\n\n\nVoila! The make_sine() function calculated the y values! Another fun thing is I can map the values I passed to freq to another aesthetic with after_stat()\n\nsine_plot + \n  aes(color = after_stat(freq))\n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nI wanted to see if I could plot some discrete Fourier transform basis functions too. For this, I have two copies of the input data frame bound row-wise to each other, one with the sine functions and the other with the cosine. To get the right groupings by line, I define the group aesthetic as well. I’ve also added in whether the function is sine or cosine.\n\nmake_dft &lt;- function(df){\n  df |&gt; \n    mutate(\n      y = sin(2 * pi * x * freq),\n      group = str_glue(\"sin{freq}\"),\n      func = \"sin\"\n    ) |&gt; \n    bind_rows(\n      df |&gt; \n        mutate(\n          y = cos(2 * pi * x * freq),\n          group = str_glue(\"cos{freq}\"),\n          func = \"cos\"\n        )\n    )\n}\n\nThe code here is basically the same as above, but I’ve swapped in make_dft.\n\nexpand_grid(\n  time = seq(0,1, length = 500),\n  frequency = c(0, 1, 2)\n) |&gt; \n  ggplot(\n    aes(time, freq = frequency)\n  )+\n  geom_line(\n    stat = \"manual\",\n    fun = make_dft\n  ) -&gt; \n  dft_plot\n\ndft_plot\ndft_plot + theme_darkmode()\n\n\n\n\n\n\n\nAnd again, we can also map any data columns processed or created by our custom stat function to other aesthetics.\n\ndft_plot +\n  aes(\n    color = after_stat(freq),\n    linetype = after_stat(func)\n  )\n\nlast_plot() + theme_darkmode()",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "09",
      "Working with stat_manual()"
    ]
  },
  {
    "objectID": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html#multi-aggregation-plots",
    "href": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html#multi-aggregation-plots",
    "title": "Working with stat_manual()",
    "section": "Multi-aggregation plots",
    "text": "Multi-aggregation plots\nI’m commonly in the situation of wanting to visualize data at multiple levels of aggregation on the same plot. For example: I might want a plot of by-speaker & by-vowel means, along with a point indicating by-speaker grand means. Previously, this has involved aggregating the data twice, then adding each aggregation to the plot.\n\nspeaker_data |&gt; \n  summarise(\n    .by = c(speaker, vowel),\n    across(F1:F2, mean)\n  ) -&gt;\n  vowel_means\n\nspeaker_data |&gt; \n  summarise(\n    .by = c(speaker),\n    across(F1:F2, mean)\n  ) -&gt;\n  speaker_means\n\nggplot(\n  data = vowel_means,\n  aes(F2, F1, color = speaker)\n) +\n  geom_text(\n    aes(label = vowel)\n  ) +\n  geom_point(\n    data = speaker_means,\n    size = 3\n  ) +\n  scale_x_reverse() +\n  scale_y_reverse()\n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nBut with the right stat_manual() function, we can skip this initial aggregation step.\nWith make_means(), I’ve made sure to summarize input data grouped by all data columns that are not x or y. This will make sure that the aggregation will respect any aesthetic mapping we define in the plot.\n\nmake_means &lt;- function(df){\n  df |&gt; \n    summarise(\n      .by = matches(\"[^xy]\"),\n      across(c(x,y), mean)\n    )\n}\n\nSo, if we use this stat_manual() function without defining any other aesthetic mapping, we’ll get just one point in the middle of the plot at the mean of all x and y data.\n\nspeaker_data |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_point(\n    stat = \"manual\",\n    fun = make_means,\n    size = 5\n  ) +\n  scale_x_reverse() +\n  scale_y_reverse() \n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nBut if we map speaker to color, we’ll now get a point for each speaker\n\nspeaker_data |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_point(\n    aes(color = speaker),\n    stat = \"manual\",\n    fun = make_means,\n    size = 5\n  ) +\n  scale_x_reverse() +\n  scale_y_reverse() \n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nGetting the mean for each vowel just involves adding a geom_text() layer and mapping vowel to label.\n\nspeaker_data |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_point(\n    aes(color = speaker),\n    stat = \"manual\",\n    fun = make_means,\n    size = 5\n  ) +\n  geom_text(\n    aes(\n      color = speaker,\n      label = vowel\n    ),\n    stat = \"manual\",\n    fun = make_means\n  ) +\n  scale_x_reverse() +\n  scale_y_reverse() \n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nBoom! All of the data aggregation happened inside the ggplot processing! And what’s cool is I can change up my aesthetic mapping and the data will be re-aggregated correctly. There’s another vowel class coding in the plt_vclass column I can use instead by just mapping it to label\n\nspeaker_data |&gt; \n  ggplot(\n    aes(F2, F1)\n  ) +\n  geom_point(\n    aes(color = speaker),\n    stat = \"manual\",\n    fun = make_means,\n    size = 5\n  ) +\n  geom_text(\n    aes(\n      color = speaker,\n      label = plt_vclass\n    ),\n    stat = \"manual\",\n    fun = make_means\n  ) +\n  scale_x_reverse() +\n  scale_y_reverse() \n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nI’ll make a typical kind of plot you’ll see at a sociolinguistics conference. Let’s say I’m interested in whether or not the /o/ and /oh/ distributions overlap for these two speakers. I’ll want to plot the raw data, and maybe some data ellipses, and also a label in the center of each ellipse.\n\nspeaker_data |&gt; \n  filter(\n    plt_vclass %in% c(\"o\", \"oh\")\n  ) |&gt; \n  ggplot(\n    aes(\n      F2, F1,\n      color = speaker,\n      shape = plt_vclass\n    )\n  ) +\n  geom_point(alpha = 0.6) +\n  stat_ellipse(aes(linetype = plt_vclass)) +\n  geom_label(\n    aes(\n      label = plt_vclass,\n      linetype = plt_vclass\n    ),\n    stat = \"manual\",\n    fun = make_means,\n    show.legend = F\n  ) +\n  scale_shape_manual(\n    values = c(19, 1)\n  ) +\n  scale_y_reverse() +\n  scale_x_reverse()\n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nSure, there’s a lot of ggplot in there, but I didn’t have to do any annoying additional aggregation to get the label positions!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "09",
      "Working with stat_manual()"
    ]
  },
  {
    "objectID": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html#wrapping-up",
    "href": "posts/2025/09/2025-09-12_working-with-stat-manual/index.html#wrapping-up",
    "title": "Working with stat_manual()",
    "section": "Wrapping up",
    "text": "Wrapping up\nI foresee my ggplot life getting supercharged by this. For example:\n\nz_score &lt;- function(df){\n  df |&gt; \n    mutate(\n      .by = speaker,\n      across(c(x,y), \\(x)(x-mean(x))/sd(x))\n    )\n}\n\n\nspeaker_data |&gt; \n  ggplot(\n    aes(\n      F2, F1, \n      speaker = speaker, \n      color = speaker\n    )\n  ) +\n  geom_text(\n    aes(label = ipa_vclass),\n    stat = \"manual\",\n    fun = \\(df){df |&gt; z_score() |&gt; make_means()},\n    family = \"Voces\",\n    key_glyph = \"rect\"\n   ) +\n  scale_x_reverse() +\n  scale_y_reverse()+\n  coord_fixed()\n\nlast_plot() + theme_darkmode()\n\n\n\n\n\n\n\nThat’s Lobanov normalization done right there in ggplot!",
    "crumbs": [
      "About",
      "Posts",
      "2025",
      "09",
      "Working with stat_manual()"
    ]
  }
]